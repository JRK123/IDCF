{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this file we generate input for idcf algorithm for douban bookreview dataset. \n",
    "- it generates core and non core users\n",
    "- core users goes as an input to idcf\n",
    "- non core user goes as an input to GMF\n",
    "- We generate non core user using idcf algorithm\n",
    "- Using generated non core user we tweak the original non core user and generate tweaked matrix\n",
    "- We give tweaked matrix as an input to GMF\n",
    "- we compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fileinput import filename\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from scipy.sparse.linalg import svds\n",
    "import random \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as ss\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- code to read and clean douban book review file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml1m_dir = 'data/bookreviews_cleaned.txt'\n",
    "# ml1m_rating = pd.read_csv(ml1m_dir, delim_whitespace=True,index_col=0, engine='python')\n",
    "ml1m_rating = pd.read_csv(ml1m_dir, delim_whitespace=True, usecols=range(3), header=None)\n",
    "# df = ml1m_rating[[\"user_id\", book_id\", \"rating\", \"time\"]]\n",
    "ml1m_rating.head()\n",
    "ml1m_rating.to_csv('data/douban_bookreview.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData(threshold=30):\n",
    "    # uncomment this line for movielens dataset\n",
    "    ml1m_dir = 'data/douban_bookreview.csv'\n",
    "    ml1m_rating = pd.read_csv(ml1m_dir, sep=',', header=None, names=['uid', 'mid', 'rating'],  engine='python')\n",
    "    # use the below 2 lines for pinterest dataset.\n",
    "    # ml1m_dir = 'data/pin-interest-main.txt'\n",
    "    # ml1m_rating = pd.read_csv(ml1m_dir, delim_whitespace=True, header=None, names=['uid', 'mid', 'rating', 'timestamp'],  engine='python')\n",
    "\n",
    "    unique_uid = np.unique(np.array(ml1m_rating['uid'].tolist()))\n",
    "    unique_mid = np.unique(np.array(ml1m_rating['mid'].tolist()))\n",
    "    uid_dict = dict([(y,x) for x,y in enumerate(unique_uid)])\n",
    "    mid_dict = dict([(y,x) for x,y in enumerate(unique_mid)])\n",
    "    print('DICTIONARY PREPARED:')\n",
    "\n",
    "    # init user item dictionary:\n",
    "    \n",
    "    uid_list = ml1m_rating['uid'].tolist()\n",
    "    uid_list_len = len(uid_list)\n",
    "    mid_list = ml1m_rating['mid'].tolist()\n",
    "    mid_list_len = len(mid_list)\n",
    "    rating_list = ml1m_rating['rating'].tolist()\n",
    "    user_item_dict = {x:set() for x in range(len(unique_uid))}\n",
    "    item_user_dict = {x:set() for x in range(len(unique_mid))}\n",
    "    for i in range(uid_list_len):\n",
    "        uid_list[i] = uid_dict[uid_list[i]]\n",
    "        mid_list[i] = mid_dict[mid_list[i]]\n",
    "        # rating_list[i] = 1 # comment this line if you want to activate explicit ratings\n",
    "        user_item_dict[uid_list[i]].add(mid_list[i])\n",
    "        item_user_dict[mid_list[i]].add(uid_list[i])\n",
    "    tmp_df = pd.DataFrame({\"uid\":uid_list, \"mid\":mid_list, \"ratings\":rating_list})\n",
    "    v = tmp_df.uid.value_counts()\n",
    "    tmp_df = pd.DataFrame({\"uid\":uid_list, \"mid\":mid_list, \"ratings\":rating_list})\n",
    "    # v = tmp_df.uid.value_counts()\n",
    "    # df = tmp_df[tmp_df.uid.isin(v.index[v.gt(30)])]\n",
    "### code to store less than 30 interactions:\n",
    "    # df_less_30 = tmp_df[tmp_df.uid.isin(v.index[v.le(30)])]\n",
    "    return tmp_df, len(np.unique(mid_list)), len(unique_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DICTIONARY PREPARED:\n",
      "UNIQUE MIDS:  95872\n",
      "UNIQUE UIDS:  2212\n"
     ]
    }
   ],
   "source": [
    "df, unique_mids, unique_uids = ReadData()\n",
    "print(\"UNIQUE MIDS: \", unique_mids)\n",
    "print(\"UNIQUE UIDS: \", unique_uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF USERS IN SUPPORT TEST: 1371\n",
      "NUMBER OF USERS IN SUPPORT TRAIN 1371\n",
      "NUMBER OF USERS IN QUERY TEST: 841\n",
      "NUMBER OF USERS IN QUERY TRAIN 725\n"
     ]
    }
   ],
   "source": [
    "# support_test_df = df_gt_30.groupby(\"uid\").tail(1)\n",
    "# # print(len(df_gt_30['uid']))\n",
    "# support_train_df = df_gt_30.drop(df_gt_30.groupby('uid').tail(1).index, inplace=False)\n",
    "# assert(len(df_gt_30)== len(support_test_df) + len(support_train_df))\n",
    "# # print(len(test_df))\n",
    "# # print(len(train_df))\n",
    "# query_test_df = df_le_30.groupby(\"uid\").tail(1)\n",
    "# query_train_df = df_le_30.drop(df_le_30.groupby('uid').tail(1).index, inplace=False)\n",
    "# assert(len(df_le_30)== len(query_test_df) + len(query_train_df))\n",
    "# dic_support_train_df_uid_mapping = dict([(y,x) for x,y in enumerate(np.unique(support_train_df['uid']))])\n",
    "# dic_support_train_df_uid_rmapping = dict([(x,y) for x,y in enumerate(np.unique(support_train_df['uid']))])\n",
    "# ### no need for mid mapping\n",
    "\n",
    "# uid_of_train_df = support_train_df['uid'].tolist()\n",
    "# for i in range(len(uid_of_train_df)):\n",
    "#     uid_of_train_df[i] = dic_support_train_df_uid_mapping[uid_of_train_df[i]]\n",
    "# # for index, row in train_df.iterrows():\n",
    "# #     train_df['uid'][index] = dic_train_df_uid_mapping[train_df['uid'][index]]\n",
    "# core_user_ko_input_train_df = pd.DataFrame({'uid':uid_of_train_df, 'mid':support_train_df['mid'], 'ratings':support_train_df['ratings']})\n",
    "# print(\"NUMBER OF USERS IN SUPPORT TEST:\", len(np.unique(support_test_df['uid'])))\n",
    "# print(\"NUMBER OF USERS IN SUPPORT TRAIN\", len(np.unique(support_train_df['uid'])))\n",
    "# print(\"NUMBER OF USERS IN QUERY TEST:\", len(np.unique(query_test_df['uid'])))\n",
    "# print(\"NUMBER OF USERS IN QUERY TRAIN\", len(np.unique(query_train_df['uid'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ui_dic = {}    \n",
    "# for user in range(unique_uids):\n",
    "#     train_ui_dic[user] = []\n",
    "# for index,row in support_train_df.iterrows():\n",
    "#         train_ui_dic[row['uid']].append(row['mid'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- utility functions for CUR coreusers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MID = 27277 + 1\n",
    "def select_cols(mat, k, dup=False):\n",
    "    # prob 1d array of probabilities of all columns\n",
    "    prob = mat.T.dot(mat)\n",
    "    prob = np.array(np.diagonal(prob))\n",
    "    denom = np.abs(prob).sum(axis = 0)\n",
    "    prob = prob/denom\n",
    "\n",
    "    C = np.zeros((mat.shape[0], k))\n",
    "    ind_cols = np.arange(0, prob.size)\n",
    "    c_ind = []\n",
    "    i = 0\n",
    "    while(i < k):\n",
    "        rand_sel = np.random.choice(ind_cols, 1, p=prob)\n",
    "        if rand_sel in c_ind:\n",
    "            continue\n",
    "        c_ind.append(rand_sel[0])\n",
    "        C[:, i] = mat[:, rand_sel[0]]\n",
    "        i += 1\n",
    "        # C[:, i] = C[:, i]/np.sqrt(k*prob[rand_sel[0]])\n",
    "\n",
    "    return C, c_ind\n",
    "\n",
    "def select_rows(mat, k, dup=False):\n",
    "\n",
    "    prob = mat.dot(mat.T)\n",
    "    prob = np.array(np.diagonal(prob))\n",
    "    denom = np.abs(prob).sum(axis=0)\n",
    "    prob = prob/denom\n",
    "    print(prob)\n",
    "    r = np.zeros((k, mat.shape[1]))\n",
    "    ind_rows = np.arange(0, prob.size)\n",
    "    r_ind = []\n",
    "    i = 0\n",
    "    while(i < k):\n",
    "        # print(ind_rows)\n",
    "        rand_sel = np.random.choice(ind_rows, 1, p=prob)\n",
    "        if rand_sel in r_ind:\n",
    "            continue\n",
    "        r_ind.append(rand_sel[0])\n",
    "        r[i, :] = mat[rand_sel[0], :]\n",
    "        i += 1\n",
    "        # r[i, :] = r[i, :]/np.sqrt(k*prob[rand_sel[0]])\n",
    "    r_ind = np.array(r_ind)\n",
    "    return r, r_ind\n",
    "\n",
    "# def matIntersection(mat, c_ind, r_ind):\n",
    "    \n",
    "#     W = np.zeros((len(r_ind), len(c_ind)))\n",
    "#     for i in range(len(r_ind)):\n",
    "#         W[i] = mat[r_ind[i], c_ind]\n",
    "    \n",
    "#     return W\n",
    "\n",
    "# def pseudoInverse(W):\n",
    "#     # U = WP (W+)\n",
    "\n",
    "#     # W = X.Z.YT\n",
    "#     X, Z, YT = np.linalg.svd(W)\n",
    "    \n",
    "#     # W+ = Y.Z+.XT\n",
    "#     XT = X.T\n",
    "#     Y = YT.T\n",
    "#     # Z+ = reciprocal(Z)\n",
    "#     ZP = np.reciprocal(Z)\n",
    "#     ZP = sp.spdiags(ZP, 0, ZP.size, ZP.size)\n",
    "#     ZP = ZP@ZP\n",
    "    \n",
    "#     # W+ = Y.Z+.XT\n",
    "#     WP = Y@ZP\n",
    "#     WP = WP@XT\n",
    "\n",
    "#     return WP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractCoreUsers(dataframe, unique_user_len, unique_item_len, percent_core_user=0.40):\n",
    "    # print(\"# of rows in ml1m_ratings: \", len(dataframe))\n",
    "    u_len = unique_user_len\n",
    "    print(\"USER LEN:\", u_len)\n",
    "    # print(user_id)\n",
    "\n",
    "    m_len = unique_item_len\n",
    "    print(\"MOVIE LEN:\", m_len)\n",
    "    userItemMatrix = np.zeros(shape=(u_len, m_len))\n",
    "    # print(userItemMatrix)\n",
    "    \n",
    "    unique_uid = np.unique(np.array(dataframe['uid'].tolist()))\n",
    "    assert(u_len == len(unique_uid))\n",
    "    uid_dict = dict([(y,x) for x,y in enumerate(unique_uid)])\n",
    "    rev_uid_dict = dict([(x,y) for x,y in enumerate(unique_uid)])\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        userItemMatrix[uid_dict[int(row['uid'])]][int(row['mid'])] = int(row['ratings'])\n",
    "        # print(row['uid'], row['mid'])\n",
    "    print(\"USER ITEM MATRIX: \\n\", userItemMatrix)\n",
    "\n",
    "    df = pd.DataFrame(userItemMatrix)\n",
    "    cosineSimilarity = cosine_similarity(df)\n",
    "    print(\"SHAPE OF COSINE MATIX:\\n \", cosineSimilarity.shape)\n",
    "\n",
    "    listToStoreTopFiftyOfEveryUser = []\n",
    "    for i in range(0, cosineSimilarity.shape[0]):\n",
    "        idx = np.argpartition(cosineSimilarity[i], -50)[-50:]\n",
    "        listToStoreTopFiftyOfEveryUser.append(idx)\n",
    "    # print(\"Top fifty list: \\n\", listToStoreTopFiftyOfEveryUser)\n",
    "    # listToStoreTopFiftyOfEveryUser = np.array(listToStoreTopFiftyOfEveryUser)\n",
    "    flatten = np.concatenate(listToStoreTopFiftyOfEveryUser)\n",
    "    listToStoreTopFiftyOfEveryUser = flatten.ravel()\n",
    "\n",
    "    # print(\"List of top 50\", listToStoreTopFiftyOfEveryUser)\n",
    "    df = pd.DataFrame(listToStoreTopFiftyOfEveryUser)\n",
    "    allUserList = df.value_counts().index.tolist()\n",
    "    # print(\"ALL USERS LIST\", allUserList)\n",
    "    allUserList = list(sum(allUserList,()))\n",
    "    # print(\"ALL USERS LIST\", allUserList)\n",
    "    twentyPercentUserList = allUserList[:int(len(allUserList)*0.2)]\n",
    "    # print(\"TWENTY PERCENT USER:\", len(twentyPercentUserList))\n",
    "    # print(\"TWENTY PERCENT USER:\", (twentyPercentUserList))\n",
    "    \n",
    "    r_ind = twentyPercentUserList\n",
    "    len_r_ind = len(r_ind)\n",
    "    new_r_ind = [rev_uid_dict[r_ind[i]] for i in range(len_r_ind)]\n",
    "    cos_coreusers = dataframe.iloc[np.where(dataframe.uid.isin(new_r_ind))]\n",
    "    # coreusers = dataframe.iloc[np.where(dataframe.uid.isin(twentyPercentUserList))]\n",
    "    # coreusers.reset_index()\n",
    "    # print(\"CORE USERS:\\n\", coreusers)\n",
    "    return cos_coreusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CUR_ExtractCoreUsers(dataframe, unique_user_len, unique_item_len, percent_core_user=0.40):\n",
    "    # print(\"# of rows in ml1m_ratings: \", len(dataframe))\n",
    "    u_len = unique_user_len\n",
    "    print(\"USER LEN:\", u_len)\n",
    "    # print(user_id)\n",
    "\n",
    "    m_len = unique_item_len\n",
    "    print(\"MOVIE LEN:\", m_len)\n",
    "    \n",
    "    # print(userItemMatrix)\n",
    "\n",
    "    unique_uid = np.unique(np.array(dataframe['uid'].tolist()))\n",
    "    assert(u_len == len(unique_uid))\n",
    "    # print(\"UNIQUE UID:\", unique_uid)\n",
    "    uid_dict = dict([(y,x) for x,y in enumerate(unique_uid)])\n",
    "    rev_uid_dict = dict([(x,y) for x,y in enumerate(unique_uid)])\n",
    "\n",
    "    userItemMatrix = np.zeros(shape=(u_len, m_len))\n",
    "    for index, row in dataframe.iterrows():\n",
    "        userItemMatrix[uid_dict[int(row['uid'])]][int(row['mid'])] = int(row['ratings'])\n",
    "        # print(row['uid'], row['mid'])\n",
    "    print(\"USER ITEM MATRIX: \\n\", userItemMatrix)\n",
    "\n",
    "    mat = userItemMatrix\n",
    "    print(\"MAT:\", mat)\n",
    "    print(mat.shape)\n",
    "    # C, c_ind = select_cols(mat, int(u_len * percent_core_user)) ## getting 20% core users\n",
    "    r, r_ind= select_rows(mat, int(u_len * percent_core_user))\n",
    "    # print(\"r\", r)\n",
    "    # print(\"r_ind len\", len(r_ind))\n",
    "\n",
    "    len_r_ind = len(r_ind)\n",
    "    new_r_ind = [rev_uid_dict[r_ind[i]] for i in range(len_r_ind)]\n",
    "    cur_coreusers = dataframe.iloc[np.where(dataframe.uid.isin(new_r_ind))]\n",
    "    # # coreusers.reset_index()\n",
    "    # # print(\"CORE USERS:\\n\", coreusers)\n",
    "    return cur_coreusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomSampling(dataframe, unique_user_len, unique_item_len, percent_core_user=0.40):\n",
    "    u_len = unique_user_len\n",
    "    print(\"USER LEN:\", u_len)\n",
    "    # print(user_id)\n",
    "\n",
    "    m_len = unique_item_len\n",
    "    print(\"MOVIE LEN:\", m_len)\n",
    "\n",
    "    how_much_to_sample = int(percent_core_user * u_len)\n",
    "    indices = random.sample(range(u_len), how_much_to_sample)\n",
    "    cur_coreusers = dataframe.iloc[np.where(dataframe.uid.isin(indices))]\n",
    "    return cur_coreusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class largest_leveragescores_Sampler:\n",
    "    def __init__(self, A, k, Q,N):\n",
    "        \"\"\" Create largest k-leverage scores Sampler for the matrix :math:`A` for k-low rank apparoximation.\n",
    "        :param A: \n",
    "            Matrix :math:`A`.\n",
    "        :type A: \n",
    "            array_type\n",
    "        :param Q: \n",
    "            Matrix containig the right singular vectors of :math:`A`.\n",
    "        :type Q: \n",
    "            array_type\n",
    "        :param k: \n",
    "            The order of low rank apparoximation.\n",
    "        :type k: \n",
    "            int\n",
    "        :param N: \n",
    "            The dimension of subsampling (the number of columns) of A.\n",
    "        :type N: \n",
    "            int\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "        # print(\"k is\", type(k))\n",
    "        self.Q = np.transpose(Q[0:k,:])\n",
    "        self.N = N\n",
    "        self.k = k\n",
    "        \n",
    "        self.sampling_list = []\n",
    "        self.lvs_array = self.Estimate_Leverage_Scores()\n",
    "    def Estimate_Leverage_Scores(self):\n",
    "        return 1/(self.k)*np.diag(np.dot(self.Q,self.Q.T))\n",
    "    def MultiRounds(self):\n",
    "        temp_list = list(reversed(np.argsort(self.lvs_array)))\n",
    "        sampled_indices_ = temp_list[0:self.k]\n",
    "        self.sampling_list = sampled_indices_\n",
    "        return sampled_indices_\n",
    "from scipy.sparse.linalg import svds\n",
    "def Extract_LLS(dataframe, unique_user_len, unique_item_len, percent_core_user=0.40):\n",
    "    u_len = unique_user_len\n",
    "    print(\"USER LEN:\", u_len)\n",
    "    # print(user_id)\n",
    "\n",
    "    m_len = unique_item_len\n",
    "    print(\"MOVIE LEN:\", m_len)\n",
    "    \n",
    "    # print(userItemMatrix)\n",
    "\n",
    "    unique_uid = np.unique(np.array(dataframe['uid'].tolist()))\n",
    "    assert(u_len == len(unique_uid))\n",
    "    # print(\"UNIQUE UID:\", unique_uid)\n",
    "    uid_dict = dict([(y,x) for x,y in enumerate(unique_uid)])\n",
    "    rev_uid_dict = dict([(x,y) for x,y in enumerate(unique_uid)])\n",
    "\n",
    "    userItemMatrix = np.zeros(shape=(u_len, m_len))\n",
    "    for index, row in dataframe.iterrows():\n",
    "        userItemMatrix[uid_dict[int(row['uid'])]][int(row['mid'])] = int(row['ratings'])\n",
    "        # print(row['uid'], row['mid'])\n",
    "    print(\"USER ITEM MATRIX: \\n\", userItemMatrix)\n",
    "    A = userItemMatrix.T\n",
    "    number_of_coreusers = int(u_len * percent_core_user)\n",
    "    print(number_of_coreusers)\n",
    "    u4, s4, Q = svds(A, k=number_of_coreusers)\n",
    "    lls = largest_leveragescores_Sampler(A, number_of_coreusers, Q, number_of_coreusers)\n",
    "    indx = lls.MultiRounds()\n",
    "    # indx = indx.\n",
    "    print(\"LENGTH OF INDEX:\", len(indx))\n",
    "    print(\"INDICES ARE:\", indx)\n",
    "    cur_coreusers = dataframe.iloc[np.where(dataframe.uid.isin(indx))]\n",
    "    # # coreusers.reset_index()\n",
    "    # # print(\"CORE USERS:\\n\", coreusers)\n",
    "    return cur_coreusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER LEN: 2212\n",
      "MOVIE LEN: 95872\n",
      "USER ITEM MATRIX: \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "884\n",
      "LENGTH OF INDEX: 884\n",
      "INDICES ARE: [787, 1137, 1235, 1255, 1194, 894, 2056, 114, 1205, 1642, 2143, 1565, 901, 1643, 1249, 2198, 1462, 337, 340, 1667, 1694, 1419, 1192, 929, 1107, 955, 1509, 1987, 1911, 2209, 1003, 2003, 1821, 1832, 2019, 2036, 402, 2007, 1297, 1520, 1405, 1864, 977, 1254, 1329, 1082, 1191, 881, 322, 2059, 1931, 1915, 1611, 1919, 1264, 2181, 1482, 1121, 1539, 1498, 1564, 986, 1665, 1523, 2177, 971, 807, 308, 1360, 23, 2034, 2186, 2182, 1866, 976, 2130, 1234, 427, 1538, 1727, 1112, 1617, 1836, 411, 328, 1305, 1026, 338, 1540, 1949, 731, 1209, 429, 1804, 1926, 1469, 2125, 1651, 1318, 1282, 350, 1017, 1826, 1869, 1313, 920, 1456, 1296, 2119, 2121, 2066, 866, 1150, 306, 1202, 1166, 2030, 441, 1546, 2200, 2075, 1048, 1421, 1324, 1991, 1167, 1423, 1788, 904, 931, 1741, 1891, 1774, 906, 1764, 1301, 1532, 1207, 1201, 1801, 314, 1957, 2205, 1441, 1148, 1655, 1350, 207, 624, 2001, 2073, 1251, 2179, 1698, 682, 87, 2038, 1714, 1288, 1778, 1822, 1055, 1400, 1092, 2065, 885, 2160, 959, 1726, 405, 933, 1710, 1407, 1341, 1389, 1575, 760, 534, 1005, 1294, 961, 1284, 2149, 1078, 1478, 1076, 1449, 2032, 416, 26, 1577, 1002, 1036, 364, 25, 1518, 975, 1846, 1024, 1818, 2106, 1352, 1660, 2077, 1798, 1541, 2151, 1139, 1559, 1064, 1158, 1123, 536, 1797, 2004, 378, 1393, 855, 2086, 1265, 943, 1126, 1842, 617, 1854, 610, 1417, 992, 614, 735, 1990, 1560, 1814, 1530, 374, 2166, 385, 1455, 1773, 397, 1630, 1879, 925, 1070, 1529, 2114, 1164, 1063, 1503, 1206, 865, 417, 1999, 978, 1583, 1882, 1544, 1806, 950, 258, 1348, 1368, 1943, 2115, 1379, 1809, 111, 1686, 668, 2153, 1779, 1845, 1111, 2109, 1040, 1872, 798, 981, 974, 1080, 686, 419, 1095, 1479, 1637, 444, 1105, 1594, 1440, 572, 1491, 1385, 1865, 2017, 878, 937, 1676, 1713, 1808, 1717, 1480, 2063, 902, 1680, 1740, 1195, 897, 1436, 1871, 2134, 1767, 2084, 1610, 1027, 1666, 1034, 1406, 355, 1670, 1226, 2136, 1831, 2124, 1945, 1257, 880, 970, 1154, 438, 631, 1974, 1447, 303, 503, 390, 1547, 627, 1766, 1248, 2002, 2089, 701, 1381, 2072, 1977, 613, 1243, 857, 1289, 1673, 2142, 1614, 1101, 60, 1266, 1640, 2050, 1104, 1662, 1293, 1382, 1198, 1827, 2162, 1748, 1091, 1274, 2069, 1146, 2206, 1719, 1426, 1624, 525, 1445, 1256, 2148, 280, 1855, 2014, 1223, 1709, 951, 1998, 1481, 1018, 35, 1772, 947, 564, 1094, 2169, 1183, 692, 175, 1162, 428, 2172, 1878, 1512, 1025, 758, 1734, 1084, 957, 1459, 1242, 422, 1386, 1824, 471, 1384, 716, 995, 1691, 663, 51, 877, 1675, 1398, 999, 1008, 1571, 1595, 980, 319, 178, 2074, 1950, 1718, 1578, 1631, 1122, 1472, 1918, 377, 935, 1007, 1754, 1038, 2197, 1628, 982, 1493, 946, 18, 1134, 638, 1199, 535, 478, 886, 434, 1117, 825, 1247, 537, 548, 1333, 889, 546, 1936, 45, 966, 2147, 1511, 879, 2080, 1536, 1705, 919, 2116, 514, 1749, 2207, 1245, 334, 1786, 1589, 392, 1851, 1712, 1424, 1023, 2127, 2098, 1097, 997, 228, 1598, 330, 1567, 1129, 2145, 1047, 965, 1220, 1537, 1678, 1514, 482, 1820, 1430, 1791, 1172, 2048, 80, 1738, 1971, 1185, 200, 2188, 1897, 78, 698, 923, 916, 1928, 755, 609, 1751, 1898, 2128, 1721, 2076, 2008, 1442, 1555, 2054, 884, 365, 14, 1794, 2174, 893, 1395, 1762, 1402, 8, 1500, 389, 1510, 1136, 1125, 315, 224, 1942, 1077, 1190, 1715, 1980, 1212, 1669, 1403, 1531, 1701, 1906, 346, 1621, 2199, 1353, 1330, 71, 2201, 342, 1982, 1573, 988, 1693, 233, 349, 1295, 1909, 369, 1410, 113, 1314, 1657, 845, 497, 861, 1434, 401, 1776, 58, 2155, 2005, 1115, 372, 333, 1672, 296, 2053, 602, 2154, 475, 1867, 1839, 1357, 1337, 2015, 1601, 2141, 359, 1346, 587, 744, 2110, 1563, 1283, 2187, 1505, 1182, 2087, 135, 57, 869, 956, 403, 1140, 567, 1739, 92, 1635, 1948, 1901, 1885, 769, 112, 551, 903, 1390, 1533, 1304, 5, 1045, 1043, 2168, 862, 1490, 1579, 1905, 1639, 1211, 1119, 1312, 1838, 1302, 2203, 1963, 639, 529, 777, 1815, 2025, 2079, 1067, 1591, 1443, 439, 1153, 1236, 445, 2183, 1081, 6, 1570, 1976, 2091, 404, 1551, 1699, 1147, 2061, 910, 968, 1193, 611, 882, 856, 1178, 872, 1165, 962, 1986, 2108, 2060, 1001, 838, 460, 2067, 2131, 1203, 800, 2137, 1394, 1819, 510, 523, 680, 1088, 386, 853, 1582, 302, 1011, 2047, 1458, 1290, 1039, 1996, 930, 1877, 1415, 2193, 1093, 435, 1638, 2176, 1700, 1636, 1556, 381, 1895, 1366, 757, 1425, 1096, 1962, 1145, 1169, 1071, 709, 964, 1888, 1947, 1807, 842, 2178, 2013, 2029, 1525, 771, 1917, 1253, 76, 97, 1850, 2022, 1492, 61, 858, 1656, 2144, 1585, 2020, 347, 1383, 395, 993, 316, 912, 1315, 1959, 1428, 1114, 1409, 679, 1073, 1156, 2204, 1173, 1937, 1732, 1777, 918, 1370, 1239, 1261, 1988, 1542, 1841, 1916, 703, 1083, 332, 1013, 509, 907, 1187, 896, 672, 522, 1784, 541, 1281, 1387, 1259, 1323, 1890, 2156, 547, 837, 710, 694, 1780, 1984, 1380, 388, 1218, 1010, 2093, 784, 1946, 442, 1894, 1175, 1258, 1612, 921, 1528, 726, 1648, 376, 1332, 606, 584, 891, 530, 642, 2088, 1813, 1450, 1127, 1118, 1581, 1277, 1843, 491, 1320, 1873, 1087, 88, 151, 1309, 470, 685, 1602, 2132, 1600, 1576, 1627, 1925, 1444, 909, 1527, 1729, 158, 1596, 1967, 1343, 1708, 2027, 1902, 286, 560, 1863, 262, 193, 589, 647, 1770, 582, 927, 13, 1339, 2191, 1452, 499, 1416, 1768, 1730, 1994, 1204]\n",
      "CORE USERS:          uid    mid  ratings\n",
      "0        613  55945        3\n",
      "1        613  81196        4\n",
      "2        613  32933        4\n",
      "3        613  62950        5\n",
      "4        613  59289        4\n",
      "...      ...    ...      ...\n",
      "227246  1814   4305        5\n",
      "227247  1814   5877        5\n",
      "227248  1814    809        5\n",
      "227249  1814   1022        4\n",
      "227250  1814   1051        5\n",
      "\n",
      "[202637 rows x 3 columns]\n",
      "NUMBER OF CORE USERS: 884\n"
     ]
    }
   ],
   "source": [
    "core_users = Extract_LLS(df, len(np.unique(df['uid'])), unique_mids, percent_core_user=0.40)\n",
    "support_user_list = np.unique(core_users['uid'])\n",
    "print(\"CORE USERS:\" ,core_users)\n",
    "print(\"NUMBER OF CORE USERS:\", len(support_user_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE SNIPPET FOR GETTING NESTED CORE USERS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER LEN: 884\n",
      "MOVIE LEN: 95872\n",
      "USER ITEM MATRIX: \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "MAT: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(884, 95872)\n",
      "[0.00044968 0.00049554 0.00038351 0.00035704 0.00055433 0.00044506\n",
      " 0.00234227 0.00149462 0.00213944 0.00094614 0.00055894 0.00066821\n",
      " 0.00053186 0.00050293 0.00058942 0.00046753 0.00072731 0.00045922\n",
      " 0.00063343 0.00062266 0.0013318  0.00038135 0.00048077 0.00042813\n",
      " 0.00118006 0.00058942 0.00067498 0.0081007  0.00045153 0.00032133\n",
      " 0.0003081  0.00062389 0.00083349 0.00029025 0.00087966 0.00152602\n",
      " 0.00042198 0.00070422 0.00039674 0.00321424 0.0002964  0.00038997\n",
      " 0.00029363 0.00076362 0.00033795 0.00063435 0.00294062 0.00145523\n",
      " 0.00266607 0.00048754 0.00041921 0.00073839 0.00146661 0.00274517\n",
      " 0.00054263 0.00031056 0.00077409 0.00067221 0.00245954 0.00252449\n",
      " 0.00503327 0.00092891 0.00073161 0.00037242 0.00048754 0.00252356\n",
      " 0.00099939 0.00069899 0.00086735 0.00098215 0.00085812 0.00067283\n",
      " 0.00168145 0.00029763 0.00061342 0.00065713 0.00041305 0.00130287\n",
      " 0.000494   0.0003041  0.00068668 0.0004466  0.00100339 0.00034411\n",
      " 0.00097446 0.00048384 0.00233612 0.0004149  0.00046999 0.00070853\n",
      " 0.00343954 0.00128194 0.00144261 0.00130933 0.0010868  0.00240968\n",
      " 0.00033087 0.00246416 0.00055587 0.00037427 0.00094122 0.00044476\n",
      " 0.00053617 0.00029579 0.00115729 0.00056695 0.00037858 0.00029917\n",
      " 0.00082641 0.00044876 0.00074885 0.00075439 0.00036719 0.00053001\n",
      " 0.0003558  0.00094276 0.00041521 0.00044537 0.00049985 0.00037027\n",
      " 0.00039459 0.00142968 0.00056356 0.00031702 0.00202648 0.00061096\n",
      " 0.00062481 0.00046107 0.00044229 0.00067775 0.00046538 0.00060265\n",
      " 0.000494   0.00034965 0.00041767 0.00046476 0.00140875 0.00034595\n",
      " 0.00038504 0.00059219 0.0002847  0.00057741 0.00037396 0.00044876\n",
      " 0.00085627 0.00047769 0.00146354 0.00063097 0.00129025 0.00164082\n",
      " 0.00092398 0.00111974 0.00092737 0.00056664 0.00038566 0.00041305\n",
      " 0.00044322 0.00114836 0.00033118 0.00034534 0.00036504 0.00191476\n",
      " 0.0003675  0.00094645 0.00068298 0.00034688 0.00065067 0.00091167\n",
      " 0.00036535 0.00033672 0.00031179 0.0015808  0.00033426 0.00122254\n",
      " 0.00128071 0.00063682 0.00101201 0.00053432 0.00091352 0.00118868\n",
      " 0.00046476 0.00036596 0.00053771 0.00034996 0.00033518 0.00061866\n",
      " 0.00040967 0.00096984 0.00092214 0.00039582 0.00043121 0.00037396\n",
      " 0.00066113 0.0004466  0.00127455 0.00048569 0.00097323 0.00041982\n",
      " 0.00066544 0.00046353 0.00135366 0.00227241 0.00045922 0.00034134\n",
      " 0.00053217 0.00140136 0.00070084 0.00096738 0.00238906 0.00054171\n",
      " 0.00063805 0.00181226 0.00050108 0.00107726 0.00034996 0.00075747\n",
      " 0.01256118 0.00043337 0.00179749 0.00673596 0.00098031 0.00040997\n",
      " 0.0020825  0.00202033 0.00041428 0.00035919 0.00038658 0.00045799\n",
      " 0.00075285 0.00034103 0.00051093 0.00268238 0.00032656 0.0006916\n",
      " 0.0011539  0.00030163 0.00539493 0.00065067 0.0023669  0.00106587\n",
      " 0.00153217 0.00080918 0.00079933 0.00091967 0.00105879 0.00107418\n",
      " 0.00065097 0.00131549 0.00040413 0.00066975 0.00138998 0.00160173\n",
      " 0.00039736 0.00040443 0.00108803 0.00062635 0.00037612 0.00103232\n",
      " 0.00464669 0.00072546 0.0009169  0.00228933 0.00281381 0.00091629\n",
      " 0.00080271 0.00153648 0.00045953 0.00069837 0.00042229 0.00115667\n",
      " 0.00044229 0.00106557 0.000727   0.00100955 0.00034996 0.00157526\n",
      " 0.0023552  0.00093014 0.00052632 0.00091875 0.00033487 0.00048846\n",
      " 0.00043152 0.0014463  0.00123423 0.0005491  0.00182273 0.00083195\n",
      " 0.00341461 0.00103971 0.00099385 0.00098585 0.000723   0.00057895\n",
      " 0.00132011 0.00047954 0.00048046 0.00043214 0.00225548 0.00188675\n",
      " 0.00179564 0.00128194 0.00044937 0.00089259 0.00046661 0.00036596\n",
      " 0.00170084 0.00042321 0.00226133 0.00160789 0.00046445 0.00280119\n",
      " 0.0003281  0.00063897 0.00032379 0.00045953 0.00056387 0.00070084\n",
      " 0.00042783 0.00128379 0.00055217 0.00043491 0.00035673 0.00065128\n",
      " 0.00091444 0.0010551  0.00218038 0.00107234 0.00468824 0.00036535\n",
      " 0.00050016 0.00046907 0.00030933 0.00039551 0.00122531 0.00109204\n",
      " 0.00156757 0.00083811 0.00214744 0.00029732 0.00041859 0.00056941\n",
      " 0.00063343 0.01345253 0.00211944 0.00044968 0.00041398 0.00106218\n",
      " 0.00058603 0.00162697 0.00272209 0.00046507 0.00158173 0.00034749\n",
      " 0.00153494 0.00064266 0.00094984 0.00065097 0.00192522 0.00212559\n",
      " 0.0003755  0.00051986 0.00040105 0.00036935 0.00045368 0.00046784\n",
      " 0.00100893 0.00051555 0.00034903 0.00032995 0.00112035 0.00242846\n",
      " 0.00050908 0.0041216  0.00036042 0.00065005 0.0009643  0.00069653\n",
      " 0.0018455  0.00041336 0.00031764 0.00748912 0.00096523 0.00061866\n",
      " 0.00304249 0.0005491  0.00031056 0.00029763 0.00052386 0.00085627\n",
      " 0.00162266 0.00425549 0.00952084 0.00042937 0.0005097  0.00108003\n",
      " 0.00084396 0.00054694 0.00081564 0.00139428 0.00091906 0.0014306\n",
      " 0.00042937 0.00170392 0.00904807 0.00055063 0.00148816 0.0003792\n",
      " 0.00026778 0.00046168 0.00308528 0.00203849 0.00069837 0.00050016\n",
      " 0.00033303 0.00032472 0.00183011 0.00063035 0.00172731 0.00157342\n",
      " 0.00066606 0.00047184 0.00089536 0.00185843 0.00077009 0.00123916\n",
      " 0.00386091 0.00171161 0.00062358 0.00046568 0.00109481 0.00032749\n",
      " 0.00052016 0.00243061 0.00056018 0.00039828 0.00229457 0.00032164\n",
      " 0.00038905 0.00128225 0.00383597 0.00046291 0.00028778 0.00051155\n",
      " 0.00041705 0.00030933 0.00091167 0.00028532 0.00045091 0.00185012\n",
      " 0.0028609  0.00209512 0.00058141 0.00048231 0.00163374 0.00044106\n",
      " 0.00155649 0.00037273 0.00118837 0.0003318  0.0013278  0.00111758\n",
      " 0.00033426 0.00125516 0.00090736 0.00096492 0.00039859 0.00214283\n",
      " 0.00038566 0.00136905 0.0003595  0.00066913 0.00102555 0.00098369\n",
      " 0.00043768 0.00070699 0.00439492 0.0007073  0.00189444 0.00031795\n",
      " 0.00054817 0.00047092 0.00029148 0.00178364 0.00473964 0.00175809\n",
      " 0.00118406 0.00087412 0.00036719 0.00070853 0.00038628 0.00047923\n",
      " 0.00060111 0.00138443 0.00104402 0.00311144 0.00071623 0.00046692\n",
      " 0.00038073 0.00079871 0.00163128 0.00203756 0.00029948 0.00031148\n",
      " 0.00171192 0.00323363 0.00053155 0.00090151 0.0042915  0.00197262\n",
      " 0.00052693 0.00200555 0.00129333 0.00146261 0.00130195 0.00214714\n",
      " 0.00061312 0.00085565 0.00033918 0.00157219 0.0043863  0.00064605\n",
      " 0.00049862 0.00028009 0.00614962 0.00061127 0.00104525 0.00109327\n",
      " 0.00077378 0.00173562 0.00489877 0.00054386 0.00045984 0.00029455\n",
      " 0.00032133 0.00125578 0.00093722 0.00074454 0.00203664 0.000494\n",
      " 0.00075285 0.00048323 0.00224132 0.00481751 0.00227333 0.00117391\n",
      " 0.00043337 0.0008218  0.00191876 0.00117791 0.00040474 0.00090244\n",
      " 0.00039797 0.0025842  0.0017704  0.0004269  0.00281011 0.00878645\n",
      " 0.00112682 0.00050416 0.00065744 0.00063251 0.00171131 0.00034411\n",
      " 0.00119145 0.00101878 0.00052478 0.00032041 0.0004032  0.00059865\n",
      " 0.00044937 0.0009603  0.00033795 0.00077778 0.00068483 0.00029794\n",
      " 0.00081503 0.00031887 0.00055156 0.00033518 0.00066544 0.00365222\n",
      " 0.00035796 0.00097477 0.00227148 0.00044876 0.0005177  0.00034965\n",
      " 0.00093629 0.00138844 0.00105818 0.00058141 0.00047492 0.00138597\n",
      " 0.00038351 0.00045953 0.0009643  0.00151371 0.00750205 0.00030071\n",
      " 0.00102309 0.00170638 0.00047492 0.00051709 0.00216776 0.00100585\n",
      " 0.00244138 0.00081533 0.00491693 0.00072177 0.00114898 0.00044322\n",
      " 0.00077748 0.00058911 0.00101909 0.00050477 0.00086427 0.00102955\n",
      " 0.00105818 0.00057495 0.00475996 0.00171008 0.00038043 0.00039643\n",
      " 0.00078517 0.00045153 0.00031271 0.0010077  0.00129333 0.00062173\n",
      " 0.00148908 0.00124347 0.00055063 0.00189752 0.00063035 0.00127794\n",
      " 0.00058111 0.00152109 0.00178856 0.00032995 0.00033549 0.00037119\n",
      " 0.00060911 0.00062481 0.0004466  0.00137889 0.00127948 0.00109511\n",
      " 0.00061404 0.00061496 0.00087166 0.00096738 0.00251864 0.00094799\n",
      " 0.00099077 0.00033118 0.00034318 0.0008929  0.00067406 0.00285474\n",
      " 0.00030225 0.00033364 0.00128317 0.00162266 0.00034226 0.00039028\n",
      " 0.00048292 0.00201048 0.00052355 0.00052386 0.00176209 0.00142845\n",
      " 0.00248509 0.00170823 0.00197847 0.00033888 0.00132903 0.00214437\n",
      " 0.0003558  0.00179472 0.00042937 0.00097415 0.00048169 0.00054509\n",
      " 0.00226317 0.00225917 0.00075101 0.00233981 0.00094276 0.00044876\n",
      " 0.00493632 0.00239337 0.00033611 0.00050108 0.00032595 0.00132257\n",
      " 0.00036381 0.00140383 0.00142383 0.0004149  0.00082611 0.00167376\n",
      " 0.00099262 0.00032995 0.00462822 0.00073777 0.00202987 0.00065928\n",
      " 0.00210128 0.00105633 0.00093014 0.00031117 0.00046753 0.00079533\n",
      " 0.00134996 0.00099754 0.00053401 0.00044629 0.0003515  0.00228041\n",
      " 0.0003518  0.00041613 0.00092552 0.00055925 0.00056233 0.00035026\n",
      " 0.00045245 0.00050601 0.00075624 0.00342785 0.00155218 0.00035519\n",
      " 0.00039305 0.00091475 0.00264329 0.00031671 0.00293754 0.00074085\n",
      " 0.00334505 0.0008495  0.00044999 0.00056726 0.00129394 0.0007587\n",
      " 0.00032379 0.00044106 0.00057895 0.00089751 0.00065374 0.00123547\n",
      " 0.00036904 0.00049123 0.00041859 0.00029055 0.00062758 0.00058049\n",
      " 0.00043891 0.00049339 0.00035519 0.00048508 0.00046199 0.00048015\n",
      " 0.00379165 0.00031702 0.00071838 0.00375564 0.00029486 0.0004072\n",
      " 0.0007507  0.00178487 0.00311082 0.00121669 0.00208496 0.00097538\n",
      " 0.00049062 0.00219084 0.00085381 0.00047122 0.00065005 0.00036042\n",
      " 0.00149585 0.00397356 0.0003635  0.00042229 0.00064266 0.00033734\n",
      " 0.00038227 0.00161989 0.00137674 0.00113297 0.0042124  0.00083134\n",
      " 0.00064266 0.00062143 0.00077994 0.0005888  0.00058572 0.00394801\n",
      " 0.00333951 0.00048661 0.00044414 0.00083041 0.00218038 0.00178887\n",
      " 0.00061773 0.0012607  0.00110465 0.0006362  0.00101878 0.00237367\n",
      " 0.00031918 0.00125886 0.00039551 0.00082611 0.00097015 0.00211913\n",
      " 0.00060265 0.00032133 0.00077932 0.00045891 0.00041798 0.00108126\n",
      " 0.00204741 0.00042967 0.00148077 0.00042044 0.00130041 0.00134011\n",
      " 0.00115021 0.00151155 0.00450265 0.0006599  0.00513577 0.00067129\n",
      " 0.00066636 0.00262636 0.00039489 0.00032256 0.00149093 0.00119422\n",
      " 0.00031979 0.00043429 0.00064389 0.00042844 0.00029548 0.00094891\n",
      " 0.00099724 0.00122192 0.00124193 0.00143799 0.00076116 0.00069222\n",
      " 0.00039212 0.00031518 0.001221   0.00111204 0.00127702 0.00058203\n",
      " 0.00089782 0.00083534 0.00077501 0.00062666 0.00191045 0.00035981\n",
      " 0.00146261 0.00351156 0.00232535 0.00057249 0.00317884 0.00046384\n",
      " 0.00087289 0.00031025 0.00043675 0.00088366 0.00473687 0.00048785\n",
      " 0.00290984 0.00083903 0.00045676 0.00036535 0.00197601 0.00110158\n",
      " 0.00062266 0.00394401]\n",
      "442\n",
      "NESTED CORE USERS:\n",
      "          uid    mid  ratings\n",
      "0        613  55945        3\n",
      "1        613  81196        4\n",
      "2        613  32933        4\n",
      "3        613  62950        5\n",
      "4        613  59289        4\n",
      "...      ...    ...      ...\n",
      "226685  1348  78309        3\n",
      "226686  1348  83113        2\n",
      "226687  1348  80709        5\n",
      "226688  1348  17170        3\n",
      "226689  1348   6066        5\n",
      "\n",
      "[140178 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "core_users_nested = CUR_ExtractCoreUsers(core_users, len(np.unique(core_users['uid'])), unique_mids, percent_core_user=0.50)\n",
    "print(len(np.unique(core_users_nested['uid'])))\n",
    "print(\"NESTED CORE USERS:\\n\" ,core_users_nested)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE SNIPPET FOR GETTING NESTED CORE USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON CORE USERS:          uid    mid  ratings\n",
      "285     1707  53320        1\n",
      "286     1707  60332        5\n",
      "287     1707  56218        5\n",
      "288     1707  56068        4\n",
      "289     1707  51775        4\n",
      "...      ...    ...      ...\n",
      "227246  1814   4305        5\n",
      "227247  1814   5877        5\n",
      "227248  1814    809        5\n",
      "227249  1814   1022        4\n",
      "227250  1814   1051        5\n",
      "\n",
      "[87073 rows x 3 columns]\n",
      "CORE USERS:          uid    mid  ratings\n",
      "0        613  55945        3\n",
      "1        613  81196        4\n",
      "2        613  32933        4\n",
      "3        613  62950        5\n",
      "4        613  59289        4\n",
      "...      ...    ...      ...\n",
      "226685  1348  78309        3\n",
      "226686  1348  83113        2\n",
      "226687  1348  80709        5\n",
      "226688  1348  17170        3\n",
      "226689  1348   6066        5\n",
      "\n",
      "[140178 rows x 3 columns]\n",
      "1770\n",
      "[   0    1    2 ... 2208 2210 2211]\n",
      "NEW DF:         uid    mid  ratings\n",
      "0      1707  53320        1\n",
      "1      1707  60332        1\n",
      "2      1707  56218        1\n",
      "3      1707  56068        1\n",
      "4      1707  51775        1\n",
      "...     ...    ...      ...\n",
      "87068  1814   4305        1\n",
      "87069  1814   5877        1\n",
      "87070  1814    809        1\n",
      "87071  1814   1022        1\n",
      "87072  1814   1051        1\n",
      "\n",
      "[87073 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "core_users_index_list_nested = core_users_nested.index.to_list()\n",
    "non_core_user_index = (df.index.difference(core_users_nested.index))\n",
    "non_core_user_index = non_core_user_index.tolist()\n",
    "\n",
    "core_users_df_nested = df.loc[core_users_index_list_nested]\n",
    "non_core_user_df = df.loc[non_core_user_index]\n",
    "print(\"NON CORE USERS:\" ,non_core_user_df)\n",
    "print(\"CORE USERS:\", core_users_nested)\n",
    "print(len(np.unique(non_core_user_df['uid'])))\n",
    "print(np.unique(non_core_user_df['uid']))\n",
    "li = [1]*len(non_core_user_df)\n",
    "new_non_core_user_df = pd.DataFrame({'uid':non_core_user_df['uid'].tolist(), 'mid':non_core_user_df['mid'].tolist(), 'ratings':li})\n",
    "print(\"NEW DF:\", new_non_core_user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1770\n",
      "43522\n"
     ]
    }
   ],
   "source": [
    "# core_users_df.to_csv(\"ml-1m_core_users.csv\", index=False, header=False)\n",
    "new_non_core_user_df.to_csv(\"douban_new_non_core_users.csv\", index=False, header=False)\n",
    "print(len(np.unique(new_non_core_user_df['uid'])))\n",
    "print(len(np.unique(new_non_core_user_df['mid'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_of_core_users = np.unique(core_users_df_nested['uid']).tolist()\n",
    "uid_non_core_users = np.unique(non_core_user_df['uid']).tolist()\n",
    "import pickle\n",
    "with open(\"douban_core_users_and_non_core_users_obj.pkl\", \"wb\") as f:\n",
    "    pickle.dump(uid_of_core_users, f)\n",
    "    pickle.dump(uid_non_core_users, f)\n",
    "    pickle.dump(non_core_user_df,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros((unique_uids, unique_mids))\n",
    "for index, row in non_core_user_df.iterrows():\n",
    "    matrix[int(row['uid'])][int(row['mid'])] = int(1)\n",
    "## write this numpy matrix to a file\n",
    "with open(\"douban_non_core_user_numpy_matrix.pkl\", \"wb\") as f:\n",
    "    pickle.dump(matrix, f)\n",
    "    pickle.dump(unique_uids, f)\n",
    "    pickle.dump(unique_mids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_test_df = core_users_df_nested.groupby(\"uid\").tail(1)\n",
    "# print(len(df_gt_30))\n",
    "support_train_df = core_users_df_nested.drop(core_users_df_nested.groupby('uid').tail(1).index, inplace=False)\n",
    "# print(\"#TEST INSTANCES: \" ,len(support_test_df))\n",
    "# print(\"#TRAIN INSTANCES: \" ,len(support_train_df))\n",
    "assert(len(core_users_df_nested) == len(support_test_df) + len(support_train_df))\n",
    "# print(len(test_df))\n",
    "# print(len(train_df))\n",
    "query_test_df = non_core_user_df.groupby(\"uid\").tail(1)\n",
    "query_train_df = non_core_user_df.drop(non_core_user_df.groupby('uid').tail(1).index, inplace=False)\n",
    "assert(len(non_core_user_df)== len(query_test_df) + len(query_train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ui_dic = {}    \n",
    "for user in range(unique_uids):\n",
    "    train_ui_dic[user] = []\n",
    "for index,row in support_train_df.iterrows():\n",
    "        train_ui_dic[row['uid']].append(row['mid'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE SNIPPET FOR GETTING NESTED CORE USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "SUPPORT TEST DF:          uid    mid  ratings\n",
      "284      613   6169        4\n",
      "485     1639  25721        3\n",
      "576     1977  78976        5\n",
      "760      478  79963        4\n",
      "1420    1264  18677        2\n",
      "...      ...    ...      ...\n",
      "224411   930   3806        5\n",
      "224870  1766  16754        2\n",
      "225896  1617  81268        5\n",
      "226241  1740  72519        5\n",
      "226689  1348   6066        5\n",
      "\n",
      "[442 rows x 3 columns]\n",
      "QUERY TEST DF:\n",
      "          uid    mid  ratings\n",
      "295     1707  87421        5\n",
      "299     1620  18452        4\n",
      "302     1889  74757        4\n",
      "307      447  75026        4\n",
      "343     2010  81065        4\n",
      "...      ...    ...      ...\n",
      "226705  1960   5762        4\n",
      "226748  1219    187        5\n",
      "226841   837  43146        1\n",
      "226930  1067  94845        5\n",
      "227250  1814   1051        5\n",
      "\n",
      "[1770 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "unique_uids_in_support_trian = np.unique(np.array(core_users_df_nested['uid']))\n",
    "unique_uids_in_query_trian = np.unique(query_train_df['uid'])\n",
    "print(len(unique_uids_in_support_trian))\n",
    "support_test_df = support_test_df.loc[support_test_df['uid'].isin(unique_uids_in_support_trian)]\n",
    "print(\"SUPPORT TEST DF:\" ,support_test_df)\n",
    "query_test_df = query_test_df\n",
    "print(\"QUERY TEST DF:\\n\", query_test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE SNIPPET FOR GETTING NESTED CORE USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_train = []\n",
    "for index,row in core_users_df_nested.iterrows():\n",
    "    support_train.append([row['uid'], row['mid'], row['ratings']])\n",
    "query_train = []\n",
    "for index, row in query_train_df.iterrows():\n",
    "    query_train.append([row['uid'], row['mid'], row['ratings']])\n",
    "support_test = []\n",
    "for index, row in support_test_df.iterrows():\n",
    "    support_test.append([row['uid'], row['mid'], row['ratings']])\n",
    "query_test = []\n",
    "for index, row in query_test_df.iterrows():\n",
    "    query_test.append([row['uid'], row['mid'], row['ratings']])\n",
    "user_his_dic = {}\n",
    "for u in train_ui_dic.keys():\n",
    "    user_his_dic[u] = train_ui_dic[u]\n",
    "user_supp_list = np.unique(core_users_df_nested['uid']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"douban_input_for_getting_query_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(support_train, f)\n",
    "    pickle.dump(query_train, f)\n",
    "    pickle.dump(support_test, f)\n",
    "    pickle.dump(query_test, f)\n",
    "    pickle.dump(user_supp_list, f)\n",
    "    pickle.dump(user_his_dic, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generating non core user embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Dataset Info--------\n",
      "split way [threshold] with threshold 30 training_ratio 1.0\n",
      "train set size: support/query 140178/85303\n",
      "test set size: support/query 442/1770\n",
      "Epoch 0 Step 131: Train 4.9505 Reg: 0.6656\n",
      "Test: 1.0248 MAE: 0.8670 RMSE: 1.0123\n",
      "Val: 0.7608 MAE: 0.6879 RMSE: 0.8723\n",
      "Epoch 1 Step 262: Train 0.7518 Reg: 0.6438\n",
      "Test: 0.8767 MAE: 0.7910 RMSE: 0.9363\n",
      "Val: 0.7149 MAE: 0.6695 RMSE: 0.8455\n",
      "Epoch 2 Step 393: Train 0.7090 Reg: 0.9199\n",
      "Test: 0.8109 MAE: 0.7603 RMSE: 0.9005\n",
      "Val: 0.7002 MAE: 0.6668 RMSE: 0.8368\n",
      "Epoch 3 Step 524: Train 0.6759 Reg: 1.1599\n",
      "Test: 0.7526 MAE: 0.7363 RMSE: 0.8675\n",
      "Val: 0.7008 MAE: 0.6649 RMSE: 0.8372\n",
      "Epoch 4 Step 655: Train 0.6423 Reg: 1.3241\n",
      "Test: 0.7137 MAE: 0.7110 RMSE: 0.8448\n",
      "Val: 0.7020 MAE: 0.6666 RMSE: 0.8379\n",
      "Epoch 5 Step 786: Train 0.6057 Reg: 1.4574\n",
      "Test: 0.6848 MAE: 0.6831 RMSE: 0.8275\n",
      "Val: 0.7103 MAE: 0.6696 RMSE: 0.8428\n",
      "Epoch 6 Step 917: Train 0.5686 Reg: 1.5358\n",
      "Test: 0.6357 MAE: 0.6582 RMSE: 0.7973\n",
      "Val: 0.7176 MAE: 0.6717 RMSE: 0.8471\n",
      "Epoch 7 Step 1048: Train 0.5410 Reg: 1.5442\n",
      "Test: 0.6407 MAE: 0.6562 RMSE: 0.8004\n",
      "Val: 0.7181 MAE: 0.6715 RMSE: 0.8474\n",
      "Epoch 8 Step 1179: Train 0.5250 Reg: 1.5207\n",
      "Test: 0.5786 MAE: 0.6257 RMSE: 0.7607\n",
      "Val: 0.7215 MAE: 0.6711 RMSE: 0.8494\n",
      "Epoch 9 Step 1310: Train 0.5136 Reg: 1.4897\n",
      "Test: 0.5725 MAE: 0.6100 RMSE: 0.7566\n",
      "Val: 0.7189 MAE: 0.6676 RMSE: 0.8479\n",
      "Epoch 10 Step 1441: Train 0.5045 Reg: 1.4465\n",
      "Test: 0.5981 MAE: 0.6304 RMSE: 0.7733\n",
      "Val: 0.7240 MAE: 0.6708 RMSE: 0.8509\n",
      "Epoch 11 Step 1572: Train 0.4960 Reg: 1.4157\n",
      "Test: 0.5580 MAE: 0.6148 RMSE: 0.7470\n",
      "Val: 0.7149 MAE: 0.6686 RMSE: 0.8455\n",
      "Epoch 12 Step 1703: Train 0.4866 Reg: 1.3834\n",
      "Test: 0.5463 MAE: 0.5983 RMSE: 0.7391\n",
      "Val: 0.7268 MAE: 0.6730 RMSE: 0.8525\n",
      "Epoch 13 Step 1834: Train 0.4766 Reg: 1.3475\n",
      "Test: 0.5560 MAE: 0.5976 RMSE: 0.7456\n",
      "Val: 0.7242 MAE: 0.6716 RMSE: 0.8510\n",
      "Epoch 14 Step 1965: Train 0.4688 Reg: 1.3198\n",
      "Test: 0.5471 MAE: 0.5995 RMSE: 0.7397\n",
      "Val: 0.7251 MAE: 0.6708 RMSE: 0.8515\n",
      "Epoch 15 Step 2096: Train 0.4604 Reg: 1.2788\n",
      "Test: 0.5156 MAE: 0.5794 RMSE: 0.7181\n",
      "Val: 0.7241 MAE: 0.6707 RMSE: 0.8510\n",
      "Epoch 16 Step 2227: Train 0.4512 Reg: 1.2447\n",
      "Test: 0.5338 MAE: 0.5832 RMSE: 0.7306\n",
      "Val: 0.7288 MAE: 0.6702 RMSE: 0.8537\n",
      "Epoch 17 Step 2358: Train 0.4423 Reg: 1.2096\n",
      "Test: 0.5346 MAE: 0.5829 RMSE: 0.7312\n",
      "Val: 0.7282 MAE: 0.6716 RMSE: 0.8533\n",
      "Epoch 18 Step 2489: Train 0.4347 Reg: 1.1719\n",
      "Test: 0.5388 MAE: 0.5779 RMSE: 0.7340\n",
      "Val: 0.7318 MAE: 0.6714 RMSE: 0.8554\n",
      "Epoch 19 Step 2620: Train 0.4260 Reg: 1.1323\n",
      "Test: 0.5215 MAE: 0.5727 RMSE: 0.7222\n",
      "Val: 0.7325 MAE: 0.6729 RMSE: 0.8558\n",
      "Epoch 20 Step 2751: Train 0.4188 Reg: 1.0946\n",
      "Test: 0.5230 MAE: 0.5754 RMSE: 0.7232\n",
      "Val: 0.7350 MAE: 0.6706 RMSE: 0.8573\n",
      "Epoch 21 Step 2882: Train 0.4115 Reg: 1.0549\n",
      "Test: 0.5300 MAE: 0.5763 RMSE: 0.7280\n",
      "Val: 0.7343 MAE: 0.6724 RMSE: 0.8569\n",
      "Epoch 22 Step 3013: Train 0.4040 Reg: 1.0179\n",
      "Test: 0.5028 MAE: 0.5591 RMSE: 0.7091\n",
      "Val: 0.7352 MAE: 0.6730 RMSE: 0.8575\n",
      "Epoch 23 Step 3144: Train 0.3975 Reg: 0.9800\n",
      "Test: 0.5095 MAE: 0.5547 RMSE: 0.7138\n",
      "Val: 0.7368 MAE: 0.6720 RMSE: 0.8584\n",
      "Epoch 24 Step 3275: Train 0.3899 Reg: 0.9450\n",
      "Test: 0.4979 MAE: 0.5569 RMSE: 0.7056\n",
      "Val: 0.7320 MAE: 0.6706 RMSE: 0.8556\n",
      "Epoch 25 Step 3406: Train 0.3833 Reg: 0.9075\n",
      "Test: 0.5149 MAE: 0.5603 RMSE: 0.7176\n",
      "Val: 0.7351 MAE: 0.6717 RMSE: 0.8574\n",
      "Epoch 26 Step 3537: Train 0.3775 Reg: 0.8732\n",
      "Test: 0.5085 MAE: 0.5489 RMSE: 0.7131\n",
      "Val: 0.7358 MAE: 0.6713 RMSE: 0.8578\n",
      "Epoch 27 Step 3668: Train 0.3712 Reg: 0.8401\n",
      "Test: 0.5011 MAE: 0.5486 RMSE: 0.7079\n",
      "Val: 0.7418 MAE: 0.6730 RMSE: 0.8613\n",
      "Epoch 28 Step 3799: Train 0.3654 Reg: 0.8096\n",
      "Test: 0.4880 MAE: 0.5373 RMSE: 0.6986\n",
      "Val: 0.7399 MAE: 0.6739 RMSE: 0.8602\n",
      "Epoch 29 Step 3930: Train 0.3601 Reg: 0.7789\n",
      "Test: 0.4913 MAE: 0.5335 RMSE: 0.7009\n",
      "Val: 0.7346 MAE: 0.6701 RMSE: 0.8571\n",
      "Epoch 30 Step 4061: Train 0.3552 Reg: 0.7504\n",
      "Test: 0.4876 MAE: 0.5375 RMSE: 0.6983\n",
      "Val: 0.7410 MAE: 0.6724 RMSE: 0.8608\n",
      "Epoch 31 Step 4192: Train 0.3506 Reg: 0.7231\n",
      "Test: 0.4932 MAE: 0.5324 RMSE: 0.7023\n",
      "Val: 0.7444 MAE: 0.6750 RMSE: 0.8628\n",
      "Epoch 32 Step 4323: Train 0.3459 Reg: 0.6975\n",
      "Test: 0.4893 MAE: 0.5350 RMSE: 0.6995\n",
      "Val: 0.7383 MAE: 0.6722 RMSE: 0.8592\n",
      "Epoch 33 Step 4454: Train 0.3424 Reg: 0.6731\n",
      "Test: 0.4835 MAE: 0.5260 RMSE: 0.6953\n",
      "Val: 0.7418 MAE: 0.6721 RMSE: 0.8613\n",
      "Epoch 34 Step 4585: Train 0.3379 Reg: 0.6505\n",
      "Test: 0.4779 MAE: 0.5241 RMSE: 0.6913\n",
      "Val: 0.7410 MAE: 0.6725 RMSE: 0.8608\n",
      "Epoch 35 Step 4716: Train 0.3346 Reg: 0.6287\n",
      "Test: 0.4742 MAE: 0.5169 RMSE: 0.6886\n",
      "Val: 0.7461 MAE: 0.6738 RMSE: 0.8638\n",
      "Epoch 36 Step 4847: Train 0.3312 Reg: 0.6083\n",
      "Test: 0.4932 MAE: 0.5269 RMSE: 0.7023\n",
      "Val: 0.7434 MAE: 0.6744 RMSE: 0.8622\n",
      "Epoch 37 Step 4978: Train 0.3286 Reg: 0.5888\n",
      "Test: 0.4909 MAE: 0.5253 RMSE: 0.7006\n",
      "Val: 0.7464 MAE: 0.6741 RMSE: 0.8639\n",
      "Epoch 38 Step 5109: Train 0.3253 Reg: 0.5708\n",
      "Test: 0.4801 MAE: 0.5186 RMSE: 0.6929\n",
      "Val: 0.7462 MAE: 0.6745 RMSE: 0.8638\n",
      "Epoch 39 Step 5240: Train 0.3225 Reg: 0.5547\n",
      "Test: 0.4791 MAE: 0.5155 RMSE: 0.6922\n",
      "Val: 0.7464 MAE: 0.6743 RMSE: 0.8639\n",
      "Epoch 40 Step 5371: Train 0.3202 Reg: 0.5384\n",
      "Test: 0.4822 MAE: 0.5191 RMSE: 0.6944\n",
      "Val: 0.7460 MAE: 0.6739 RMSE: 0.8637\n",
      "Epoch 41 Step 5502: Train 0.3176 Reg: 0.5234\n",
      "Test: 0.4855 MAE: 0.5223 RMSE: 0.6968\n",
      "Val: 0.7452 MAE: 0.6750 RMSE: 0.8633\n",
      "Epoch 42 Step 5633: Train 0.3154 Reg: 0.5093\n",
      "Test: 0.4769 MAE: 0.5154 RMSE: 0.6906\n",
      "Val: 0.7448 MAE: 0.6737 RMSE: 0.8630\n",
      "Epoch 43 Step 5764: Train 0.3132 Reg: 0.4959\n",
      "Test: 0.4708 MAE: 0.5131 RMSE: 0.6862\n",
      "Val: 0.7465 MAE: 0.6743 RMSE: 0.8640\n",
      "Epoch 44 Step 5895: Train 0.3110 Reg: 0.4835\n",
      "Test: 0.4730 MAE: 0.5125 RMSE: 0.6878\n",
      "Val: 0.7475 MAE: 0.6753 RMSE: 0.8646\n",
      "Epoch 45 Step 6026: Train 0.3089 Reg: 0.4721\n",
      "Test: 0.4839 MAE: 0.5174 RMSE: 0.6957\n",
      "Val: 0.7474 MAE: 0.6748 RMSE: 0.8645\n",
      "Epoch 46 Step 6157: Train 0.3069 Reg: 0.4602\n",
      "Test: 0.4766 MAE: 0.5084 RMSE: 0.6904\n",
      "Val: 0.7454 MAE: 0.6736 RMSE: 0.8633\n",
      "Epoch 47 Step 6288: Train 0.3054 Reg: 0.4496\n",
      "Test: 0.4723 MAE: 0.5051 RMSE: 0.6872\n",
      "Val: 0.7488 MAE: 0.6755 RMSE: 0.8653\n",
      "Epoch 48 Step 6419: Train 0.3034 Reg: 0.4396\n",
      "Test: 0.4797 MAE: 0.5107 RMSE: 0.6926\n",
      "Val: 0.7464 MAE: 0.6752 RMSE: 0.8640\n",
      "Epoch 49 Step 6550: Train 0.3019 Reg: 0.4299\n",
      "Test: 0.4739 MAE: 0.5093 RMSE: 0.6884\n",
      "Val: 0.7477 MAE: 0.6750 RMSE: 0.8647\n",
      "Epoch 50 Step 6681: Train 0.3002 Reg: 0.4207\n",
      "Test: 0.4679 MAE: 0.5051 RMSE: 0.6840\n",
      "Val: 0.7508 MAE: 0.6747 RMSE: 0.8665\n",
      "Epoch 51 Step 6812: Train 0.2988 Reg: 0.4120\n",
      "Test: 0.4760 MAE: 0.5114 RMSE: 0.6899\n",
      "Val: 0.7484 MAE: 0.6754 RMSE: 0.8651\n",
      "Epoch 52 Step 6943: Train 0.2970 Reg: 0.4040\n",
      "Test: 0.4803 MAE: 0.5121 RMSE: 0.6930\n",
      "Val: 0.7478 MAE: 0.6756 RMSE: 0.8647\n",
      "Epoch 53 Step 7074: Train 0.2956 Reg: 0.3959\n",
      "Test: 0.4760 MAE: 0.5057 RMSE: 0.6899\n",
      "Val: 0.7508 MAE: 0.6752 RMSE: 0.8665\n",
      "Epoch 54 Step 7205: Train 0.2943 Reg: 0.3885\n",
      "Test: 0.4771 MAE: 0.5109 RMSE: 0.6907\n",
      "Val: 0.7491 MAE: 0.6759 RMSE: 0.8655\n",
      "Epoch 55 Step 7336: Train 0.2931 Reg: 0.3815\n",
      "Test: 0.4727 MAE: 0.5062 RMSE: 0.6875\n",
      "Val: 0.7502 MAE: 0.6753 RMSE: 0.8661\n",
      "Epoch 56 Step 7467: Train 0.2917 Reg: 0.3749\n",
      "Test: 0.4729 MAE: 0.5059 RMSE: 0.6877\n",
      "Val: 0.7508 MAE: 0.6758 RMSE: 0.8665\n",
      "Epoch 57 Step 7598: Train 0.2905 Reg: 0.3685\n",
      "Test: 0.4739 MAE: 0.5067 RMSE: 0.6884\n",
      "Val: 0.7486 MAE: 0.6746 RMSE: 0.8652\n",
      "Epoch 58 Step 7729: Train 0.2892 Reg: 0.3624\n",
      "Test: 0.4675 MAE: 0.5000 RMSE: 0.6837\n",
      "Val: 0.7512 MAE: 0.6744 RMSE: 0.8667\n",
      "Epoch 59 Step 7860: Train 0.2881 Reg: 0.3569\n",
      "Test: 0.4732 MAE: 0.5055 RMSE: 0.6879\n",
      "Val: 0.7514 MAE: 0.6756 RMSE: 0.8668\n",
      "Epoch 60 Step 7991: Train 0.2870 Reg: 0.3514\n",
      "Test: 0.4771 MAE: 0.5052 RMSE: 0.6907\n",
      "Val: 0.7506 MAE: 0.6755 RMSE: 0.8664\n",
      "Epoch 61 Step 8122: Train 0.2860 Reg: 0.3462\n",
      "Test: 0.4766 MAE: 0.5064 RMSE: 0.6904\n",
      "Val: 0.7504 MAE: 0.6760 RMSE: 0.8663\n",
      "Epoch 62 Step 8253: Train 0.2849 Reg: 0.3412\n",
      "Test: 0.4740 MAE: 0.5059 RMSE: 0.6885\n",
      "Val: 0.7511 MAE: 0.6758 RMSE: 0.8666\n",
      "Epoch 63 Step 8384: Train 0.2839 Reg: 0.3366\n",
      "Test: 0.4778 MAE: 0.5072 RMSE: 0.6912\n",
      "Val: 0.7506 MAE: 0.6762 RMSE: 0.8664\n",
      "Epoch 64 Step 8515: Train 0.2831 Reg: 0.3321\n",
      "Test: 0.4789 MAE: 0.5078 RMSE: 0.6920\n",
      "Val: 0.7516 MAE: 0.6766 RMSE: 0.8669\n",
      "Epoch 65 Step 8646: Train 0.2820 Reg: 0.3278\n",
      "Test: 0.4745 MAE: 0.5053 RMSE: 0.6888\n",
      "Val: 0.7511 MAE: 0.6758 RMSE: 0.8667\n",
      "Epoch 66 Step 8777: Train 0.2811 Reg: 0.3239\n",
      "Test: 0.4781 MAE: 0.5075 RMSE: 0.6914\n",
      "Val: 0.7509 MAE: 0.6765 RMSE: 0.8666\n",
      "Epoch 67 Step 8908: Train 0.2802 Reg: 0.3200\n",
      "Test: 0.4753 MAE: 0.5050 RMSE: 0.6894\n",
      "Val: 0.7518 MAE: 0.6761 RMSE: 0.8671\n",
      "Epoch 68 Step 9039: Train 0.2794 Reg: 0.3164\n",
      "Test: 0.4775 MAE: 0.5060 RMSE: 0.6910\n",
      "Val: 0.7523 MAE: 0.6766 RMSE: 0.8673\n",
      "Epoch 69 Step 9170: Train 0.2785 Reg: 0.3129\n",
      "Test: 0.4739 MAE: 0.5037 RMSE: 0.6884\n",
      "Val: 0.7521 MAE: 0.6757 RMSE: 0.8672\n",
      "Epoch 70 Step 9301: Train 0.2778 Reg: 0.3097\n",
      "Test: 0.4726 MAE: 0.5028 RMSE: 0.6874\n",
      "Val: 0.7530 MAE: 0.6761 RMSE: 0.8678\n",
      "Epoch 71 Step 9432: Train 0.2769 Reg: 0.3066\n",
      "Test: 0.4745 MAE: 0.5038 RMSE: 0.6888\n",
      "Val: 0.7521 MAE: 0.6761 RMSE: 0.8672\n",
      "Epoch 72 Step 9563: Train 0.2763 Reg: 0.3037\n",
      "Test: 0.4754 MAE: 0.5039 RMSE: 0.6895\n",
      "Val: 0.7517 MAE: 0.6760 RMSE: 0.8670\n",
      "Epoch 73 Step 9694: Train 0.2757 Reg: 0.3008\n",
      "Test: 0.4779 MAE: 0.5057 RMSE: 0.6913\n",
      "Val: 0.7520 MAE: 0.6762 RMSE: 0.8672\n",
      "Epoch 74 Step 9825: Train 0.2750 Reg: 0.2982\n",
      "Test: 0.4766 MAE: 0.5048 RMSE: 0.6904\n",
      "Val: 0.7519 MAE: 0.6764 RMSE: 0.8671\n",
      "Epoch 75 Step 9956: Train 0.2744 Reg: 0.2955\n",
      "Test: 0.4724 MAE: 0.5025 RMSE: 0.6873\n",
      "Val: 0.7531 MAE: 0.6761 RMSE: 0.8678\n",
      "Epoch 76 Step 10087: Train 0.2738 Reg: 0.2932\n",
      "Test: 0.4762 MAE: 0.5045 RMSE: 0.6901\n",
      "Val: 0.7521 MAE: 0.6763 RMSE: 0.8673\n",
      "Epoch 77 Step 10218: Train 0.2731 Reg: 0.2908\n",
      "Test: 0.4753 MAE: 0.5043 RMSE: 0.6895\n",
      "Val: 0.7520 MAE: 0.6763 RMSE: 0.8672\n",
      "Epoch 78 Step 10349: Train 0.2726 Reg: 0.2887\n",
      "Test: 0.4733 MAE: 0.5036 RMSE: 0.6880\n",
      "Val: 0.7522 MAE: 0.6761 RMSE: 0.8673\n",
      "Epoch 79 Step 10480: Train 0.2720 Reg: 0.2866\n",
      "Test: 0.4740 MAE: 0.5036 RMSE: 0.6884\n",
      "Val: 0.7526 MAE: 0.6762 RMSE: 0.8675\n",
      "Epoch 80 Step 10611: Train 0.2715 Reg: 0.2846\n",
      "Test: 0.4764 MAE: 0.5051 RMSE: 0.6902\n",
      "Val: 0.7521 MAE: 0.6763 RMSE: 0.8672\n",
      "Epoch 81 Step 10742: Train 0.2710 Reg: 0.2828\n",
      "Test: 0.4756 MAE: 0.5047 RMSE: 0.6896\n",
      "Val: 0.7521 MAE: 0.6763 RMSE: 0.8672\n",
      "Epoch 82 Step 10873: Train 0.2706 Reg: 0.2809\n",
      "Test: 0.4739 MAE: 0.5030 RMSE: 0.6884\n",
      "Val: 0.7522 MAE: 0.6760 RMSE: 0.8673\n",
      "Epoch 83 Step 11004: Train 0.2701 Reg: 0.2791\n",
      "Test: 0.4734 MAE: 0.5024 RMSE: 0.6880\n",
      "Val: 0.7524 MAE: 0.6761 RMSE: 0.8674\n",
      "Epoch 84 Step 11135: Train 0.2696 Reg: 0.2775\n",
      "Test: 0.4718 MAE: 0.5021 RMSE: 0.6869\n",
      "Val: 0.7524 MAE: 0.6758 RMSE: 0.8674\n",
      "Epoch 85 Step 11266: Train 0.2692 Reg: 0.2760\n",
      "Test: 0.4757 MAE: 0.5049 RMSE: 0.6897\n",
      "Val: 0.7521 MAE: 0.6763 RMSE: 0.8672\n",
      "Epoch 86 Step 11397: Train 0.2688 Reg: 0.2745\n",
      "Test: 0.4751 MAE: 0.5050 RMSE: 0.6893\n",
      "Val: 0.7524 MAE: 0.6763 RMSE: 0.8674\n",
      "Epoch 87 Step 11528: Train 0.2684 Reg: 0.2731\n",
      "Test: 0.4726 MAE: 0.5031 RMSE: 0.6875\n",
      "Val: 0.7528 MAE: 0.6761 RMSE: 0.8676\n",
      "Epoch 88 Step 11659: Train 0.2681 Reg: 0.2717\n",
      "Test: 0.4740 MAE: 0.5038 RMSE: 0.6885\n",
      "Val: 0.7526 MAE: 0.6760 RMSE: 0.8675\n",
      "Epoch 89 Step 11790: Train 0.2677 Reg: 0.2704\n",
      "Test: 0.4761 MAE: 0.5052 RMSE: 0.6900\n",
      "Val: 0.7524 MAE: 0.6763 RMSE: 0.8674\n",
      "Epoch 90 Step 11921: Train 0.2674 Reg: 0.2692\n",
      "Test: 0.4750 MAE: 0.5043 RMSE: 0.6892\n",
      "Val: 0.7522 MAE: 0.6762 RMSE: 0.8673\n",
      "Epoch 91 Step 12052: Train 0.2671 Reg: 0.2681\n",
      "Test: 0.4741 MAE: 0.5040 RMSE: 0.6886\n",
      "Val: 0.7525 MAE: 0.6761 RMSE: 0.8674\n",
      "Epoch 92 Step 12183: Train 0.2668 Reg: 0.2669\n",
      "Test: 0.4739 MAE: 0.5042 RMSE: 0.6884\n",
      "Val: 0.7526 MAE: 0.6762 RMSE: 0.8675\n",
      "Epoch 93 Step 12314: Train 0.2665 Reg: 0.2659\n",
      "Test: 0.4741 MAE: 0.5039 RMSE: 0.6885\n",
      "Val: 0.7528 MAE: 0.6762 RMSE: 0.8677\n",
      "Epoch 94 Step 12445: Train 0.2662 Reg: 0.2649\n",
      "Test: 0.4744 MAE: 0.5046 RMSE: 0.6888\n",
      "Val: 0.7525 MAE: 0.6761 RMSE: 0.8675\n",
      "Epoch 95 Step 12576: Train 0.2659 Reg: 0.2640\n",
      "Test: 0.4747 MAE: 0.5046 RMSE: 0.6890\n",
      "Val: 0.7524 MAE: 0.6762 RMSE: 0.8674\n",
      "Epoch 96 Step 12707: Train 0.2656 Reg: 0.2631\n",
      "Test: 0.4743 MAE: 0.5044 RMSE: 0.6887\n",
      "Val: 0.7526 MAE: 0.6761 RMSE: 0.8675\n",
      "Epoch 97 Step 12838: Train 0.2654 Reg: 0.2622\n",
      "Test: 0.4744 MAE: 0.5046 RMSE: 0.6888\n",
      "Val: 0.7526 MAE: 0.6762 RMSE: 0.8675\n",
      "Epoch 98 Step 12969: Train 0.2652 Reg: 0.2614\n",
      "Test: 0.4729 MAE: 0.5036 RMSE: 0.6877\n",
      "Val: 0.7527 MAE: 0.6760 RMSE: 0.8676\n",
      "Epoch 99 Step 13100: Train 0.2649 Reg: 0.2606\n",
      "Test: 0.4740 MAE: 0.5042 RMSE: 0.6885\n",
      "Val: 0.7528 MAE: 0.6761 RMSE: 0.8677\n",
      "-------Dataset Info--------\n",
      "split way [threshold] with threshold 30 training_ratio 1.0\n",
      "train set size: support/query 140178/85303\n",
      "test set size: support/query 442/1770\n",
      "Epoch 0: TrainLoss 0.9493 RecLoss: 0.0000 (left: 0:37:55)\n",
      "TestLoss: 0.9113 MAE: 0.7686 RMSE: 0.9546\n",
      "ValLoss: 0.7356 MAE: 0.6798 RMSE: 0.8576\n",
      "Epoch 1: TrainLoss 0.7780 RecLoss: 0.0000 (left: 0:38:40)\n",
      "TestLoss: 0.8913 MAE: 0.7583 RMSE: 0.9441\n",
      "ValLoss: 0.7142 MAE: 0.6701 RMSE: 0.8451\n",
      "Epoch 2: TrainLoss 0.7623 RecLoss: 0.0000 (left: 0:38:01)\n",
      "TestLoss: 0.8902 MAE: 0.7585 RMSE: 0.9435\n",
      "ValLoss: 0.7091 MAE: 0.6677 RMSE: 0.8421\n",
      "Epoch 3: TrainLoss 0.7569 RecLoss: 0.0000 (left: 0:38:19)\n",
      "TestLoss: 0.8691 MAE: 0.7425 RMSE: 0.9323\n",
      "ValLoss: 0.7075 MAE: 0.6632 RMSE: 0.8411\n",
      "Epoch 4: TrainLoss 0.7539 RecLoss: 0.0000 (left: 0:37:55)\n",
      "TestLoss: 0.8886 MAE: 0.7563 RMSE: 0.9426\n",
      "ValLoss: 0.7065 MAE: 0.6654 RMSE: 0.8406\n",
      "Epoch 5: TrainLoss 0.7525 RecLoss: 0.0000 (left: 0:37:18)\n",
      "TestLoss: 0.8938 MAE: 0.7598 RMSE: 0.9454\n",
      "ValLoss: 0.7071 MAE: 0.6660 RMSE: 0.8409\n",
      "Epoch 6: TrainLoss 0.7521 RecLoss: 0.0000 (left: 0:37:48)\n",
      "TestLoss: 0.9248 MAE: 0.7791 RMSE: 0.9616\n",
      "ValLoss: 0.7158 MAE: 0.6737 RMSE: 0.8461\n",
      "Epoch 7: TrainLoss 0.7520 RecLoss: 0.0000 (left: 0:38:24)\n",
      "TestLoss: 0.8808 MAE: 0.7457 RMSE: 0.9385\n",
      "ValLoss: 0.7071 MAE: 0.6623 RMSE: 0.8409\n",
      "Epoch 8: TrainLoss 0.7500 RecLoss: 0.0000 (left: 0:39:28)\n",
      "TestLoss: 0.9088 MAE: 0.7661 RMSE: 0.9533\n",
      "ValLoss: 0.7089 MAE: 0.6672 RMSE: 0.8419\n",
      "Epoch 9: TrainLoss 0.7495 RecLoss: 0.0000 (left: 0:40:01)\n",
      "TestLoss: 0.8760 MAE: 0.7344 RMSE: 0.9360\n",
      "ValLoss: 0.7166 MAE: 0.6636 RMSE: 0.8465\n",
      "Epoch 10: TrainLoss 0.7516 RecLoss: 0.0000 (left: 0:40:03)\n",
      "TestLoss: 0.9054 MAE: 0.7612 RMSE: 0.9515\n",
      "ValLoss: 0.7080 MAE: 0.6656 RMSE: 0.8414\n",
      "Epoch 11: TrainLoss 0.7504 RecLoss: 0.0000 (left: 0:39:48)\n",
      "TestLoss: 0.9019 MAE: 0.7564 RMSE: 0.9497\n",
      "ValLoss: 0.7072 MAE: 0.6639 RMSE: 0.8410\n",
      "Epoch 12: TrainLoss 0.7482 RecLoss: 0.0000 (left: 0:39:21)\n",
      "TestLoss: 0.8906 MAE: 0.7469 RMSE: 0.9437\n",
      "ValLoss: 0.7091 MAE: 0.6627 RMSE: 0.8421\n",
      "Epoch 13: TrainLoss 0.7493 RecLoss: 0.0000 (left: 0:38:44)\n",
      "TestLoss: 0.9082 MAE: 0.7594 RMSE: 0.9530\n",
      "ValLoss: 0.7076 MAE: 0.6645 RMSE: 0.8412\n",
      "Epoch 14: TrainLoss 0.7488 RecLoss: 0.0000 (left: 0:38:07)\n",
      "TestLoss: 0.9296 MAE: 0.7730 RMSE: 0.9642\n",
      "ValLoss: 0.7112 MAE: 0.6691 RMSE: 0.8433\n",
      "Epoch 15: TrainLoss 0.7481 RecLoss: 0.0000 (left: 0:37:13)\n",
      "TestLoss: 0.9156 MAE: 0.7617 RMSE: 0.9569\n",
      "ValLoss: 0.7072 MAE: 0.6645 RMSE: 0.8410\n",
      "Epoch 16: TrainLoss 0.7486 RecLoss: 0.0000 (left: 0:36:25)\n",
      "TestLoss: 0.8949 MAE: 0.7441 RMSE: 0.9460\n",
      "ValLoss: 0.7125 MAE: 0.6625 RMSE: 0.8441\n",
      "Epoch 17: TrainLoss 0.7479 RecLoss: 0.0000 (left: 0:35:53)\n",
      "TestLoss: 0.8948 MAE: 0.7442 RMSE: 0.9459\n",
      "ValLoss: 0.7128 MAE: 0.6628 RMSE: 0.8443\n",
      "Epoch 18: TrainLoss 0.7484 RecLoss: 0.0000 (left: 0:35:26)\n",
      "TestLoss: 0.9288 MAE: 0.7703 RMSE: 0.9638\n",
      "ValLoss: 0.7096 MAE: 0.6673 RMSE: 0.8424\n",
      "Epoch 19: TrainLoss 0.7487 RecLoss: 0.0000 (left: 0:35:07)\n",
      "TestLoss: 0.9134 MAE: 0.7581 RMSE: 0.9557\n",
      "ValLoss: 0.7084 MAE: 0.6639 RMSE: 0.8417\n",
      "Epoch 20: TrainLoss 0.7473 RecLoss: 0.0000 (left: 0:34:36)\n",
      "TestLoss: 0.9088 MAE: 0.7541 RMSE: 0.9533\n",
      "ValLoss: 0.7090 MAE: 0.6631 RMSE: 0.8420\n",
      "Epoch 21: TrainLoss 0.7469 RecLoss: 0.0000 (left: 0:34:13)\n",
      "TestLoss: 0.9072 MAE: 0.7517 RMSE: 0.9525\n",
      "ValLoss: 0.7085 MAE: 0.6626 RMSE: 0.8417\n",
      "Epoch 22: TrainLoss 0.7475 RecLoss: 0.0000 (left: 0:33:55)\n",
      "TestLoss: 0.9123 MAE: 0.7544 RMSE: 0.9552\n",
      "ValLoss: 0.7089 MAE: 0.6632 RMSE: 0.8419\n",
      "Epoch 23: TrainLoss 0.7476 RecLoss: 0.0000 (left: 0:33:31)\n",
      "TestLoss: 0.9074 MAE: 0.7507 RMSE: 0.9526\n",
      "ValLoss: 0.7106 MAE: 0.6631 RMSE: 0.8430\n",
      "Epoch 24: TrainLoss 0.7470 RecLoss: 0.0000 (left: 0:33:07)\n",
      "TestLoss: 0.9119 MAE: 0.7547 RMSE: 0.9549\n",
      "ValLoss: 0.7083 MAE: 0.6628 RMSE: 0.8416\n",
      "Epoch 25: TrainLoss 0.7475 RecLoss: 0.0000 (left: 0:32:47)\n",
      "TestLoss: 0.9126 MAE: 0.7534 RMSE: 0.9553\n",
      "ValLoss: 0.7099 MAE: 0.6632 RMSE: 0.8426\n",
      "Epoch 26: TrainLoss 0.7478 RecLoss: 0.0000 (left: 0:32:22)\n",
      "TestLoss: 0.9265 MAE: 0.7631 RMSE: 0.9625\n",
      "ValLoss: 0.7085 MAE: 0.6649 RMSE: 0.8417\n",
      "Epoch 27: TrainLoss 0.7464 RecLoss: 0.0000 (left: 0:31:49)\n",
      "TestLoss: 0.9013 MAE: 0.7423 RMSE: 0.9494\n",
      "ValLoss: 0.7175 MAE: 0.6636 RMSE: 0.8471\n",
      "Epoch 28: TrainLoss 0.7471 RecLoss: 0.0000 (left: 0:31:12)\n",
      "TestLoss: 0.9207 MAE: 0.7586 RMSE: 0.9595\n",
      "ValLoss: 0.7080 MAE: 0.6633 RMSE: 0.8414\n",
      "Epoch 29: TrainLoss 0.7468 RecLoss: 0.0000 (left: 0:30:32)\n",
      "TestLoss: 0.9098 MAE: 0.7512 RMSE: 0.9538\n",
      "ValLoss: 0.7100 MAE: 0.6626 RMSE: 0.8426\n",
      "Epoch 30: TrainLoss 0.7465 RecLoss: 0.0000 (left: 0:29:57)\n",
      "TestLoss: 0.9104 MAE: 0.7509 RMSE: 0.9542\n",
      "ValLoss: 0.7101 MAE: 0.6626 RMSE: 0.8427\n",
      "Epoch 31: TrainLoss 0.7465 RecLoss: 0.0000 (left: 0:29:23)\n",
      "TestLoss: 0.9642 MAE: 0.7880 RMSE: 0.9819\n",
      "ValLoss: 0.7188 MAE: 0.6748 RMSE: 0.8478\n",
      "Epoch 32: TrainLoss 0.7472 RecLoss: 0.0000 (left: 0:28:54)\n",
      "TestLoss: 0.9073 MAE: 0.7487 RMSE: 0.9525\n",
      "ValLoss: 0.7124 MAE: 0.6625 RMSE: 0.8440\n",
      "Epoch 33: TrainLoss 0.7467 RecLoss: 0.0000 (left: 0:28:26)\n",
      "TestLoss: 0.9626 MAE: 0.7864 RMSE: 0.9811\n",
      "ValLoss: 0.7173 MAE: 0.6737 RMSE: 0.8469\n",
      "Epoch 34: TrainLoss 0.7473 RecLoss: 0.0000 (left: 0:27:57)\n",
      "TestLoss: 0.9157 MAE: 0.7519 RMSE: 0.9569\n",
      "ValLoss: 0.7124 MAE: 0.6630 RMSE: 0.8440\n",
      "Epoch 35: TrainLoss 0.7471 RecLoss: 0.0000 (left: 0:27:36)\n",
      "TestLoss: 0.9439 MAE: 0.7753 RMSE: 0.9715\n",
      "ValLoss: 0.7115 MAE: 0.6688 RMSE: 0.8435\n",
      "Epoch 36: TrainLoss 0.7471 RecLoss: 0.0000 (left: 0:27:13)\n",
      "TestLoss: 0.9558 MAE: 0.7831 RMSE: 0.9776\n",
      "ValLoss: 0.7154 MAE: 0.6724 RMSE: 0.8458\n",
      "Epoch 37: TrainLoss 0.7466 RecLoss: 0.0000 (left: 0:26:53)\n",
      "TestLoss: 0.9783 MAE: 0.7958 RMSE: 0.9891\n",
      "ValLoss: 0.7238 MAE: 0.6785 RMSE: 0.8507\n",
      "Epoch 38: TrainLoss 0.7471 RecLoss: 0.0000 (left: 0:26:27)\n",
      "TestLoss: 0.9261 MAE: 0.7621 RMSE: 0.9624\n",
      "ValLoss: 0.7081 MAE: 0.6639 RMSE: 0.8415\n",
      "Epoch 39: TrainLoss 0.7467 RecLoss: 0.0000 (left: 0:26:03)\n",
      "TestLoss: 1.0078 MAE: 0.8135 RMSE: 1.0039\n",
      "ValLoss: 0.7392 MAE: 0.6894 RMSE: 0.8598\n",
      "Epoch 40: TrainLoss 0.7477 RecLoss: 0.0000 (left: 0:25:43)\n",
      "TestLoss: 0.9171 MAE: 0.7547 RMSE: 0.9576\n",
      "ValLoss: 0.7087 MAE: 0.6626 RMSE: 0.8418\n",
      "Epoch 41: TrainLoss 0.7460 RecLoss: 0.0000 (left: 0:25:21)\n",
      "TestLoss: 0.9358 MAE: 0.7701 RMSE: 0.9674\n",
      "ValLoss: 0.7106 MAE: 0.6671 RMSE: 0.8430\n",
      "Epoch 42: TrainLoss 0.7453 RecLoss: 0.0000 (left: 0:24:56)\n",
      "TestLoss: 0.9533 MAE: 0.7803 RMSE: 0.9764\n",
      "ValLoss: 0.7136 MAE: 0.6706 RMSE: 0.8447\n",
      "Epoch 43: TrainLoss 0.7464 RecLoss: 0.0000 (left: 0:24:26)\n",
      "TestLoss: 0.9404 MAE: 0.7718 RMSE: 0.9697\n",
      "ValLoss: 0.7103 MAE: 0.6672 RMSE: 0.8428\n",
      "Epoch 44: TrainLoss 0.7455 RecLoss: 0.0000 (left: 0:23:59)\n",
      "TestLoss: 0.9289 MAE: 0.7628 RMSE: 0.9638\n",
      "ValLoss: 0.7076 MAE: 0.6639 RMSE: 0.8412\n",
      "Epoch 45: TrainLoss 0.7461 RecLoss: 0.0000 (left: 0:23:31)\n",
      "TestLoss: 0.9177 MAE: 0.7535 RMSE: 0.9580\n",
      "ValLoss: 0.7100 MAE: 0.6628 RMSE: 0.8426\n",
      "Epoch 46: TrainLoss 0.7463 RecLoss: 0.0000 (left: 0:23:01)\n",
      "TestLoss: 0.9501 MAE: 0.7785 RMSE: 0.9748\n",
      "ValLoss: 0.7128 MAE: 0.6698 RMSE: 0.8443\n",
      "Epoch 47: TrainLoss 0.7456 RecLoss: 0.0000 (left: 0:22:34)\n",
      "TestLoss: 0.9333 MAE: 0.7657 RMSE: 0.9661\n",
      "ValLoss: 0.7084 MAE: 0.6647 RMSE: 0.8417\n",
      "Epoch 48: TrainLoss 0.7478 RecLoss: 0.0000 (left: 0:22:06)\n",
      "TestLoss: 0.9404 MAE: 0.7714 RMSE: 0.9697\n",
      "ValLoss: 0.7103 MAE: 0.6670 RMSE: 0.8428\n",
      "Epoch 49: TrainLoss 0.7460 RecLoss: 0.0000 (left: 0:21:39)\n",
      "TestLoss: 0.9162 MAE: 0.7522 RMSE: 0.9572\n",
      "ValLoss: 0.7106 MAE: 0.6625 RMSE: 0.8430\n",
      "Epoch 50: TrainLoss 0.7456 RecLoss: 0.0000 (left: 0:21:14)\n",
      "TestLoss: 0.9292 MAE: 0.7621 RMSE: 0.9639\n",
      "ValLoss: 0.7084 MAE: 0.6640 RMSE: 0.8417\n",
      "Epoch 51: TrainLoss 0.7450 RecLoss: 0.0000 (left: 0:20:49)\n",
      "TestLoss: 0.9361 MAE: 0.7681 RMSE: 0.9675\n",
      "ValLoss: 0.7091 MAE: 0.6659 RMSE: 0.8421\n",
      "Epoch 52: TrainLoss 0.7463 RecLoss: 0.0000 (left: 0:20:25)\n",
      "TestLoss: 0.9290 MAE: 0.7630 RMSE: 0.9638\n",
      "ValLoss: 0.7081 MAE: 0.6641 RMSE: 0.8415\n",
      "Epoch 53: TrainLoss 0.7452 RecLoss: 0.0000 (left: 0:20:00)\n",
      "TestLoss: 0.9331 MAE: 0.7655 RMSE: 0.9660\n",
      "ValLoss: 0.7089 MAE: 0.6650 RMSE: 0.8420\n",
      "Epoch 54: TrainLoss 0.7462 RecLoss: 0.0000 (left: 0:19:37)\n",
      "TestLoss: 0.9502 MAE: 0.7775 RMSE: 0.9748\n",
      "ValLoss: 0.7121 MAE: 0.6697 RMSE: 0.8439\n",
      "Epoch 55: TrainLoss 0.7458 RecLoss: 0.0000 (left: 0:19:14)\n",
      "TestLoss: 0.9313 MAE: 0.7645 RMSE: 0.9650\n",
      "ValLoss: 0.7080 MAE: 0.6645 RMSE: 0.8415\n",
      "Epoch 56: TrainLoss 0.7455 RecLoss: 0.0000 (left: 0:18:50)\n",
      "TestLoss: 0.9335 MAE: 0.7664 RMSE: 0.9662\n",
      "ValLoss: 0.7087 MAE: 0.6652 RMSE: 0.8418\n",
      "Epoch 57: TrainLoss 0.7448 RecLoss: 0.0000 (left: 0:18:25)\n",
      "TestLoss: 0.9611 MAE: 0.7839 RMSE: 0.9804\n",
      "ValLoss: 0.7155 MAE: 0.6723 RMSE: 0.8459\n",
      "Epoch 58: TrainLoss 0.7458 RecLoss: 0.0000 (left: 0:17:57)\n",
      "TestLoss: 0.9271 MAE: 0.7602 RMSE: 0.9629\n",
      "ValLoss: 0.7076 MAE: 0.6631 RMSE: 0.8412\n",
      "Epoch 59: TrainLoss 0.7449 RecLoss: 0.0000 (left: 0:17:29)\n",
      "TestLoss: 0.9511 MAE: 0.7779 RMSE: 0.9752\n",
      "ValLoss: 0.7128 MAE: 0.6697 RMSE: 0.8443\n",
      "Epoch 60: TrainLoss 0.7457 RecLoss: 0.0000 (left: 0:17:04)\n",
      "TestLoss: 0.9359 MAE: 0.7664 RMSE: 0.9674\n",
      "ValLoss: 0.7084 MAE: 0.6651 RMSE: 0.8417\n",
      "Epoch 61: TrainLoss 0.7447 RecLoss: 0.0000 (left: 0:16:37)\n",
      "TestLoss: 0.9237 MAE: 0.7572 RMSE: 0.9611\n",
      "ValLoss: 0.7087 MAE: 0.6630 RMSE: 0.8419\n",
      "Epoch 62: TrainLoss 0.7447 RecLoss: 0.0000 (left: 0:16:10)\n",
      "TestLoss: 0.9234 MAE: 0.7573 RMSE: 0.9609\n",
      "ValLoss: 0.7080 MAE: 0.6630 RMSE: 0.8414\n",
      "Epoch 63: TrainLoss 0.7449 RecLoss: 0.0000 (left: 0:15:41)\n",
      "TestLoss: 0.9289 MAE: 0.7628 RMSE: 0.9638\n",
      "ValLoss: 0.7082 MAE: 0.6643 RMSE: 0.8415\n",
      "Epoch 64: TrainLoss 0.7446 RecLoss: 0.0000 (left: 0:15:16)\n",
      "TestLoss: 0.9176 MAE: 0.7540 RMSE: 0.9579\n",
      "ValLoss: 0.7091 MAE: 0.6625 RMSE: 0.8421\n",
      "Epoch 65: TrainLoss 0.7472 RecLoss: 0.0000 (left: 0:14:50)\n",
      "TestLoss: 0.9331 MAE: 0.7639 RMSE: 0.9659\n",
      "ValLoss: 0.7082 MAE: 0.6644 RMSE: 0.8415\n",
      "Epoch 66: TrainLoss 0.7441 RecLoss: 0.0000 (left: 0:14:24)\n",
      "TestLoss: 0.9219 MAE: 0.7575 RMSE: 0.9602\n",
      "ValLoss: 0.7077 MAE: 0.6627 RMSE: 0.8412\n",
      "Epoch 67: TrainLoss 0.7441 RecLoss: 0.0000 (left: 0:13:59)\n",
      "TestLoss: 0.9276 MAE: 0.7614 RMSE: 0.9631\n",
      "ValLoss: 0.7074 MAE: 0.6635 RMSE: 0.8411\n",
      "Epoch 68: TrainLoss 0.7443 RecLoss: 0.0000 (left: 0:13:33)\n",
      "TestLoss: 0.9541 MAE: 0.7813 RMSE: 0.9768\n",
      "ValLoss: 0.7143 MAE: 0.6715 RMSE: 0.8452\n",
      "Epoch 69: TrainLoss 0.7454 RecLoss: 0.0000 (left: 0:13:09)\n",
      "TestLoss: 0.9409 MAE: 0.7714 RMSE: 0.9700\n",
      "ValLoss: 0.7100 MAE: 0.6671 RMSE: 0.8426\n",
      "Epoch 70: TrainLoss 0.7443 RecLoss: 0.0000 (left: 0:12:43)\n",
      "TestLoss: 0.9574 MAE: 0.7810 RMSE: 0.9785\n",
      "ValLoss: 0.7133 MAE: 0.6707 RMSE: 0.8446\n",
      "Epoch 71: TrainLoss 0.7443 RecLoss: 0.0000 (left: 0:12:17)\n",
      "TestLoss: 0.9364 MAE: 0.7693 RMSE: 0.9677\n",
      "ValLoss: 0.7100 MAE: 0.6667 RMSE: 0.8426\n",
      "Epoch 72: TrainLoss 0.7449 RecLoss: 0.0000 (left: 0:11:52)\n",
      "TestLoss: 0.9448 MAE: 0.7733 RMSE: 0.9720\n",
      "ValLoss: 0.7098 MAE: 0.6672 RMSE: 0.8425\n",
      "Epoch 73: TrainLoss 0.7443 RecLoss: 0.0000 (left: 0:11:25)\n",
      "TestLoss: 0.9424 MAE: 0.7700 RMSE: 0.9708\n",
      "ValLoss: 0.7088 MAE: 0.6658 RMSE: 0.8419\n",
      "Epoch 74: TrainLoss 0.7457 RecLoss: 0.0000 (left: 0:10:59)\n",
      "TestLoss: 0.9327 MAE: 0.7644 RMSE: 0.9658\n",
      "ValLoss: 0.7076 MAE: 0.6641 RMSE: 0.8412\n",
      "Epoch 75: TrainLoss 0.7447 RecLoss: 0.0000 (left: 0:10:33)\n",
      "TestLoss: 0.9419 MAE: 0.7719 RMSE: 0.9705\n",
      "ValLoss: 0.7100 MAE: 0.6672 RMSE: 0.8426\n",
      "Epoch 76: TrainLoss 0.7441 RecLoss: 0.0000 (left: 0:10:07)\n",
      "TestLoss: 0.9475 MAE: 0.7743 RMSE: 0.9734\n",
      "ValLoss: 0.7102 MAE: 0.6676 RMSE: 0.8427\n",
      "Epoch 77: TrainLoss 0.7445 RecLoss: 0.0000 (left: 0:09:40)\n",
      "TestLoss: 0.9191 MAE: 0.7546 RMSE: 0.9587\n",
      "ValLoss: 0.7085 MAE: 0.6624 RMSE: 0.8417\n",
      "Epoch 78: TrainLoss 0.7441 RecLoss: 0.0000 (left: 0:09:13)\n",
      "TestLoss: 0.9203 MAE: 0.7538 RMSE: 0.9593\n",
      "ValLoss: 0.7090 MAE: 0.6624 RMSE: 0.8420\n",
      "Epoch 79: TrainLoss 0.7437 RecLoss: 0.0000 (left: 0:08:47)\n",
      "TestLoss: 0.9467 MAE: 0.7723 RMSE: 0.9730\n",
      "ValLoss: 0.7098 MAE: 0.6668 RMSE: 0.8425\n",
      "Epoch 80: TrainLoss 0.7442 RecLoss: 0.0000 (left: 0:08:21)\n",
      "TestLoss: 0.9120 MAE: 0.7493 RMSE: 0.9550\n",
      "ValLoss: 0.7110 MAE: 0.6621 RMSE: 0.8432\n",
      "Epoch 81: TrainLoss 0.7443 RecLoss: 0.0000 (left: 0:07:55)\n",
      "TestLoss: 0.9328 MAE: 0.7660 RMSE: 0.9658\n",
      "ValLoss: 0.7090 MAE: 0.6653 RMSE: 0.8420\n",
      "Epoch 82: TrainLoss 0.7445 RecLoss: 0.0000 (left: 0:07:28)\n",
      "TestLoss: 0.9385 MAE: 0.7670 RMSE: 0.9687\n",
      "ValLoss: 0.7081 MAE: 0.6649 RMSE: 0.8415\n",
      "Epoch 83: TrainLoss 0.7437 RecLoss: 0.0000 (left: 0:07:02)\n",
      "TestLoss: 0.9184 MAE: 0.7532 RMSE: 0.9583\n",
      "ValLoss: 0.7092 MAE: 0.6621 RMSE: 0.8421\n",
      "Epoch 84: TrainLoss 0.7448 RecLoss: 0.0000 (left: 0:06:36)\n",
      "TestLoss: 0.9547 MAE: 0.7789 RMSE: 0.9771\n",
      "ValLoss: 0.7121 MAE: 0.6694 RMSE: 0.8439\n",
      "Epoch 85: TrainLoss 0.7443 RecLoss: 0.0000 (left: 0:06:11)\n",
      "TestLoss: 0.9550 MAE: 0.7797 RMSE: 0.9772\n",
      "ValLoss: 0.7129 MAE: 0.6702 RMSE: 0.8443\n",
      "Epoch 86: TrainLoss 0.7439 RecLoss: 0.0000 (left: 0:05:45)\n",
      "TestLoss: 0.9231 MAE: 0.7577 RMSE: 0.9608\n",
      "ValLoss: 0.7076 MAE: 0.6628 RMSE: 0.8412\n",
      "Epoch 87: TrainLoss 0.7441 RecLoss: 0.0000 (left: 0:05:20)\n",
      "TestLoss: 0.9470 MAE: 0.7731 RMSE: 0.9731\n",
      "ValLoss: 0.7102 MAE: 0.6673 RMSE: 0.8427\n",
      "Epoch 88: TrainLoss 0.7438 RecLoss: 0.0000 (left: 0:04:55)\n",
      "TestLoss: 0.9237 MAE: 0.7589 RMSE: 0.9611\n",
      "ValLoss: 0.7082 MAE: 0.6635 RMSE: 0.8416\n",
      "Epoch 89: TrainLoss 0.7439 RecLoss: 0.0000 (left: 0:04:29)\n",
      "TestLoss: 0.9521 MAE: 0.7773 RMSE: 0.9758\n",
      "ValLoss: 0.7118 MAE: 0.6692 RMSE: 0.8437\n",
      "Epoch 90: TrainLoss 0.7439 RecLoss: 0.0000 (left: 0:04:04)\n",
      "TestLoss: 0.9349 MAE: 0.7670 RMSE: 0.9669\n",
      "ValLoss: 0.7082 MAE: 0.6654 RMSE: 0.8416\n",
      "Epoch 91: TrainLoss 0.7433 RecLoss: 0.0000 (left: 0:03:39)\n",
      "TestLoss: 0.9228 MAE: 0.7558 RMSE: 0.9606\n",
      "ValLoss: 0.7082 MAE: 0.6625 RMSE: 0.8415\n",
      "Epoch 92: TrainLoss 0.7433 RecLoss: 0.0000 (left: 0:03:14)\n",
      "TestLoss: 0.9320 MAE: 0.7640 RMSE: 0.9654\n",
      "ValLoss: 0.7074 MAE: 0.6641 RMSE: 0.8411\n",
      "Epoch 93: TrainLoss 0.7429 RecLoss: 0.0000 (left: 0:02:49)\n",
      "TestLoss: 0.9230 MAE: 0.7575 RMSE: 0.9607\n",
      "ValLoss: 0.7078 MAE: 0.6628 RMSE: 0.8413\n",
      "Epoch 94: TrainLoss 0.7434 RecLoss: 0.0000 (left: 0:02:25)\n",
      "TestLoss: 0.9356 MAE: 0.7665 RMSE: 0.9673\n",
      "ValLoss: 0.7080 MAE: 0.6649 RMSE: 0.8414\n",
      "Epoch 95: TrainLoss 0.7430 RecLoss: 0.0000 (left: 0:02:00)\n",
      "TestLoss: 0.9217 MAE: 0.7554 RMSE: 0.9601\n",
      "ValLoss: 0.7076 MAE: 0.6622 RMSE: 0.8412\n",
      "Epoch 96: TrainLoss 0.7431 RecLoss: 0.0000 (left: 0:01:36)\n",
      "TestLoss: 0.9175 MAE: 0.7538 RMSE: 0.9579\n",
      "ValLoss: 0.7082 MAE: 0.6622 RMSE: 0.8415\n",
      "Epoch 97: TrainLoss 0.7429 RecLoss: 0.0000 (left: 0:01:12)\n",
      "TestLoss: 0.9511 MAE: 0.7770 RMSE: 0.9752\n",
      "ValLoss: 0.7111 MAE: 0.6686 RMSE: 0.8433\n",
      "Epoch 98: TrainLoss 0.7438 RecLoss: 0.0000 (left: 0:00:47)\n",
      "TestLoss: 0.9096 MAE: 0.7460 RMSE: 0.9537\n",
      "ValLoss: 0.7128 MAE: 0.6622 RMSE: 0.8443\n",
      "Epoch 99: TrainLoss 0.7437 RecLoss: 0.0000 (left: 0:00:23)\n",
      "TestLoss: 0.9371 MAE: 0.7677 RMSE: 0.9681\n",
      "ValLoss: 0.7080 MAE: 0.6651 RMSE: 0.8414\n"
     ]
    }
   ],
   "source": [
    "!python pretrain-1m.py\n",
    "!python train-1m.py\n",
    "# !python test-1m.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n",
      "Extra : False\n",
      "-------Dataset Info--------\n",
      "split way [threshold] with threshold 30 training_ratio 1.0\n",
      "train set size: support/query 140178/85303\n",
      "test set size: support/query 442/1770\n",
      "CORE IS SELECTED:\n",
      "SIZE OF MODEL: torch.Size([2212, 32])\n",
      "SIZE OF MODEL: torch.Size([95872, 32])\n",
      "torch.Size([2212, 95872])\n",
      "MATRIX IS: tensor([[-9.8008e-10, -4.7807e-09, -2.2821e-07,  ...,  1.7130e-07,\n",
      "          2.7591e-08,  3.8432e-08],\n",
      "        [-1.1211e-08, -2.8426e-09, -4.0827e-06,  ...,  6.7347e-06,\n",
      "          1.8111e-10,  8.7163e-07],\n",
      "        [-1.9119e-09,  1.6175e-08,  8.2559e-07,  ..., -1.4087e-06,\n",
      "         -3.1535e-08, -1.8115e-07],\n",
      "        ...,\n",
      "        [ 2.8586e-05, -5.8327e-05,  1.5924e-03,  ..., -2.2678e-03,\n",
      "          3.6967e-05, -2.6993e-04],\n",
      "        [-2.0551e-09,  2.2095e-09, -1.4603e-06,  ...,  2.2979e-06,\n",
      "          2.5325e-08,  2.7943e-07],\n",
      "        [-4.2709e-10, -1.5555e-08,  8.5302e-07,  ..., -1.5550e-06,\n",
      "          1.8722e-08, -1.4775e-07]], device='cuda:0')\n",
      "NUMPY MATRIX: [[-9.8007769e-10 -4.7806799e-09 -2.2821034e-07 ...  1.7130363e-07\n",
      "   2.7590735e-08  3.8432411e-08]\n",
      " [-1.1210699e-08 -2.8425773e-09 -4.0827072e-06 ...  6.7347069e-06\n",
      "   1.8110730e-10  8.7163204e-07]\n",
      " [-1.9118975e-09  1.6174752e-08  8.2559029e-07 ... -1.4086586e-06\n",
      "  -3.1535343e-08 -1.8114709e-07]\n",
      " ...\n",
      " [ 2.8586499e-05 -5.8327416e-05  1.5924486e-03 ... -2.2677507e-03\n",
      "   3.6966892e-05 -2.6993355e-04]\n",
      " [-2.0551072e-09  2.2095128e-09 -1.4603078e-06 ...  2.2979082e-06\n",
      "   2.5324608e-08  2.7942659e-07]\n",
      " [-4.2709297e-10 -1.5554575e-08  8.5301906e-07 ... -1.5550095e-06\n",
      "   1.8722455e-08 -1.4775007e-07]]\n",
      "188\n",
      "755\n",
      "TOTAL INTERACTIONS: 58878\n",
      "dimension of mul: (2212, 95872)\n",
      "HEKKI\n",
      "MATRIX SHAPE: (2212, 95872)\n",
      "ORIGINAL MATRIX SHAPE: (2212, 95872)\n",
      "Traceback (most recent call last):\n",
      "  File \"test-1m.py\", line 263, in <module>\n",
      "    ntdf.to_csv('douban_20_tweaked_df.csv', index=False, header=False)\n",
      "  File \"/raid/home/jayantkalani/miniconda3/envs/ncf2/lib/python3.8/site-packages/pandas/core/generic.py\", line 3551, in to_csv\n",
      "    return DataFrameRenderer(formatter).to_csv(\n",
      "  File \"/raid/home/jayantkalani/miniconda3/envs/ncf2/lib/python3.8/site-packages/pandas/io/formats/format.py\", line 1180, in to_csv\n",
      "    csv_formatter.save()\n",
      "  File \"/raid/home/jayantkalani/miniconda3/envs/ncf2/lib/python3.8/site-packages/pandas/io/formats/csvs.py\", line 261, in save\n",
      "    self._save()\n",
      "  File \"/raid/home/jayantkalani/miniconda3/envs/ncf2/lib/python3.8/site-packages/pandas/io/formats/csvs.py\", line 266, in _save\n",
      "    self._save_body()\n",
      "  File \"/raid/home/jayantkalani/miniconda3/envs/ncf2/lib/python3.8/site-packages/pandas/io/formats/csvs.py\", line 304, in _save_body\n",
      "    self._save_chunk(start_i, end_i)\n",
      "  File \"/raid/home/jayantkalani/miniconda3/envs/ncf2/lib/python3.8/site-packages/pandas/io/formats/csvs.py\", line 315, in _save_chunk\n",
      "    libwriters.write_csv_rows(\n",
      "  File \"pandas/_libs/writers.pyx\", line 72, in pandas._libs.writers.write_csv_rows\n",
      "OSError: [Errno 28] No space left on device\n"
     ]
    }
   ],
   "source": [
    "!python test-1m.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
