{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code to seperate out users with threshold > 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fileinput import filename\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from scipy.sparse.linalg import svds\n",
    "import random \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as ss\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData():\n",
    "    ml1m_dir = 'data/ratings.dat'\n",
    "    ml1m_rating = pd.read_csv(ml1m_dir, sep='::', header=None, names=['uid', 'mid', 'rating', 'timestamp'],  engine='python')\n",
    "    unique_uid = np.unique(np.array(ml1m_rating['uid'].tolist()))\n",
    "    unique_mid = np.unique(np.array(ml1m_rating['mid'].tolist()))\n",
    "    uid_dict = dict([(y,x) for x,y in enumerate(unique_uid)])\n",
    "    mid_dict = dict([(y,x) for x,y in enumerate(unique_mid)])\n",
    "    print('DICTIONARY PREPARED:')\n",
    "\n",
    "    # init user item dictionary:\n",
    "    \n",
    "    uid_list = ml1m_rating['uid'].tolist()\n",
    "    uid_list_len = len(uid_list)\n",
    "    mid_list = ml1m_rating['mid'].tolist()\n",
    "    mid_list_len = len(mid_list)\n",
    "    rating_list = ml1m_rating['rating'].tolist()\n",
    "    user_item_dict = {x:set() for x in range(len(unique_uid))}\n",
    "    item_user_dict = {x:set() for x in range(len(unique_mid))}\n",
    "    for i in range(uid_list_len):\n",
    "        uid_list[i] = uid_dict[uid_list[i]]\n",
    "        mid_list[i] = mid_dict[mid_list[i]]\n",
    "        # rating_list[i] = 1 # comment this line if you want to activate explicit ratings\n",
    "        user_item_dict[uid_list[i]].add(mid_list[i])\n",
    "        item_user_dict[mid_list[i]].add(uid_list[i])\n",
    "    tmp_df = pd.DataFrame({\"uid\":uid_list, \"mid\":mid_list, \"ratings\":rating_list})\n",
    "    v = tmp_df.uid.value_counts()\n",
    "    df = tmp_df[tmp_df.uid.isin(v.index[v.gt(30)])]\n",
    "### code to store less than 30 interactions:\n",
    "    df_less_30 = tmp_df[tmp_df.uid.isin(v.index[v.le(30)])]\n",
    "    return df, df_less_30, len(np.unique(mid_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DICTIONARY PREPARED:\n",
      "GREATER THAN 30:\n",
      "           uid   mid  ratings\n",
      "0           0  1104        5\n",
      "1           0   639        3\n",
      "2           0   853        3\n",
      "3           0  3177        4\n",
      "4           0  2162        5\n",
      "...       ...   ...      ...\n",
      "1000204  6039  1019        1\n",
      "1000205  6039  1022        5\n",
      "1000206  6039   548        5\n",
      "1000207  6039  1024        4\n",
      "1000208  6039  1025        4\n",
      "\n",
      "[980300 rows x 3 columns]\n",
      "LESS THAN 30: \n",
      "          uid   mid  ratings\n",
      "233        3  3235        5\n",
      "234        3  1120        3\n",
      "235        3  2743        4\n",
      "236        3  1124        4\n",
      "237        3   971        4\n",
      "...      ...   ...      ...\n",
      "999740  6037  1288        2\n",
      "999741  6037  2495        1\n",
      "999742  6037  2511        3\n",
      "999743  6037  3165        3\n",
      "999744  6037  1007        5\n",
      "\n",
      "[19909 rows x 3 columns]\n",
      "980300\n",
      "19909\n",
      "UNIQUE MIDS:  3706\n"
     ]
    }
   ],
   "source": [
    "df_gt_30, df_le_30, unique_mids = ReadData()\n",
    "print(\"GREATER THAN 30:\\n\", df_gt_30)\n",
    "print(\"LESS THAN 30: \\n\", df_le_30)\n",
    "print(len(df_gt_30))\n",
    "print(len(df_le_30))\n",
    "print(\"UNIQUE MIDS: \", unique_mids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_gt_30.groupby(\"uid\").tail(1)\n",
    "# print(len(df_gt_30))\n",
    "train_df = df_gt_30.drop(df_gt_30.groupby('uid').tail(1).index, inplace=False)\n",
    "assert(len(df_gt_30)== len(test_df) + len(train_df))\n",
    "# print(len(test_df))\n",
    "# print(len(train_df))\n",
    "dic_train_df_uid_mapping = dict([(y,x) for x,y in enumerate(np.unique(train_df['uid']))])\n",
    "dic_train_df_uid_rmapping = dict([(x,y) for x,y in enumerate(np.unique(train_df['uid']))])\n",
    "### no need for mid mapping\n",
    "\n",
    "uid_of_train_df = train_df['uid'].tolist()\n",
    "for i in range(len(uid_of_train_df)):\n",
    "    uid_of_train_df[i] = dic_train_df_uid_mapping[uid_of_train_df[i]]\n",
    "# for index, row in train_df.iterrows():\n",
    "#     train_df['uid'][index] = dic_train_df_uid_mapping[train_df['uid'][index]]\n",
    "core_user_ko_input_train_df = pd.DataFrame({'uid':uid_of_train_df, 'mid':train_df['mid'], 'ratings':train_df['ratings']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ui_dic = {}    \n",
    "for user in range(6040):\n",
    "    train_ui_dic[user] = []\n",
    "for index,row in train_df.iterrows():\n",
    "        train_ui_dic[row['uid']].append(row['mid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3699\n"
     ]
    }
   ],
   "source": [
    "print(len(np.unique(train_df['mid'])))\n",
    "# print(dic_train_df_uid_rmapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractCoreUsers(dataframe, unique_user_len, unique_item_len):\n",
    "    # print(\"# of rows in ml1m_ratings: \", len(dataframe))\n",
    "    u_len = unique_user_len\n",
    "    print(\"USER LEN:\", u_len)\n",
    "    # print(user_id)\n",
    "\n",
    "    m_len = unique_item_len\n",
    "    print(\"MOVIE LEN:\", m_len)\n",
    "    userItemMatrix = np.zeros(shape=(u_len, m_len))\n",
    "    # print(userItemMatrix)\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        userItemMatrix[row['uid']][row['mid']] = row['ratings']\n",
    "        # print(row['uid'], row['mid'])\n",
    "    print(\"USER ITEM MATRIX: \\n\", userItemMatrix)\n",
    "\n",
    "    df = pd.DataFrame(userItemMatrix)\n",
    "    cosineSimilarity = cosine_similarity(df)\n",
    "    print(\"SHAPE OF COSINE MATIX:\\n \", cosineSimilarity.shape)\n",
    "\n",
    "    listToStoreTopFiftyOfEveryUser = []\n",
    "    for i in range(0, cosineSimilarity.shape[0]):\n",
    "        idx = np.argpartition(cosineSimilarity[i], -50)[-50:]\n",
    "        listToStoreTopFiftyOfEveryUser.append(idx)\n",
    "    # print(\"Top fifty list: \\n\", listToStoreTopFiftyOfEveryUser)\n",
    "    # listToStoreTopFiftyOfEveryUser = np.array(listToStoreTopFiftyOfEveryUser)\n",
    "    flatten = np.concatenate(listToStoreTopFiftyOfEveryUser)\n",
    "    listToStoreTopFiftyOfEveryUser = flatten.ravel()\n",
    "\n",
    "    # print(\"List of top 50\", listToStoreTopFiftyOfEveryUser)\n",
    "    df = pd.DataFrame(listToStoreTopFiftyOfEveryUser)\n",
    "    allUserList = df.value_counts().index.tolist()\n",
    "    # print(\"ALL USERS LIST\", allUserList)\n",
    "    allUserList = list(sum(allUserList,()))\n",
    "    # print(\"ALL USERS LIST\", allUserList)\n",
    "    twentyPercentUserList = allUserList[:int(len(allUserList)*0.2)]\n",
    "    # print(\"TWENTY PERCENT USER:\", len(twentyPercentUserList))\n",
    "    # print(\"TWENTY PERCENT USER:\", (twentyPercentUserList))\n",
    "    coreusers = dataframe.iloc[np.where(dataframe.uid.isin(twentyPercentUserList))]\n",
    "    # coreusers.reset_index()\n",
    "    # print(\"CORE USERS:\\n\", coreusers)\n",
    "    return coreusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER LEN: 5231\n",
      "MOVIE LEN: 3706\n",
      "USER ITEM MATRIX: \n",
      " [[5. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [3. 0. 0. ... 0. 0. 0.]]\n",
      "SHAPE OF COSINE MATIX:\n",
      "  (5231, 5231)\n",
      "CORE USERS:           uid   mid  ratings\n",
      "523         5   627        4\n",
      "524         5   805        4\n",
      "525         5  2708        5\n",
      "526         5  3341        3\n",
      "527         5  3550        3\n",
      "...       ...   ...      ...\n",
      "1000203  5230  1018        3\n",
      "1000204  5230  1019        1\n",
      "1000205  5230  1022        5\n",
      "1000206  5230   548        5\n",
      "1000207  5230  1024        4\n",
      "\n",
      "[366403 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "core_users = ExtractCoreUsers(core_user_ko_input_train_df, len(np.unique(uid_of_train_df)), unique_mids)\n",
    "support_user_list = np.unique(core_users['uid'])\n",
    "print(\"CORE USERS:\" ,core_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6038\n",
      "          uid   mid  ratings\n",
      "0           0  1104        5\n",
      "1           0   639        3\n",
      "2           0   853        3\n",
      "3           0  3177        4\n",
      "4           0  2162        5\n",
      "...       ...   ...      ...\n",
      "1000203  6039  1018        3\n",
      "1000204  6039  1019        1\n",
      "1000205  6039  1022        5\n",
      "1000206  6039   548        5\n",
      "1000207  6039  1024        4\n",
      "\n",
      "[975069 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df['uid'][999866])\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON CORE USERS:          uid   mid  ratings\n",
      "0          0  1104        5\n",
      "1          0   639        3\n",
      "2          0   853        3\n",
      "3          0  3177        4\n",
      "4          0  2162        5\n",
      "...      ...   ...      ...\n",
      "999862  6038  1006        4\n",
      "999863  6038  1009        4\n",
      "999864  6038  1011        3\n",
      "999865  6038  1014        4\n",
      "999866  6038  1016        4\n",
      "\n",
      "[608666 rows x 3 columns]\n",
      "CORE USERS:           uid   mid  ratings\n",
      "523         5   627        4\n",
      "524         5   805        4\n",
      "525         5  2708        5\n",
      "526         5  3341        3\n",
      "527         5  3550        3\n",
      "...       ...   ...      ...\n",
      "1000203  5230  1018        3\n",
      "1000204  5230  1019        1\n",
      "1000205  5230  1022        5\n",
      "1000206  5230   548        5\n",
      "1000207  5230  1024        4\n",
      "\n",
      "[366403 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# print(\"CORE USERS:\" ,core_users)\n",
    "# print(\"UNIQUE USERS:\", len(np.unique(core_users['uid'])))\n",
    "core_users_index_list = core_users.index.to_list()\n",
    "non_core_user_index = (train_df.index.difference(core_users.index))\n",
    "non_core_user_index = non_core_user_index.tolist()\n",
    "\n",
    "core_users_df = train_df.loc[core_users_index_list]\n",
    "non_core_user_df = train_df.loc[non_core_user_index]\n",
    "print(\"NON CORE USERS:\" ,non_core_user_df)\n",
    "print(\"CORE USERS:\" ,core_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DF CONTAINS TEST FOR CORE AND NON CORE ENTITIES:\n",
      "           uid   mid  ratings\n",
      "52          0  1154        4\n",
      "181         1  1155        5\n",
      "232         2  1900        4\n",
      "451         4   683        4\n",
      "522         5    33        4\n",
      "...       ...   ...      ...\n",
      "998634   6034  1865        4\n",
      "999522   6035  1867        3\n",
      "999724   6036  1025        5\n",
      "999867   6038  1025        4\n",
      "1000208  6039  1025        4\n",
      "\n",
      "[5231 rows x 3 columns]\n",
      "1046\n",
      "SUPPORT TEST DF:           uid   mid  ratings\n",
      "553         6  3186        3\n",
      "798         8   414        3\n",
      "1199        9  1868        5\n",
      "1467       12  1865        4\n",
      "1693       14  1934        4\n",
      "...       ...   ...      ...\n",
      "997247   6020  1025        3\n",
      "997889   6025  1025        5\n",
      "998118   6029  1025        3\n",
      "999724   6036  1025        5\n",
      "1000208  6039  1025        4\n",
      "\n",
      "[1046 rows x 3 columns]\n",
      "QUERY TEST DF:\n",
      "          uid   mid  ratings\n",
      "52         0  1154        4\n",
      "181        1  1155        5\n",
      "232        2  1900        4\n",
      "451        4   683        4\n",
      "522        5    33        4\n",
      "...      ...   ...      ...\n",
      "998273  6031  1012        2\n",
      "998333  6032  1848        5\n",
      "998634  6034  1865        4\n",
      "999522  6035  1867        3\n",
      "999867  6038  1025        4\n",
      "\n",
      "[4185 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"TEST DF CONTAINS TEST FOR CORE AND NON CORE ENTITIES:\\n\" ,test_df)\n",
    "# print(core_users['uid'])\n",
    "unique_uids_in_support_trian = np.unique(np.array(core_users_df['uid']))\n",
    "unique_uids_in_query_trian = np.unique(non_core_user_df['uid'])\n",
    "print(len(unique_uids_in_support_trian))\n",
    "support_test_df = test_df.loc[test_df['uid'].isin(unique_uids_in_support_trian)]\n",
    "print(\"SUPPORT TEST DF:\" ,support_test_df)\n",
    "query_test_df = test_df.loc[test_df['uid'].isin(unique_uids_in_query_trian)]\n",
    "print(\"QUERY TEST DF:\\n\", query_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_train = []\n",
    "for index,row in core_users_df.iterrows():\n",
    "    support_train.append([row['uid'], row['mid'], row['ratings']])\n",
    "query_train = []\n",
    "for index, row in non_core_user_df.iterrows():\n",
    "    query_train.append([row['uid'], row['mid'], row['ratings']])\n",
    "support_test = []\n",
    "for index, row in support_test_df.iterrows():\n",
    "    support_test.append([row['uid'], row['mid'], row['ratings']])\n",
    "query_test = []\n",
    "for index, row in query_test_df.iterrows():\n",
    "    query_test.append([row['uid'], row['mid'], row['ratings']])\n",
    "user_his_dic = {}\n",
    "for u in train_ui_dic.keys():\n",
    "    user_his_dic[u] = train_ui_dic[u]\n",
    "user_supp_list = np.unique(core_users_df['uid']).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"utils_output_file_core_users.pkl\", \"wb\") as f:\n",
    "    pickle.dump(support_train, f)\n",
    "    pickle.dump(query_train, f)\n",
    "    pickle.dump(support_test, f)\n",
    "    pickle.dump(query_test, f)\n",
    "    pickle.dump(user_supp_list, f)\n",
    "    pickle.dump(user_his_dic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Dataset Info--------\n",
      "split way [threshold] with threshold 30 training_ratio 1.0\n",
      "train set size: support/query 366403/608666\n",
      "test set size: support/query 1046/4185\n",
      "Epoch 0 Step 340: Train 2.0963 Reg: 0.5619\n",
      "Test: 0.8439 MAE: 0.7313 RMSE: 0.9186\n",
      "Val: 0.7924 MAE: 0.7044 RMSE: 0.8902\n",
      "Epoch 1 Step 680: Train 0.7729 Reg: 0.4613\n",
      "Test: 0.7797 MAE: 0.7014 RMSE: 0.8830\n",
      "Val: 0.7811 MAE: 0.6994 RMSE: 0.8838\n",
      "Epoch 2 Step 1020: Train 0.7684 Reg: 0.3822\n",
      "Test: 0.7868 MAE: 0.7054 RMSE: 0.8870\n",
      "Val: 0.7808 MAE: 0.7000 RMSE: 0.8836\n",
      "Epoch 3 Step 1360: Train 0.7653 Reg: 0.3303\n",
      "Test: 0.7669 MAE: 0.7027 RMSE: 0.8757\n",
      "Val: 0.7793 MAE: 0.6983 RMSE: 0.8828\n",
      "Epoch 4 Step 1700: Train 0.7626 Reg: 0.2984\n",
      "Test: 0.7830 MAE: 0.7062 RMSE: 0.8849\n",
      "Val: 0.7756 MAE: 0.6963 RMSE: 0.8807\n",
      "Epoch 5 Step 2040: Train 0.7586 Reg: 0.2793\n",
      "Test: 0.7717 MAE: 0.7038 RMSE: 0.8784\n",
      "Val: 0.7707 MAE: 0.6946 RMSE: 0.8779\n",
      "Epoch 6 Step 2380: Train 0.7531 Reg: 0.2702\n",
      "Test: 0.7715 MAE: 0.7011 RMSE: 0.8784\n",
      "Val: 0.7647 MAE: 0.6923 RMSE: 0.8745\n",
      "Epoch 7 Step 2720: Train 0.7436 Reg: 0.2684\n",
      "Test: 0.7624 MAE: 0.7020 RMSE: 0.8731\n",
      "Val: 0.7542 MAE: 0.6851 RMSE: 0.8685\n",
      "Epoch 8 Step 3060: Train 0.7311 Reg: 0.2744\n",
      "Test: 0.7709 MAE: 0.6999 RMSE: 0.8780\n",
      "Val: 0.7398 MAE: 0.6779 RMSE: 0.8601\n",
      "Epoch 9 Step 3400: Train 0.7139 Reg: 0.2833\n",
      "Test: 0.7558 MAE: 0.6939 RMSE: 0.8694\n",
      "Val: 0.7260 MAE: 0.6725 RMSE: 0.8521\n",
      "Epoch 10 Step 3740: Train 0.7004 Reg: 0.2865\n",
      "Test: 0.7538 MAE: 0.7012 RMSE: 0.8682\n",
      "Val: 0.7199 MAE: 0.6692 RMSE: 0.8485\n",
      "Epoch 11 Step 4080: Train 0.6917 Reg: 0.2852\n",
      "Test: 0.7360 MAE: 0.6877 RMSE: 0.8579\n",
      "Val: 0.7148 MAE: 0.6662 RMSE: 0.8455\n",
      "Epoch 12 Step 4420: Train 0.6838 Reg: 0.2859\n",
      "Test: 0.7392 MAE: 0.6900 RMSE: 0.8598\n",
      "Val: 0.7089 MAE: 0.6643 RMSE: 0.8419\n",
      "Epoch 13 Step 4760: Train 0.6704 Reg: 0.2966\n",
      "Test: 0.7181 MAE: 0.6805 RMSE: 0.8474\n",
      "Val: 0.6970 MAE: 0.6587 RMSE: 0.8349\n",
      "Epoch 14 Step 5100: Train 0.6544 Reg: 0.3107\n",
      "Test: 0.7060 MAE: 0.6728 RMSE: 0.8402\n",
      "Val: 0.6896 MAE: 0.6525 RMSE: 0.8304\n",
      "Epoch 15 Step 5440: Train 0.6349 Reg: 0.3290\n",
      "Test: 0.7033 MAE: 0.6766 RMSE: 0.8387\n",
      "Val: 0.6825 MAE: 0.6499 RMSE: 0.8261\n",
      "Epoch 16 Step 5780: Train 0.6113 Reg: 0.3537\n",
      "Test: 0.7032 MAE: 0.6760 RMSE: 0.8386\n",
      "Val: 0.6760 MAE: 0.6458 RMSE: 0.8222\n",
      "Epoch 17 Step 6120: Train 0.5856 Reg: 0.3816\n",
      "Test: 0.7018 MAE: 0.6678 RMSE: 0.8377\n",
      "Val: 0.6741 MAE: 0.6427 RMSE: 0.8210\n",
      "Epoch 18 Step 6460: Train 0.5589 Reg: 0.4074\n",
      "Test: 0.7208 MAE: 0.6789 RMSE: 0.8490\n",
      "Val: 0.6762 MAE: 0.6444 RMSE: 0.8223\n",
      "Epoch 19 Step 6800: Train 0.5341 Reg: 0.4272\n",
      "Test: 0.7222 MAE: 0.6770 RMSE: 0.8498\n",
      "Val: 0.6844 MAE: 0.6466 RMSE: 0.8273\n",
      "Epoch 20 Step 7140: Train 0.5118 Reg: 0.4458\n",
      "Test: 0.7348 MAE: 0.6810 RMSE: 0.8572\n",
      "Val: 0.6915 MAE: 0.6501 RMSE: 0.8316\n",
      "Epoch 21 Step 7480: Train 0.4902 Reg: 0.4613\n",
      "Test: 0.7396 MAE: 0.6817 RMSE: 0.8600\n",
      "Val: 0.7021 MAE: 0.6540 RMSE: 0.8379\n",
      "Epoch 22 Step 7820: Train 0.4711 Reg: 0.4731\n",
      "Test: 0.7476 MAE: 0.6836 RMSE: 0.8646\n",
      "Val: 0.7142 MAE: 0.6589 RMSE: 0.8451\n",
      "Epoch 23 Step 8160: Train 0.4535 Reg: 0.4821\n",
      "Test: 0.7591 MAE: 0.6874 RMSE: 0.8713\n",
      "Val: 0.7293 MAE: 0.6657 RMSE: 0.8540\n",
      "Epoch 24 Step 8500: Train 0.4395 Reg: 0.4877\n",
      "Test: 0.7644 MAE: 0.6890 RMSE: 0.8743\n",
      "Val: 0.7390 MAE: 0.6692 RMSE: 0.8596\n",
      "Epoch 25 Step 8840: Train 0.4265 Reg: 0.4929\n",
      "Test: 0.7862 MAE: 0.6992 RMSE: 0.8867\n",
      "Val: 0.7509 MAE: 0.6750 RMSE: 0.8665\n",
      "Epoch 26 Step 9180: Train 0.4154 Reg: 0.4965\n",
      "Test: 0.7794 MAE: 0.6928 RMSE: 0.8828\n",
      "Val: 0.7631 MAE: 0.6784 RMSE: 0.8736\n",
      "Epoch 27 Step 9520: Train 0.4063 Reg: 0.4978\n",
      "Test: 0.7994 MAE: 0.7000 RMSE: 0.8941\n",
      "Val: 0.7751 MAE: 0.6826 RMSE: 0.8804\n",
      "Epoch 28 Step 9860: Train 0.3993 Reg: 0.4974\n",
      "Test: 0.8093 MAE: 0.7060 RMSE: 0.8996\n",
      "Val: 0.7834 MAE: 0.6866 RMSE: 0.8851\n",
      "Epoch 29 Step 10200: Train 0.3936 Reg: 0.4961\n",
      "Test: 0.8170 MAE: 0.7090 RMSE: 0.9039\n",
      "Val: 0.7918 MAE: 0.6900 RMSE: 0.8898\n",
      "Epoch 30 Step 10540: Train 0.3887 Reg: 0.4941\n",
      "Test: 0.8202 MAE: 0.7058 RMSE: 0.9056\n",
      "Val: 0.7983 MAE: 0.6921 RMSE: 0.8935\n",
      "Epoch 31 Step 10880: Train 0.3845 Reg: 0.4919\n",
      "Test: 0.8247 MAE: 0.7062 RMSE: 0.9081\n",
      "Val: 0.8053 MAE: 0.6940 RMSE: 0.8974\n",
      "Epoch 32 Step 11220: Train 0.3806 Reg: 0.4890\n",
      "Test: 0.8358 MAE: 0.7120 RMSE: 0.9142\n",
      "Val: 0.8108 MAE: 0.6962 RMSE: 0.9004\n",
      "Epoch 33 Step 11560: Train 0.3771 Reg: 0.4865\n",
      "Test: 0.8414 MAE: 0.7131 RMSE: 0.9173\n",
      "Val: 0.8169 MAE: 0.6985 RMSE: 0.9038\n",
      "Epoch 34 Step 11900: Train 0.3739 Reg: 0.4837\n",
      "Test: 0.8524 MAE: 0.7186 RMSE: 0.9233\n",
      "Val: 0.8226 MAE: 0.7005 RMSE: 0.9070\n",
      "Epoch 35 Step 12240: Train 0.3711 Reg: 0.4811\n",
      "Test: 0.8550 MAE: 0.7194 RMSE: 0.9246\n",
      "Val: 0.8281 MAE: 0.7034 RMSE: 0.9100\n",
      "Epoch 36 Step 12580: Train 0.3682 Reg: 0.4784\n",
      "Test: 0.8589 MAE: 0.7199 RMSE: 0.9268\n",
      "Val: 0.8316 MAE: 0.7041 RMSE: 0.9119\n",
      "Epoch 37 Step 12920: Train 0.3657 Reg: 0.4756\n",
      "Test: 0.8646 MAE: 0.7224 RMSE: 0.9298\n",
      "Val: 0.8365 MAE: 0.7058 RMSE: 0.9146\n",
      "Epoch 38 Step 13260: Train 0.3630 Reg: 0.4731\n",
      "Test: 0.8687 MAE: 0.7231 RMSE: 0.9320\n",
      "Val: 0.8405 MAE: 0.7070 RMSE: 0.9168\n",
      "Epoch 39 Step 13600: Train 0.3609 Reg: 0.4704\n",
      "Test: 0.8763 MAE: 0.7268 RMSE: 0.9361\n",
      "Val: 0.8461 MAE: 0.7089 RMSE: 0.9199\n",
      "Epoch 40 Step 13940: Train 0.3587 Reg: 0.4679\n",
      "Test: 0.8758 MAE: 0.7258 RMSE: 0.9358\n",
      "Val: 0.8485 MAE: 0.7098 RMSE: 0.9211\n",
      "Epoch 41 Step 14280: Train 0.3566 Reg: 0.4653\n",
      "Test: 0.8841 MAE: 0.7281 RMSE: 0.9403\n",
      "Val: 0.8525 MAE: 0.7113 RMSE: 0.9233\n",
      "Epoch 42 Step 14620: Train 0.3546 Reg: 0.4629\n",
      "Test: 0.8871 MAE: 0.7313 RMSE: 0.9419\n",
      "Val: 0.8557 MAE: 0.7131 RMSE: 0.9250\n",
      "Epoch 43 Step 14960: Train 0.3528 Reg: 0.4604\n",
      "Test: 0.8908 MAE: 0.7306 RMSE: 0.9438\n",
      "Val: 0.8602 MAE: 0.7138 RMSE: 0.9274\n",
      "Epoch 44 Step 15300: Train 0.3511 Reg: 0.4581\n",
      "Test: 0.8969 MAE: 0.7347 RMSE: 0.9471\n",
      "Val: 0.8625 MAE: 0.7152 RMSE: 0.9287\n",
      "Epoch 45 Step 15640: Train 0.3494 Reg: 0.4558\n",
      "Test: 0.8982 MAE: 0.7329 RMSE: 0.9477\n",
      "Val: 0.8646 MAE: 0.7155 RMSE: 0.9299\n",
      "Epoch 46 Step 15980: Train 0.3478 Reg: 0.4535\n",
      "Test: 0.9052 MAE: 0.7355 RMSE: 0.9514\n",
      "Val: 0.8674 MAE: 0.7165 RMSE: 0.9313\n",
      "Epoch 47 Step 16320: Train 0.3463 Reg: 0.4514\n",
      "Test: 0.9066 MAE: 0.7378 RMSE: 0.9521\n",
      "Val: 0.8702 MAE: 0.7181 RMSE: 0.9329\n",
      "Epoch 48 Step 16660: Train 0.3449 Reg: 0.4493\n",
      "Test: 0.9087 MAE: 0.7377 RMSE: 0.9532\n",
      "Val: 0.8729 MAE: 0.7184 RMSE: 0.9343\n",
      "Epoch 49 Step 17000: Train 0.3435 Reg: 0.4473\n",
      "Test: 0.9156 MAE: 0.7403 RMSE: 0.9569\n",
      "Val: 0.8767 MAE: 0.7201 RMSE: 0.9363\n",
      "Epoch 50 Step 17340: Train 0.3423 Reg: 0.4454\n",
      "Test: 0.9132 MAE: 0.7380 RMSE: 0.9556\n",
      "Val: 0.8783 MAE: 0.7205 RMSE: 0.9372\n",
      "Epoch 51 Step 17680: Train 0.3412 Reg: 0.4436\n",
      "Test: 0.9181 MAE: 0.7402 RMSE: 0.9582\n",
      "Val: 0.8810 MAE: 0.7216 RMSE: 0.9386\n",
      "Epoch 52 Step 18020: Train 0.3399 Reg: 0.4417\n",
      "Test: 0.9209 MAE: 0.7417 RMSE: 0.9597\n",
      "Val: 0.8832 MAE: 0.7228 RMSE: 0.9398\n",
      "Epoch 53 Step 18360: Train 0.3388 Reg: 0.4400\n",
      "Test: 0.9222 MAE: 0.7407 RMSE: 0.9603\n",
      "Val: 0.8844 MAE: 0.7226 RMSE: 0.9404\n",
      "Epoch 54 Step 18700: Train 0.3378 Reg: 0.4383\n",
      "Test: 0.9229 MAE: 0.7408 RMSE: 0.9607\n",
      "Val: 0.8868 MAE: 0.7234 RMSE: 0.9417\n",
      "Epoch 55 Step 19040: Train 0.3368 Reg: 0.4368\n",
      "Test: 0.9277 MAE: 0.7434 RMSE: 0.9632\n",
      "Val: 0.8884 MAE: 0.7240 RMSE: 0.9426\n",
      "Epoch 56 Step 19380: Train 0.3358 Reg: 0.4352\n",
      "Test: 0.9285 MAE: 0.7424 RMSE: 0.9636\n",
      "Val: 0.8904 MAE: 0.7243 RMSE: 0.9436\n",
      "Epoch 57 Step 19720: Train 0.3349 Reg: 0.4337\n",
      "Test: 0.9346 MAE: 0.7459 RMSE: 0.9667\n",
      "Val: 0.8926 MAE: 0.7258 RMSE: 0.9448\n",
      "Epoch 58 Step 20060: Train 0.3340 Reg: 0.4324\n",
      "Test: 0.9339 MAE: 0.7443 RMSE: 0.9664\n",
      "Val: 0.8941 MAE: 0.7258 RMSE: 0.9455\n",
      "Epoch 59 Step 20400: Train 0.3333 Reg: 0.4310\n",
      "Test: 0.9397 MAE: 0.7478 RMSE: 0.9694\n",
      "Val: 0.8961 MAE: 0.7270 RMSE: 0.9466\n",
      "Epoch 60 Step 20740: Train 0.3325 Reg: 0.4297\n",
      "Test: 0.9381 MAE: 0.7458 RMSE: 0.9685\n",
      "Val: 0.8969 MAE: 0.7268 RMSE: 0.9471\n",
      "Epoch 61 Step 21080: Train 0.3317 Reg: 0.4284\n",
      "Test: 0.9402 MAE: 0.7465 RMSE: 0.9696\n",
      "Val: 0.8985 MAE: 0.7273 RMSE: 0.9479\n",
      "Epoch 62 Step 21420: Train 0.3310 Reg: 0.4272\n",
      "Test: 0.9421 MAE: 0.7478 RMSE: 0.9706\n",
      "Val: 0.8998 MAE: 0.7280 RMSE: 0.9486\n",
      "Epoch 63 Step 21760: Train 0.3304 Reg: 0.4261\n",
      "Test: 0.9429 MAE: 0.7473 RMSE: 0.9710\n",
      "Val: 0.9010 MAE: 0.7282 RMSE: 0.9492\n",
      "Epoch 64 Step 22100: Train 0.3297 Reg: 0.4250\n",
      "Test: 0.9452 MAE: 0.7482 RMSE: 0.9722\n",
      "Val: 0.9020 MAE: 0.7286 RMSE: 0.9498\n",
      "Epoch 65 Step 22440: Train 0.3291 Reg: 0.4240\n",
      "Test: 0.9455 MAE: 0.7481 RMSE: 0.9724\n",
      "Val: 0.9034 MAE: 0.7290 RMSE: 0.9505\n",
      "Epoch 66 Step 22780: Train 0.3285 Reg: 0.4230\n",
      "Test: 0.9478 MAE: 0.7497 RMSE: 0.9735\n",
      "Val: 0.9043 MAE: 0.7296 RMSE: 0.9510\n",
      "Epoch 67 Step 23120: Train 0.3280 Reg: 0.4220\n",
      "Test: 0.9502 MAE: 0.7506 RMSE: 0.9748\n",
      "Val: 0.9059 MAE: 0.7303 RMSE: 0.9518\n",
      "Epoch 68 Step 23460: Train 0.3274 Reg: 0.4211\n",
      "Test: 0.9504 MAE: 0.7499 RMSE: 0.9749\n",
      "Val: 0.9065 MAE: 0.7302 RMSE: 0.9521\n",
      "Epoch 69 Step 23800: Train 0.3269 Reg: 0.4203\n",
      "Test: 0.9523 MAE: 0.7508 RMSE: 0.9758\n",
      "Val: 0.9079 MAE: 0.7307 RMSE: 0.9529\n",
      "Epoch 70 Step 24140: Train 0.3265 Reg: 0.4195\n",
      "Test: 0.9524 MAE: 0.7509 RMSE: 0.9759\n",
      "Val: 0.9086 MAE: 0.7309 RMSE: 0.9532\n",
      "Epoch 71 Step 24480: Train 0.3260 Reg: 0.4187\n",
      "Test: 0.9539 MAE: 0.7514 RMSE: 0.9767\n",
      "Val: 0.9094 MAE: 0.7312 RMSE: 0.9536\n",
      "Epoch 72 Step 24820: Train 0.3256 Reg: 0.4179\n",
      "Test: 0.9538 MAE: 0.7508 RMSE: 0.9766\n",
      "Val: 0.9101 MAE: 0.7313 RMSE: 0.9540\n",
      "Epoch 73 Step 25160: Train 0.3252 Reg: 0.4172\n",
      "Test: 0.9545 MAE: 0.7514 RMSE: 0.9770\n",
      "Val: 0.9109 MAE: 0.7317 RMSE: 0.9544\n",
      "Epoch 74 Step 25500: Train 0.3247 Reg: 0.4165\n",
      "Test: 0.9548 MAE: 0.7509 RMSE: 0.9772\n",
      "Val: 0.9117 MAE: 0.7317 RMSE: 0.9548\n",
      "Epoch 75 Step 25840: Train 0.3244 Reg: 0.4159\n",
      "Test: 0.9559 MAE: 0.7517 RMSE: 0.9777\n",
      "Val: 0.9125 MAE: 0.7321 RMSE: 0.9553\n",
      "Epoch 76 Step 26180: Train 0.3240 Reg: 0.4152\n",
      "Test: 0.9573 MAE: 0.7521 RMSE: 0.9784\n",
      "Val: 0.9129 MAE: 0.7322 RMSE: 0.9554\n",
      "Epoch 77 Step 26520: Train 0.3237 Reg: 0.4147\n",
      "Test: 0.9577 MAE: 0.7522 RMSE: 0.9786\n",
      "Val: 0.9137 MAE: 0.7325 RMSE: 0.9559\n",
      "Epoch 78 Step 26860: Train 0.3233 Reg: 0.4141\n",
      "Test: 0.9594 MAE: 0.7530 RMSE: 0.9795\n",
      "Val: 0.9142 MAE: 0.7328 RMSE: 0.9562\n",
      "Epoch 79 Step 27200: Train 0.3230 Reg: 0.4135\n",
      "Test: 0.9601 MAE: 0.7532 RMSE: 0.9798\n",
      "Val: 0.9151 MAE: 0.7331 RMSE: 0.9566\n",
      "Epoch 80 Step 27540: Train 0.3227 Reg: 0.4130\n",
      "Test: 0.9597 MAE: 0.7531 RMSE: 0.9797\n",
      "Val: 0.9155 MAE: 0.7332 RMSE: 0.9568\n",
      "Epoch 81 Step 27880: Train 0.3225 Reg: 0.4125\n",
      "Test: 0.9601 MAE: 0.7530 RMSE: 0.9799\n",
      "Val: 0.9159 MAE: 0.7332 RMSE: 0.9570\n",
      "Epoch 82 Step 28220: Train 0.3222 Reg: 0.4121\n",
      "Test: 0.9607 MAE: 0.7531 RMSE: 0.9802\n",
      "Val: 0.9163 MAE: 0.7333 RMSE: 0.9572\n",
      "Epoch 83 Step 28560: Train 0.3220 Reg: 0.4116\n",
      "Test: 0.9621 MAE: 0.7539 RMSE: 0.9809\n",
      "Val: 0.9170 MAE: 0.7337 RMSE: 0.9576\n",
      "Epoch 84 Step 28900: Train 0.3217 Reg: 0.4112\n",
      "Test: 0.9622 MAE: 0.7537 RMSE: 0.9809\n",
      "Val: 0.9174 MAE: 0.7337 RMSE: 0.9578\n",
      "Epoch 85 Step 29240: Train 0.3215 Reg: 0.4108\n",
      "Test: 0.9635 MAE: 0.7544 RMSE: 0.9816\n",
      "Val: 0.9180 MAE: 0.7341 RMSE: 0.9581\n",
      "Epoch 86 Step 29580: Train 0.3213 Reg: 0.4104\n",
      "Test: 0.9629 MAE: 0.7535 RMSE: 0.9813\n",
      "Val: 0.9184 MAE: 0.7339 RMSE: 0.9583\n",
      "Epoch 87 Step 29920: Train 0.3211 Reg: 0.4101\n",
      "Test: 0.9644 MAE: 0.7548 RMSE: 0.9820\n",
      "Val: 0.9189 MAE: 0.7344 RMSE: 0.9586\n",
      "Epoch 88 Step 30260: Train 0.3209 Reg: 0.4097\n",
      "Test: 0.9645 MAE: 0.7546 RMSE: 0.9821\n",
      "Val: 0.9192 MAE: 0.7344 RMSE: 0.9587\n",
      "Epoch 89 Step 30600: Train 0.3207 Reg: 0.4094\n",
      "Test: 0.9651 MAE: 0.7548 RMSE: 0.9824\n",
      "Val: 0.9194 MAE: 0.7345 RMSE: 0.9589\n",
      "Epoch 90 Step 30940: Train 0.3205 Reg: 0.4091\n",
      "Test: 0.9653 MAE: 0.7548 RMSE: 0.9825\n",
      "Val: 0.9198 MAE: 0.7346 RMSE: 0.9590\n",
      "Epoch 91 Step 31280: Train 0.3203 Reg: 0.4088\n",
      "Test: 0.9658 MAE: 0.7550 RMSE: 0.9827\n",
      "Val: 0.9202 MAE: 0.7347 RMSE: 0.9592\n",
      "Epoch 92 Step 31620: Train 0.3202 Reg: 0.4085\n",
      "Test: 0.9665 MAE: 0.7556 RMSE: 0.9831\n",
      "Val: 0.9206 MAE: 0.7350 RMSE: 0.9595\n",
      "Epoch 93 Step 31960: Train 0.3200 Reg: 0.4082\n",
      "Test: 0.9661 MAE: 0.7549 RMSE: 0.9829\n",
      "Val: 0.9207 MAE: 0.7349 RMSE: 0.9596\n",
      "Epoch 94 Step 32300: Train 0.3199 Reg: 0.4080\n",
      "Test: 0.9665 MAE: 0.7552 RMSE: 0.9831\n",
      "Val: 0.9211 MAE: 0.7350 RMSE: 0.9598\n",
      "Epoch 95 Step 32640: Train 0.3197 Reg: 0.4077\n",
      "Test: 0.9669 MAE: 0.7552 RMSE: 0.9833\n",
      "Val: 0.9213 MAE: 0.7351 RMSE: 0.9599\n",
      "Epoch 96 Step 32980: Train 0.3196 Reg: 0.4075\n",
      "Test: 0.9670 MAE: 0.7552 RMSE: 0.9834\n",
      "Val: 0.9216 MAE: 0.7351 RMSE: 0.9600\n",
      "Epoch 97 Step 33320: Train 0.3195 Reg: 0.4073\n",
      "Test: 0.9679 MAE: 0.7558 RMSE: 0.9838\n",
      "Val: 0.9219 MAE: 0.7353 RMSE: 0.9601\n",
      "Epoch 98 Step 33660: Train 0.3193 Reg: 0.4071\n",
      "Test: 0.9678 MAE: 0.7557 RMSE: 0.9838\n",
      "Val: 0.9221 MAE: 0.7354 RMSE: 0.9603\n",
      "Epoch 99 Step 34000: Train 0.3192 Reg: 0.4069\n",
      "Test: 0.9675 MAE: 0.7553 RMSE: 0.9836\n",
      "Val: 0.9223 MAE: 0.7353 RMSE: 0.9604\n"
     ]
    }
   ],
   "source": [
    "!python pretrain-1m.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Dataset Info--------\n",
      "split way [threshold] with threshold 30 training_ratio 1.0\n",
      "train set size: support/query 366403/608666\n",
      "test set size: support/query 1046/4185\n",
      "Epoch 0: TrainLoss 0.8931 RecLoss: 0.0000 (left: 4:48:04)\n",
      "TestLoss: 0.8927 MAE: 0.7472 RMSE: 0.9448\n",
      "ValLoss: 0.8433 MAE: 0.7265 RMSE: 0.9183\n",
      "Epoch 1: TrainLoss 0.8482 RecLoss: 0.0000 (left: 4:49:13)\n",
      "TestLoss: 0.8934 MAE: 0.7508 RMSE: 0.9452\n",
      "ValLoss: 0.8411 MAE: 0.7264 RMSE: 0.9171\n",
      "Epoch 2: TrainLoss 0.8454 RecLoss: 0.0000 (left: 4:45:46)\n",
      "TestLoss: 0.8957 MAE: 0.7392 RMSE: 0.9464\n",
      "ValLoss: 0.8459 MAE: 0.7215 RMSE: 0.9197\n",
      "Epoch 3: TrainLoss 0.8439 RecLoss: 0.0000 (left: 4:38:06)\n",
      "TestLoss: 0.8901 MAE: 0.7490 RMSE: 0.9435\n",
      "ValLoss: 0.8407 MAE: 0.7282 RMSE: 0.9169\n",
      "Epoch 4: TrainLoss 0.8418 RecLoss: 0.0000 (left: 4:28:58)\n",
      "TestLoss: 0.8895 MAE: 0.7485 RMSE: 0.9431\n",
      "ValLoss: 0.8383 MAE: 0.7269 RMSE: 0.9156\n",
      "Epoch 5: TrainLoss 0.8388 RecLoss: 0.0000 (left: 4:19:59)\n",
      "TestLoss: 0.8912 MAE: 0.7452 RMSE: 0.9440\n",
      "ValLoss: 0.8359 MAE: 0.7238 RMSE: 0.9143\n",
      "Epoch 6: TrainLoss 0.8382 RecLoss: 0.0000 (left: 4:12:19)\n",
      "TestLoss: 0.8885 MAE: 0.7374 RMSE: 0.9426\n",
      "ValLoss: 0.8449 MAE: 0.7212 RMSE: 0.9192\n",
      "Epoch 7: TrainLoss 0.8373 RecLoss: 0.0000 (left: 4:06:37)\n",
      "TestLoss: 0.8869 MAE: 0.7445 RMSE: 0.9418\n",
      "ValLoss: 0.8362 MAE: 0.7246 RMSE: 0.9144\n",
      "Epoch 8: TrainLoss 0.8370 RecLoss: 0.0000 (left: 4:00:20)\n",
      "TestLoss: 0.8845 MAE: 0.7438 RMSE: 0.9405\n",
      "ValLoss: 0.8345 MAE: 0.7235 RMSE: 0.9135\n",
      "Epoch 9: TrainLoss 0.8361 RecLoss: 0.0000 (left: 3:55:32)\n",
      "TestLoss: 0.8891 MAE: 0.7464 RMSE: 0.9429\n",
      "ValLoss: 0.8376 MAE: 0.7238 RMSE: 0.9152\n",
      "Epoch 10: TrainLoss 0.8362 RecLoss: 0.0000 (left: 3:50:18)\n",
      "TestLoss: 0.8874 MAE: 0.7480 RMSE: 0.9420\n",
      "ValLoss: 0.8369 MAE: 0.7265 RMSE: 0.9148\n",
      "Epoch 11: TrainLoss 0.8351 RecLoss: 0.0000 (left: 3:46:11)\n",
      "TestLoss: 0.8839 MAE: 0.7427 RMSE: 0.9401\n",
      "ValLoss: 0.8334 MAE: 0.7221 RMSE: 0.9129\n",
      "Epoch 12: TrainLoss 0.8339 RecLoss: 0.0000 (left: 3:42:06)\n",
      "TestLoss: 0.8829 MAE: 0.7379 RMSE: 0.9396\n",
      "ValLoss: 0.8341 MAE: 0.7194 RMSE: 0.9133\n",
      "Epoch 13: TrainLoss 0.8339 RecLoss: 0.0000 (left: 3:38:33)\n",
      "TestLoss: 0.8845 MAE: 0.7398 RMSE: 0.9405\n",
      "ValLoss: 0.8370 MAE: 0.7202 RMSE: 0.9149\n",
      "Epoch 14: TrainLoss 0.8335 RecLoss: 0.0000 (left: 3:35:04)\n",
      "TestLoss: 0.8931 MAE: 0.7529 RMSE: 0.9450\n",
      "ValLoss: 0.8391 MAE: 0.7292 RMSE: 0.9160\n",
      "Epoch 15: TrainLoss 0.8329 RecLoss: 0.0000 (left: 3:31:48)\n",
      "TestLoss: 0.8868 MAE: 0.7397 RMSE: 0.9417\n",
      "ValLoss: 0.8337 MAE: 0.7187 RMSE: 0.9131\n",
      "Epoch 16: TrainLoss 0.8326 RecLoss: 0.0000 (left: 3:28:56)\n",
      "TestLoss: 0.8863 MAE: 0.7493 RMSE: 0.9414\n",
      "ValLoss: 0.8341 MAE: 0.7247 RMSE: 0.9133\n",
      "Epoch 17: TrainLoss 0.8321 RecLoss: 0.0000 (left: 3:25:55)\n",
      "TestLoss: 0.8836 MAE: 0.7453 RMSE: 0.9400\n",
      "ValLoss: 0.8318 MAE: 0.7227 RMSE: 0.9120\n",
      "Epoch 18: TrainLoss 0.8323 RecLoss: 0.0000 (left: 3:23:01)\n",
      "TestLoss: 0.8864 MAE: 0.7418 RMSE: 0.9415\n",
      "ValLoss: 0.8325 MAE: 0.7188 RMSE: 0.9124\n",
      "Epoch 19: TrainLoss 0.8314 RecLoss: 0.0000 (left: 3:20:30)\n",
      "TestLoss: 0.8825 MAE: 0.7383 RMSE: 0.9394\n",
      "ValLoss: 0.8361 MAE: 0.7183 RMSE: 0.9144\n",
      "Epoch 20: TrainLoss 0.8306 RecLoss: 0.0000 (left: 3:17:40)\n",
      "TestLoss: 0.8831 MAE: 0.7425 RMSE: 0.9398\n",
      "ValLoss: 0.8296 MAE: 0.7198 RMSE: 0.9108\n",
      "Epoch 21: TrainLoss 0.8306 RecLoss: 0.0000 (left: 3:14:52)\n",
      "TestLoss: 0.8819 MAE: 0.7429 RMSE: 0.9391\n",
      "ValLoss: 0.8332 MAE: 0.7208 RMSE: 0.9128\n",
      "Epoch 22: TrainLoss 0.8300 RecLoss: 0.0000 (left: 3:12:04)\n",
      "TestLoss: 0.8783 MAE: 0.7403 RMSE: 0.9372\n",
      "ValLoss: 0.8286 MAE: 0.7193 RMSE: 0.9103\n",
      "Epoch 23: TrainLoss 0.8298 RecLoss: 0.0000 (left: 3:09:15)\n",
      "TestLoss: 0.8976 MAE: 0.7547 RMSE: 0.9474\n",
      "ValLoss: 0.8332 MAE: 0.7260 RMSE: 0.9128\n",
      "Epoch 24: TrainLoss 0.8293 RecLoss: 0.0000 (left: 3:06:26)\n",
      "TestLoss: 0.8845 MAE: 0.7413 RMSE: 0.9405\n",
      "ValLoss: 0.8302 MAE: 0.7190 RMSE: 0.9112\n",
      "Epoch 25: TrainLoss 0.8294 RecLoss: 0.0000 (left: 3:03:37)\n",
      "TestLoss: 0.8836 MAE: 0.7389 RMSE: 0.9400\n",
      "ValLoss: 0.8345 MAE: 0.7190 RMSE: 0.9135\n",
      "Epoch 26: TrainLoss 0.8291 RecLoss: 0.0000 (left: 3:00:48)\n",
      "TestLoss: 0.8766 MAE: 0.7379 RMSE: 0.9363\n",
      "ValLoss: 0.8296 MAE: 0.7172 RMSE: 0.9108\n",
      "Epoch 27: TrainLoss 0.8283 RecLoss: 0.0000 (left: 2:58:10)\n",
      "TestLoss: 0.8897 MAE: 0.7492 RMSE: 0.9432\n",
      "ValLoss: 0.8293 MAE: 0.7225 RMSE: 0.9107\n",
      "Epoch 28: TrainLoss 0.8283 RecLoss: 0.0000 (left: 2:55:34)\n",
      "TestLoss: 0.8964 MAE: 0.7549 RMSE: 0.9468\n",
      "ValLoss: 0.8408 MAE: 0.7312 RMSE: 0.9170\n",
      "Epoch 29: TrainLoss 0.8284 RecLoss: 0.0000 (left: 2:52:56)\n",
      "TestLoss: 0.8808 MAE: 0.7431 RMSE: 0.9385\n",
      "ValLoss: 0.8293 MAE: 0.7204 RMSE: 0.9107\n",
      "Epoch 30: TrainLoss 0.8276 RecLoss: 0.0000 (left: 2:50:19)\n",
      "TestLoss: 0.8829 MAE: 0.7400 RMSE: 0.9397\n",
      "ValLoss: 0.8278 MAE: 0.7170 RMSE: 0.9098\n",
      "Epoch 31: TrainLoss 0.8270 RecLoss: 0.0000 (left: 2:47:30)\n",
      "TestLoss: 0.8833 MAE: 0.7382 RMSE: 0.9398\n",
      "ValLoss: 0.8308 MAE: 0.7165 RMSE: 0.9115\n",
      "Epoch 32: TrainLoss 0.8277 RecLoss: 0.0000 (left: 2:44:58)\n",
      "TestLoss: 0.8878 MAE: 0.7469 RMSE: 0.9422\n",
      "ValLoss: 0.8326 MAE: 0.7238 RMSE: 0.9125\n",
      "Epoch 33: TrainLoss 0.8272 RecLoss: 0.0000 (left: 2:42:25)\n",
      "TestLoss: 0.8822 MAE: 0.7433 RMSE: 0.9393\n",
      "ValLoss: 0.8320 MAE: 0.7235 RMSE: 0.9121\n",
      "Epoch 34: TrainLoss 0.8270 RecLoss: 0.0000 (left: 2:39:55)\n",
      "TestLoss: 0.8861 MAE: 0.7483 RMSE: 0.9413\n",
      "ValLoss: 0.8319 MAE: 0.7242 RMSE: 0.9121\n",
      "Epoch 35: TrainLoss 0.8266 RecLoss: 0.0000 (left: 2:37:23)\n",
      "TestLoss: 0.8851 MAE: 0.7352 RMSE: 0.9408\n",
      "ValLoss: 0.8313 MAE: 0.7154 RMSE: 0.9117\n",
      "Epoch 36: TrainLoss 0.8262 RecLoss: 0.0000 (left: 2:34:49)\n",
      "TestLoss: 0.8841 MAE: 0.7343 RMSE: 0.9403\n",
      "ValLoss: 0.8346 MAE: 0.7159 RMSE: 0.9136\n",
      "Epoch 37: TrainLoss 0.8265 RecLoss: 0.0000 (left: 2:32:17)\n",
      "TestLoss: 0.8778 MAE: 0.7341 RMSE: 0.9369\n",
      "ValLoss: 0.8318 MAE: 0.7160 RMSE: 0.9120\n",
      "Epoch 38: TrainLoss 0.8254 RecLoss: 0.0000 (left: 2:29:48)\n",
      "TestLoss: 0.8834 MAE: 0.7428 RMSE: 0.9399\n",
      "ValLoss: 0.8302 MAE: 0.7208 RMSE: 0.9112\n",
      "Epoch 39: TrainLoss 0.8251 RecLoss: 0.0000 (left: 2:27:13)\n",
      "TestLoss: 0.8779 MAE: 0.7385 RMSE: 0.9369\n",
      "ValLoss: 0.8259 MAE: 0.7166 RMSE: 0.9088\n",
      "Epoch 40: TrainLoss 0.8248 RecLoss: 0.0000 (left: 2:24:47)\n",
      "TestLoss: 0.8817 MAE: 0.7377 RMSE: 0.9390\n",
      "ValLoss: 0.8288 MAE: 0.7163 RMSE: 0.9104\n",
      "Epoch 41: TrainLoss 0.8249 RecLoss: 0.0000 (left: 2:22:22)\n",
      "TestLoss: 0.8797 MAE: 0.7402 RMSE: 0.9379\n",
      "ValLoss: 0.8287 MAE: 0.7188 RMSE: 0.9103\n",
      "Epoch 42: TrainLoss 0.8246 RecLoss: 0.0000 (left: 2:19:58)\n",
      "TestLoss: 0.8797 MAE: 0.7405 RMSE: 0.9379\n",
      "ValLoss: 0.8269 MAE: 0.7177 RMSE: 0.9093\n",
      "Epoch 43: TrainLoss 0.8249 RecLoss: 0.0000 (left: 2:17:35)\n",
      "TestLoss: 0.8804 MAE: 0.7395 RMSE: 0.9383\n",
      "ValLoss: 0.8267 MAE: 0.7175 RMSE: 0.9092\n",
      "Epoch 44: TrainLoss 0.8242 RecLoss: 0.0000 (left: 2:15:14)\n",
      "TestLoss: 0.8772 MAE: 0.7367 RMSE: 0.9366\n",
      "ValLoss: 0.8291 MAE: 0.7164 RMSE: 0.9105\n",
      "Epoch 45: TrainLoss 0.8240 RecLoss: 0.0000 (left: 2:12:42)\n",
      "TestLoss: 0.8823 MAE: 0.7464 RMSE: 0.9393\n",
      "ValLoss: 0.8288 MAE: 0.7225 RMSE: 0.9104\n",
      "Epoch 46: TrainLoss 0.8238 RecLoss: 0.0000 (left: 2:10:13)\n",
      "TestLoss: 0.8772 MAE: 0.7415 RMSE: 0.9366\n",
      "ValLoss: 0.8252 MAE: 0.7196 RMSE: 0.9084\n",
      "Epoch 47: TrainLoss 0.8236 RecLoss: 0.0000 (left: 2:07:40)\n",
      "TestLoss: 0.8901 MAE: 0.7380 RMSE: 0.9434\n",
      "ValLoss: 0.8347 MAE: 0.7157 RMSE: 0.9136\n",
      "Epoch 48: TrainLoss 0.8236 RecLoss: 0.0000 (left: 2:05:04)\n",
      "TestLoss: 0.8739 MAE: 0.7349 RMSE: 0.9348\n",
      "ValLoss: 0.8266 MAE: 0.7163 RMSE: 0.9092\n",
      "Epoch 49: TrainLoss 0.8230 RecLoss: 0.0000 (left: 2:02:38)\n",
      "TestLoss: 0.8748 MAE: 0.7324 RMSE: 0.9353\n",
      "ValLoss: 0.8265 MAE: 0.7142 RMSE: 0.9091\n",
      "Epoch 50: TrainLoss 0.8234 RecLoss: 0.0000 (left: 2:00:10)\n",
      "TestLoss: 0.8747 MAE: 0.7399 RMSE: 0.9353\n",
      "ValLoss: 0.8253 MAE: 0.7191 RMSE: 0.9085\n",
      "Epoch 51: TrainLoss 0.8227 RecLoss: 0.0000 (left: 1:57:37)\n",
      "TestLoss: 0.8796 MAE: 0.7415 RMSE: 0.9379\n",
      "ValLoss: 0.8278 MAE: 0.7190 RMSE: 0.9098\n",
      "Epoch 52: TrainLoss 0.8225 RecLoss: 0.0000 (left: 1:55:09)\n",
      "TestLoss: 0.8778 MAE: 0.7413 RMSE: 0.9369\n",
      "ValLoss: 0.8255 MAE: 0.7197 RMSE: 0.9086\n",
      "Epoch 53: TrainLoss 0.8221 RecLoss: 0.0000 (left: 1:52:43)\n",
      "TestLoss: 0.8771 MAE: 0.7422 RMSE: 0.9365\n",
      "ValLoss: 0.8262 MAE: 0.7207 RMSE: 0.9090\n",
      "Epoch 54: TrainLoss 0.8221 RecLoss: 0.0000 (left: 1:50:19)\n",
      "TestLoss: 0.8870 MAE: 0.7513 RMSE: 0.9418\n",
      "ValLoss: 0.8303 MAE: 0.7247 RMSE: 0.9112\n",
      "Epoch 55: TrainLoss 0.8219 RecLoss: 0.0000 (left: 1:47:54)\n",
      "TestLoss: 0.8787 MAE: 0.7393 RMSE: 0.9374\n",
      "ValLoss: 0.8256 MAE: 0.7174 RMSE: 0.9086\n",
      "Epoch 56: TrainLoss 0.8217 RecLoss: 0.0000 (left: 1:45:27)\n",
      "TestLoss: 0.8828 MAE: 0.7378 RMSE: 0.9396\n",
      "ValLoss: 0.8261 MAE: 0.7154 RMSE: 0.9089\n",
      "Epoch 57: TrainLoss 0.8214 RecLoss: 0.0000 (left: 1:43:04)\n",
      "TestLoss: 0.8799 MAE: 0.7380 RMSE: 0.9380\n",
      "ValLoss: 0.8262 MAE: 0.7163 RMSE: 0.9090\n",
      "Epoch 58: TrainLoss 0.8222 RecLoss: 0.0000 (left: 1:40:37)\n",
      "TestLoss: 0.8798 MAE: 0.7341 RMSE: 0.9380\n",
      "ValLoss: 0.8289 MAE: 0.7148 RMSE: 0.9104\n",
      "Epoch 59: TrainLoss 0.8219 RecLoss: 0.0000 (left: 1:38:11)\n",
      "TestLoss: 0.8820 MAE: 0.7383 RMSE: 0.9391\n",
      "ValLoss: 0.8273 MAE: 0.7166 RMSE: 0.9095\n",
      "Epoch 60: TrainLoss 0.8217 RecLoss: 0.0000 (left: 1:35:45)\n",
      "TestLoss: 0.8786 MAE: 0.7403 RMSE: 0.9374\n",
      "ValLoss: 0.8249 MAE: 0.7183 RMSE: 0.9082\n",
      "Epoch 61: TrainLoss 0.8211 RecLoss: 0.0000 (left: 1:33:19)\n",
      "TestLoss: 0.8806 MAE: 0.7406 RMSE: 0.9384\n",
      "ValLoss: 0.8259 MAE: 0.7172 RMSE: 0.9088\n",
      "Epoch 62: TrainLoss 0.8213 RecLoss: 0.0000 (left: 1:30:55)\n",
      "TestLoss: 0.8814 MAE: 0.7465 RMSE: 0.9388\n",
      "ValLoss: 0.8279 MAE: 0.7237 RMSE: 0.9099\n",
      "Epoch 63: TrainLoss 0.8203 RecLoss: 0.0000 (left: 1:28:29)\n",
      "TestLoss: 0.8763 MAE: 0.7349 RMSE: 0.9361\n",
      "ValLoss: 0.8244 MAE: 0.7138 RMSE: 0.9080\n",
      "Epoch 64: TrainLoss 0.8202 RecLoss: 0.0000 (left: 1:26:06)\n",
      "TestLoss: 0.8734 MAE: 0.7361 RMSE: 0.9346\n",
      "ValLoss: 0.8234 MAE: 0.7160 RMSE: 0.9074\n",
      "Epoch 65: TrainLoss 0.8200 RecLoss: 0.0000 (left: 1:23:43)\n",
      "TestLoss: 0.8758 MAE: 0.7369 RMSE: 0.9358\n",
      "ValLoss: 0.8266 MAE: 0.7169 RMSE: 0.9092\n",
      "Epoch 66: TrainLoss 0.8198 RecLoss: 0.0000 (left: 1:21:17)\n",
      "TestLoss: 0.8735 MAE: 0.7354 RMSE: 0.9346\n",
      "ValLoss: 0.8250 MAE: 0.7168 RMSE: 0.9083\n",
      "Epoch 67: TrainLoss 0.8202 RecLoss: 0.0000 (left: 1:18:51)\n",
      "TestLoss: 0.8798 MAE: 0.7423 RMSE: 0.9380\n",
      "ValLoss: 0.8239 MAE: 0.7191 RMSE: 0.9077\n",
      "Epoch 68: TrainLoss 0.8198 RecLoss: 0.0000 (left: 1:16:26)\n",
      "TestLoss: 0.8759 MAE: 0.7340 RMSE: 0.9359\n",
      "ValLoss: 0.8249 MAE: 0.7148 RMSE: 0.9082\n",
      "Epoch 69: TrainLoss 0.8193 RecLoss: 0.0000 (left: 1:14:01)\n",
      "TestLoss: 0.8756 MAE: 0.7381 RMSE: 0.9357\n",
      "ValLoss: 0.8230 MAE: 0.7165 RMSE: 0.9072\n",
      "Epoch 70: TrainLoss 0.8199 RecLoss: 0.0000 (left: 1:11:41)\n",
      "TestLoss: 0.8760 MAE: 0.7360 RMSE: 0.9359\n",
      "ValLoss: 0.8235 MAE: 0.7150 RMSE: 0.9075\n",
      "Epoch 71: TrainLoss 0.8189 RecLoss: 0.0000 (left: 1:09:18)\n",
      "TestLoss: 0.8752 MAE: 0.7354 RMSE: 0.9355\n",
      "ValLoss: 0.8241 MAE: 0.7148 RMSE: 0.9078\n",
      "Epoch 72: TrainLoss 0.8193 RecLoss: 0.0000 (left: 1:06:56)\n",
      "TestLoss: 0.8738 MAE: 0.7339 RMSE: 0.9348\n",
      "ValLoss: 0.8279 MAE: 0.7153 RMSE: 0.9099\n",
      "Epoch 73: TrainLoss 0.8190 RecLoss: 0.0000 (left: 1:04:39)\n",
      "TestLoss: 0.8776 MAE: 0.7368 RMSE: 0.9368\n",
      "ValLoss: 0.8231 MAE: 0.7146 RMSE: 0.9072\n",
      "Epoch 74: TrainLoss 0.8185 RecLoss: 0.0000 (left: 1:02:20)\n",
      "TestLoss: 0.8835 MAE: 0.7471 RMSE: 0.9400\n",
      "ValLoss: 0.8265 MAE: 0.7224 RMSE: 0.9091\n",
      "Epoch 75: TrainLoss 0.8188 RecLoss: 0.0000 (left: 0:59:55)\n",
      "TestLoss: 0.8763 MAE: 0.7385 RMSE: 0.9361\n",
      "ValLoss: 0.8248 MAE: 0.7159 RMSE: 0.9082\n",
      "Epoch 76: TrainLoss 0.8185 RecLoss: 0.0000 (left: 0:57:31)\n",
      "TestLoss: 0.8759 MAE: 0.7359 RMSE: 0.9359\n",
      "ValLoss: 0.8232 MAE: 0.7144 RMSE: 0.9073\n",
      "Epoch 77: TrainLoss 0.8183 RecLoss: 0.0000 (left: 0:55:07)\n",
      "TestLoss: 0.8774 MAE: 0.7387 RMSE: 0.9367\n",
      "ValLoss: 0.8246 MAE: 0.7160 RMSE: 0.9081\n",
      "Epoch 78: TrainLoss 0.8187 RecLoss: 0.0000 (left: 0:52:43)\n",
      "TestLoss: 0.8698 MAE: 0.7343 RMSE: 0.9326\n",
      "ValLoss: 0.8231 MAE: 0.7153 RMSE: 0.9073\n",
      "Epoch 79: TrainLoss 0.8177 RecLoss: 0.0000 (left: 0:50:20)\n",
      "TestLoss: 0.8806 MAE: 0.7405 RMSE: 0.9384\n",
      "ValLoss: 0.8236 MAE: 0.7162 RMSE: 0.9075\n",
      "Epoch 80: TrainLoss 0.8178 RecLoss: 0.0000 (left: 0:47:56)\n",
      "TestLoss: 0.8788 MAE: 0.7406 RMSE: 0.9374\n",
      "ValLoss: 0.8251 MAE: 0.7190 RMSE: 0.9083\n",
      "Epoch 81: TrainLoss 0.8177 RecLoss: 0.0000 (left: 0:45:33)\n",
      "TestLoss: 0.8749 MAE: 0.7330 RMSE: 0.9354\n",
      "ValLoss: 0.8237 MAE: 0.7130 RMSE: 0.9076\n",
      "Epoch 82: TrainLoss 0.8173 RecLoss: 0.0000 (left: 0:43:08)\n",
      "TestLoss: 0.8772 MAE: 0.7405 RMSE: 0.9366\n",
      "ValLoss: 0.8234 MAE: 0.7171 RMSE: 0.9074\n",
      "Epoch 83: TrainLoss 0.8173 RecLoss: 0.0000 (left: 0:40:42)\n",
      "TestLoss: 0.8714 MAE: 0.7350 RMSE: 0.9335\n",
      "ValLoss: 0.8250 MAE: 0.7157 RMSE: 0.9083\n",
      "Epoch 84: TrainLoss 0.8170 RecLoss: 0.0000 (left: 0:38:18)\n",
      "TestLoss: 0.8744 MAE: 0.7325 RMSE: 0.9351\n",
      "ValLoss: 0.8275 MAE: 0.7137 RMSE: 0.9096\n",
      "Epoch 85: TrainLoss 0.8172 RecLoss: 0.0000 (left: 0:35:53)\n",
      "TestLoss: 0.8763 MAE: 0.7372 RMSE: 0.9361\n",
      "ValLoss: 0.8227 MAE: 0.7159 RMSE: 0.9070\n",
      "Epoch 86: TrainLoss 0.8170 RecLoss: 0.0000 (left: 0:33:28)\n",
      "TestLoss: 0.8759 MAE: 0.7376 RMSE: 0.9359\n",
      "ValLoss: 0.8218 MAE: 0.7150 RMSE: 0.9065\n",
      "Epoch 87: TrainLoss 0.8165 RecLoss: 0.0000 (left: 0:31:04)\n",
      "TestLoss: 0.8767 MAE: 0.7379 RMSE: 0.9363\n",
      "ValLoss: 0.8209 MAE: 0.7141 RMSE: 0.9060\n",
      "Epoch 88: TrainLoss 0.8169 RecLoss: 0.0000 (left: 0:28:40)\n",
      "TestLoss: 0.8772 MAE: 0.7356 RMSE: 0.9366\n",
      "ValLoss: 0.8241 MAE: 0.7139 RMSE: 0.9078\n",
      "Epoch 89: TrainLoss 0.8164 RecLoss: 0.0000 (left: 0:26:16)\n",
      "TestLoss: 0.8752 MAE: 0.7356 RMSE: 0.9355\n",
      "ValLoss: 0.8247 MAE: 0.7152 RMSE: 0.9081\n",
      "Epoch 90: TrainLoss 0.8162 RecLoss: 0.0000 (left: 0:23:52)\n",
      "TestLoss: 0.8742 MAE: 0.7336 RMSE: 0.9350\n",
      "ValLoss: 0.8252 MAE: 0.7140 RMSE: 0.9084\n",
      "Epoch 91: TrainLoss 0.8163 RecLoss: 0.0000 (left: 0:21:29)\n",
      "TestLoss: 0.8721 MAE: 0.7368 RMSE: 0.9338\n",
      "ValLoss: 0.8221 MAE: 0.7159 RMSE: 0.9067\n",
      "Epoch 92: TrainLoss 0.8159 RecLoss: 0.0000 (left: 0:19:06)\n",
      "TestLoss: 0.8763 MAE: 0.7343 RMSE: 0.9361\n",
      "ValLoss: 0.8238 MAE: 0.7133 RMSE: 0.9077\n",
      "Epoch 93: TrainLoss 0.8158 RecLoss: 0.0000 (left: 0:16:42)\n",
      "TestLoss: 0.8779 MAE: 0.7355 RMSE: 0.9370\n",
      "ValLoss: 0.8244 MAE: 0.7137 RMSE: 0.9079\n",
      "Epoch 94: TrainLoss 0.8157 RecLoss: 0.0000 (left: 0:14:19)\n",
      "TestLoss: 0.8768 MAE: 0.7408 RMSE: 0.9364\n",
      "ValLoss: 0.8229 MAE: 0.7177 RMSE: 0.9071\n",
      "Epoch 95: TrainLoss 0.8154 RecLoss: 0.0000 (left: 0:11:55)\n",
      "TestLoss: 0.8757 MAE: 0.7402 RMSE: 0.9358\n",
      "ValLoss: 0.8218 MAE: 0.7173 RMSE: 0.9065\n",
      "Epoch 96: TrainLoss 0.8156 RecLoss: 0.0000 (left: 0:09:32)\n",
      "TestLoss: 0.8701 MAE: 0.7345 RMSE: 0.9328\n",
      "ValLoss: 0.8220 MAE: 0.7143 RMSE: 0.9067\n",
      "Epoch 97: TrainLoss 0.8157 RecLoss: 0.0000 (left: 0:07:09)\n",
      "TestLoss: 0.8730 MAE: 0.7332 RMSE: 0.9343\n",
      "ValLoss: 0.8245 MAE: 0.7142 RMSE: 0.9080\n",
      "Epoch 98: TrainLoss 0.8148 RecLoss: 0.0000 (left: 0:04:46)\n",
      "TestLoss: 0.8737 MAE: 0.7374 RMSE: 0.9347\n",
      "ValLoss: 0.8219 MAE: 0.7162 RMSE: 0.9066\n",
      "Epoch 99: TrainLoss 0.8146 RecLoss: 0.0000 (left: 0:02:22)\n",
      "TestLoss: 0.8781 MAE: 0.7329 RMSE: 0.9371\n",
      "ValLoss: 0.8287 MAE: 0.7140 RMSE: 0.9104\n"
     ]
    }
   ],
   "source": [
    "!python train-1m.py\n",
    "# !python test-1m.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### this contains the result of 20% core user as support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra : False\n",
      "-------Dataset Info--------\n",
      "split way [threshold] with threshold 30 training_ratio 1.0\n",
      "train set size: support/query 366403/608666\n",
      "test set size: support/query 1046/4185\n",
      "USER HIS DICT: 6040\n",
      "NUM IS: 6040\n",
      "Key Test Result: MAE: 0.6678 RMSE: 0.8377 NDCG: 0.0000\n",
      "CORE IS SELECTED:\n",
      "USER HIS DICT: 6040\n",
      "NUM IS: 6040\n",
      "Que Test Result: MAE: 0.7366 RMSE: 0.9350 NDCG: 0.0000\n",
      "All Test Result: MAE: 0.7228 RMSE: 0.9164 NDCG: 0.0000\n"
     ]
    }
   ],
   "source": [
    "!python test-1m.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 40% core users in support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Dataset Info--------\n",
      "split way [threshold] with threshold 30 training_ratio 1.0\n",
      "train set size: support/query 558297/416772\n",
      "test set size: support/query 2092/3139\n",
      "Epoch 0 Step 518: Train 1.8030 Reg: 0.6223\n",
      "Test: 0.8437 MAE: 0.7293 RMSE: 0.9185\n",
      "Val: 0.7883 MAE: 0.7048 RMSE: 0.8879\n",
      "Epoch 1 Step 1036: Train 0.7930 Reg: 0.4323\n",
      "Test: 0.8275 MAE: 0.7253 RMSE: 0.9097\n",
      "Val: 0.7850 MAE: 0.7049 RMSE: 0.8860\n",
      "Epoch 2 Step 1554: Train 0.7886 Reg: 0.3444\n",
      "Test: 0.8237 MAE: 0.7153 RMSE: 0.9076\n",
      "Val: 0.7790 MAE: 0.7019 RMSE: 0.8826\n",
      "Epoch 3 Step 2072: Train 0.7845 Reg: 0.3109\n",
      "Test: 0.8173 MAE: 0.7143 RMSE: 0.9040\n",
      "Val: 0.7762 MAE: 0.6987 RMSE: 0.8810\n",
      "Epoch 4 Step 2590: Train 0.7797 Reg: 0.2907\n",
      "Test: 0.8262 MAE: 0.7206 RMSE: 0.9090\n",
      "Val: 0.7708 MAE: 0.6982 RMSE: 0.8780\n",
      "Epoch 5 Step 3108: Train 0.7714 Reg: 0.2795\n",
      "Test: 0.8196 MAE: 0.7193 RMSE: 0.9053\n",
      "Val: 0.7583 MAE: 0.6914 RMSE: 0.8708\n",
      "Epoch 6 Step 3626: Train 0.7590 Reg: 0.2817\n",
      "Test: 0.8102 MAE: 0.7129 RMSE: 0.9001\n",
      "Val: 0.7514 MAE: 0.6844 RMSE: 0.8668\n",
      "Epoch 7 Step 4144: Train 0.7407 Reg: 0.2937\n",
      "Test: 0.7948 MAE: 0.7100 RMSE: 0.8915\n",
      "Val: 0.7272 MAE: 0.6749 RMSE: 0.8528\n",
      "Epoch 8 Step 4662: Train 0.7199 Reg: 0.3140\n",
      "Test: 0.7731 MAE: 0.6997 RMSE: 0.8793\n",
      "Val: 0.7103 MAE: 0.6669 RMSE: 0.8428\n",
      "Epoch 9 Step 5180: Train 0.6993 Reg: 0.3405\n",
      "Test: 0.7505 MAE: 0.6901 RMSE: 0.8663\n",
      "Val: 0.7001 MAE: 0.6597 RMSE: 0.8367\n",
      "Epoch 10 Step 5698: Train 0.6829 Reg: 0.3599\n",
      "Test: 0.7359 MAE: 0.6863 RMSE: 0.8579\n",
      "Val: 0.6899 MAE: 0.6564 RMSE: 0.8306\n",
      "Epoch 11 Step 6216: Train 0.6672 Reg: 0.3703\n",
      "Test: 0.7276 MAE: 0.6821 RMSE: 0.8530\n",
      "Val: 0.6813 MAE: 0.6510 RMSE: 0.8254\n",
      "Epoch 12 Step 6734: Train 0.6483 Reg: 0.3989\n",
      "Test: 0.7139 MAE: 0.6759 RMSE: 0.8449\n",
      "Val: 0.6713 MAE: 0.6453 RMSE: 0.8193\n",
      "Epoch 13 Step 7252: Train 0.6280 Reg: 0.4238\n",
      "Test: 0.7083 MAE: 0.6700 RMSE: 0.8416\n",
      "Val: 0.6679 MAE: 0.6427 RMSE: 0.8172\n",
      "Epoch 14 Step 7770: Train 0.6091 Reg: 0.4470\n",
      "Test: 0.7007 MAE: 0.6592 RMSE: 0.8371\n",
      "Val: 0.6682 MAE: 0.6411 RMSE: 0.8174\n",
      "Epoch 15 Step 8288: Train 0.5918 Reg: 0.4680\n",
      "Test: 0.7098 MAE: 0.6667 RMSE: 0.8425\n",
      "Val: 0.6700 MAE: 0.6433 RMSE: 0.8185\n",
      "Epoch 16 Step 8806: Train 0.5753 Reg: 0.4863\n",
      "Test: 0.7088 MAE: 0.6661 RMSE: 0.8419\n",
      "Val: 0.6724 MAE: 0.6436 RMSE: 0.8200\n",
      "Epoch 17 Step 9324: Train 0.5593 Reg: 0.5034\n",
      "Test: 0.7140 MAE: 0.6686 RMSE: 0.8450\n",
      "Val: 0.6768 MAE: 0.6448 RMSE: 0.8227\n",
      "Epoch 18 Step 9842: Train 0.5426 Reg: 0.5210\n",
      "Test: 0.7196 MAE: 0.6694 RMSE: 0.8483\n",
      "Val: 0.6868 MAE: 0.6481 RMSE: 0.8287\n",
      "Epoch 19 Step 10360: Train 0.5277 Reg: 0.5324\n",
      "Test: 0.7297 MAE: 0.6752 RMSE: 0.8542\n",
      "Val: 0.6938 MAE: 0.6511 RMSE: 0.8329\n",
      "Epoch 20 Step 10878: Train 0.5144 Reg: 0.5429\n",
      "Test: 0.7346 MAE: 0.6752 RMSE: 0.8571\n",
      "Val: 0.7025 MAE: 0.6544 RMSE: 0.8382\n",
      "Epoch 21 Step 11396: Train 0.5013 Reg: 0.5517\n",
      "Test: 0.7459 MAE: 0.6809 RMSE: 0.8636\n",
      "Val: 0.7096 MAE: 0.6572 RMSE: 0.8424\n",
      "Epoch 22 Step 11914: Train 0.4891 Reg: 0.5576\n",
      "Test: 0.7504 MAE: 0.6827 RMSE: 0.8662\n",
      "Val: 0.7161 MAE: 0.6608 RMSE: 0.8462\n",
      "Epoch 23 Step 12432: Train 0.4790 Reg: 0.5594\n",
      "Test: 0.7537 MAE: 0.6822 RMSE: 0.8682\n",
      "Val: 0.7238 MAE: 0.6635 RMSE: 0.8508\n",
      "Epoch 24 Step 12950: Train 0.4708 Reg: 0.5597\n",
      "Test: 0.7651 MAE: 0.6884 RMSE: 0.8747\n",
      "Val: 0.7304 MAE: 0.6664 RMSE: 0.8546\n",
      "Epoch 25 Step 13468: Train 0.4628 Reg: 0.5607\n",
      "Test: 0.7695 MAE: 0.6895 RMSE: 0.8772\n",
      "Val: 0.7395 MAE: 0.6703 RMSE: 0.8599\n",
      "Epoch 26 Step 13986: Train 0.4543 Reg: 0.5621\n",
      "Test: 0.7737 MAE: 0.6892 RMSE: 0.8796\n",
      "Val: 0.7464 MAE: 0.6723 RMSE: 0.8640\n",
      "Epoch 27 Step 14504: Train 0.4464 Reg: 0.5616\n",
      "Test: 0.7760 MAE: 0.6885 RMSE: 0.8809\n",
      "Val: 0.7540 MAE: 0.6745 RMSE: 0.8683\n",
      "Epoch 28 Step 15022: Train 0.4401 Reg: 0.5594\n",
      "Test: 0.7866 MAE: 0.6961 RMSE: 0.8869\n",
      "Val: 0.7591 MAE: 0.6774 RMSE: 0.8713\n",
      "Epoch 29 Step 15540: Train 0.4349 Reg: 0.5561\n",
      "Test: 0.7943 MAE: 0.6982 RMSE: 0.8912\n",
      "Val: 0.7656 MAE: 0.6792 RMSE: 0.8750\n",
      "Epoch 30 Step 16058: Train 0.4302 Reg: 0.5523\n",
      "Test: 0.7969 MAE: 0.7003 RMSE: 0.8927\n",
      "Val: 0.7729 MAE: 0.6825 RMSE: 0.8792\n",
      "Epoch 31 Step 16576: Train 0.4262 Reg: 0.5480\n",
      "Test: 0.8001 MAE: 0.6995 RMSE: 0.8945\n",
      "Val: 0.7767 MAE: 0.6831 RMSE: 0.8813\n",
      "Epoch 32 Step 17094: Train 0.4227 Reg: 0.5430\n",
      "Test: 0.8068 MAE: 0.7024 RMSE: 0.8982\n",
      "Val: 0.7854 MAE: 0.6871 RMSE: 0.8862\n",
      "Epoch 33 Step 17612: Train 0.4194 Reg: 0.5389\n",
      "Test: 0.8079 MAE: 0.7029 RMSE: 0.8988\n",
      "Val: 0.7879 MAE: 0.6886 RMSE: 0.8876\n",
      "Epoch 34 Step 18130: Train 0.4163 Reg: 0.5344\n",
      "Test: 0.8118 MAE: 0.7028 RMSE: 0.9010\n",
      "Val: 0.7933 MAE: 0.6895 RMSE: 0.8907\n",
      "Epoch 35 Step 18648: Train 0.4133 Reg: 0.5301\n",
      "Test: 0.8231 MAE: 0.7101 RMSE: 0.9073\n",
      "Val: 0.7992 MAE: 0.6927 RMSE: 0.8940\n",
      "Epoch 36 Step 19166: Train 0.4105 Reg: 0.5256\n",
      "Test: 0.8242 MAE: 0.7074 RMSE: 0.9078\n",
      "Val: 0.8029 MAE: 0.6934 RMSE: 0.8960\n",
      "Epoch 37 Step 19684: Train 0.4075 Reg: 0.5220\n",
      "Test: 0.8265 MAE: 0.7080 RMSE: 0.9091\n",
      "Val: 0.8054 MAE: 0.6937 RMSE: 0.8974\n",
      "Epoch 38 Step 20202: Train 0.4042 Reg: 0.5187\n",
      "Test: 0.8321 MAE: 0.7093 RMSE: 0.9122\n",
      "Val: 0.8100 MAE: 0.6947 RMSE: 0.9000\n",
      "Epoch 39 Step 20720: Train 0.4009 Reg: 0.5156\n",
      "Test: 0.8333 MAE: 0.7088 RMSE: 0.9128\n",
      "Val: 0.8139 MAE: 0.6961 RMSE: 0.9022\n",
      "Epoch 40 Step 21238: Train 0.3980 Reg: 0.5121\n",
      "Test: 0.8408 MAE: 0.7136 RMSE: 0.9169\n",
      "Val: 0.8175 MAE: 0.6983 RMSE: 0.9042\n",
      "Epoch 41 Step 21756: Train 0.3952 Reg: 0.5088\n",
      "Test: 0.8433 MAE: 0.7129 RMSE: 0.9183\n",
      "Val: 0.8218 MAE: 0.6993 RMSE: 0.9065\n",
      "Epoch 42 Step 22274: Train 0.3929 Reg: 0.5051\n",
      "Test: 0.8521 MAE: 0.7186 RMSE: 0.9231\n",
      "Val: 0.8251 MAE: 0.7012 RMSE: 0.9084\n",
      "Epoch 43 Step 22792: Train 0.3907 Reg: 0.5017\n",
      "Test: 0.8477 MAE: 0.7147 RMSE: 0.9207\n",
      "Val: 0.8285 MAE: 0.7013 RMSE: 0.9102\n",
      "Epoch 44 Step 23310: Train 0.3886 Reg: 0.4982\n",
      "Test: 0.8560 MAE: 0.7189 RMSE: 0.9252\n",
      "Val: 0.8320 MAE: 0.7035 RMSE: 0.9121\n",
      "Epoch 45 Step 23828: Train 0.3866 Reg: 0.4949\n",
      "Test: 0.8542 MAE: 0.7162 RMSE: 0.9242\n",
      "Val: 0.8356 MAE: 0.7042 RMSE: 0.9141\n",
      "Epoch 46 Step 24346: Train 0.3848 Reg: 0.4915\n",
      "Test: 0.8606 MAE: 0.7200 RMSE: 0.9277\n",
      "Val: 0.8371 MAE: 0.7049 RMSE: 0.9149\n",
      "Epoch 47 Step 24864: Train 0.3831 Reg: 0.4883\n",
      "Test: 0.8621 MAE: 0.7194 RMSE: 0.9285\n",
      "Val: 0.8403 MAE: 0.7061 RMSE: 0.9167\n",
      "Epoch 48 Step 25382: Train 0.3814 Reg: 0.4852\n",
      "Test: 0.8648 MAE: 0.7207 RMSE: 0.9299\n",
      "Val: 0.8433 MAE: 0.7071 RMSE: 0.9183\n",
      "Epoch 49 Step 25900: Train 0.3799 Reg: 0.4822\n",
      "Test: 0.8670 MAE: 0.7212 RMSE: 0.9311\n",
      "Val: 0.8459 MAE: 0.7080 RMSE: 0.9197\n",
      "Epoch 50 Step 26418: Train 0.3784 Reg: 0.4793\n",
      "Test: 0.8721 MAE: 0.7239 RMSE: 0.9339\n",
      "Val: 0.8489 MAE: 0.7096 RMSE: 0.9214\n",
      "Epoch 51 Step 26936: Train 0.3769 Reg: 0.4765\n",
      "Test: 0.8725 MAE: 0.7231 RMSE: 0.9341\n",
      "Val: 0.8506 MAE: 0.7097 RMSE: 0.9223\n",
      "Epoch 52 Step 27454: Train 0.3755 Reg: 0.4738\n",
      "Test: 0.8776 MAE: 0.7261 RMSE: 0.9368\n",
      "Val: 0.8534 MAE: 0.7110 RMSE: 0.9238\n",
      "Epoch 53 Step 27972: Train 0.3742 Reg: 0.4713\n",
      "Test: 0.8820 MAE: 0.7279 RMSE: 0.9391\n",
      "Val: 0.8553 MAE: 0.7119 RMSE: 0.9248\n",
      "Epoch 54 Step 28490: Train 0.3729 Reg: 0.4688\n",
      "Test: 0.8796 MAE: 0.7256 RMSE: 0.9379\n",
      "Val: 0.8568 MAE: 0.7118 RMSE: 0.9257\n",
      "Epoch 55 Step 29008: Train 0.3717 Reg: 0.4665\n",
      "Test: 0.8859 MAE: 0.7286 RMSE: 0.9412\n",
      "Val: 0.8594 MAE: 0.7131 RMSE: 0.9271\n",
      "Epoch 56 Step 29526: Train 0.3705 Reg: 0.4641\n",
      "Test: 0.8867 MAE: 0.7280 RMSE: 0.9416\n",
      "Val: 0.8618 MAE: 0.7136 RMSE: 0.9283\n",
      "Epoch 57 Step 30044: Train 0.3695 Reg: 0.4620\n",
      "Test: 0.8917 MAE: 0.7302 RMSE: 0.9443\n",
      "Val: 0.8636 MAE: 0.7144 RMSE: 0.9293\n",
      "Epoch 58 Step 30562: Train 0.3684 Reg: 0.4599\n",
      "Test: 0.8923 MAE: 0.7303 RMSE: 0.9446\n",
      "Val: 0.8647 MAE: 0.7148 RMSE: 0.9299\n",
      "Epoch 59 Step 31080: Train 0.3674 Reg: 0.4579\n",
      "Test: 0.8921 MAE: 0.7296 RMSE: 0.9445\n",
      "Val: 0.8666 MAE: 0.7153 RMSE: 0.9309\n",
      "Epoch 60 Step 31598: Train 0.3664 Reg: 0.4559\n",
      "Test: 0.8949 MAE: 0.7312 RMSE: 0.9460\n",
      "Val: 0.8682 MAE: 0.7159 RMSE: 0.9318\n",
      "Epoch 61 Step 32116: Train 0.3655 Reg: 0.4541\n",
      "Test: 0.8977 MAE: 0.7320 RMSE: 0.9475\n",
      "Val: 0.8700 MAE: 0.7164 RMSE: 0.9328\n",
      "Epoch 62 Step 32634: Train 0.3646 Reg: 0.4523\n",
      "Test: 0.9019 MAE: 0.7342 RMSE: 0.9497\n",
      "Val: 0.8717 MAE: 0.7174 RMSE: 0.9336\n",
      "Epoch 63 Step 33152: Train 0.3637 Reg: 0.4506\n",
      "Test: 0.9014 MAE: 0.7339 RMSE: 0.9494\n",
      "Val: 0.8727 MAE: 0.7176 RMSE: 0.9342\n",
      "Epoch 64 Step 33670: Train 0.3630 Reg: 0.4491\n",
      "Test: 0.9057 MAE: 0.7354 RMSE: 0.9517\n",
      "Val: 0.8743 MAE: 0.7184 RMSE: 0.9351\n",
      "Epoch 65 Step 34188: Train 0.3622 Reg: 0.4475\n",
      "Test: 0.9064 MAE: 0.7361 RMSE: 0.9521\n",
      "Val: 0.8757 MAE: 0.7188 RMSE: 0.9358\n",
      "Epoch 66 Step 34706: Train 0.3614 Reg: 0.4461\n",
      "Test: 0.9084 MAE: 0.7368 RMSE: 0.9531\n",
      "Val: 0.8775 MAE: 0.7195 RMSE: 0.9367\n",
      "Epoch 67 Step 35224: Train 0.3607 Reg: 0.4447\n",
      "Test: 0.9097 MAE: 0.7371 RMSE: 0.9538\n",
      "Val: 0.8777 MAE: 0.7195 RMSE: 0.9369\n",
      "Epoch 68 Step 35742: Train 0.3600 Reg: 0.4433\n",
      "Test: 0.9092 MAE: 0.7365 RMSE: 0.9535\n",
      "Val: 0.8787 MAE: 0.7196 RMSE: 0.9374\n",
      "Epoch 69 Step 36260: Train 0.3594 Reg: 0.4421\n",
      "Test: 0.9118 MAE: 0.7373 RMSE: 0.9549\n",
      "Val: 0.8801 MAE: 0.7201 RMSE: 0.9381\n",
      "Epoch 70 Step 36778: Train 0.3588 Reg: 0.4408\n",
      "Test: 0.9106 MAE: 0.7359 RMSE: 0.9543\n",
      "Val: 0.8812 MAE: 0.7202 RMSE: 0.9387\n",
      "Epoch 71 Step 37296: Train 0.3582 Reg: 0.4396\n",
      "Test: 0.9134 MAE: 0.7377 RMSE: 0.9557\n",
      "Val: 0.8820 MAE: 0.7207 RMSE: 0.9391\n",
      "Epoch 72 Step 37814: Train 0.3576 Reg: 0.4386\n",
      "Test: 0.9143 MAE: 0.7377 RMSE: 0.9562\n",
      "Val: 0.8830 MAE: 0.7210 RMSE: 0.9397\n",
      "Epoch 73 Step 38332: Train 0.3571 Reg: 0.4375\n",
      "Test: 0.9165 MAE: 0.7397 RMSE: 0.9574\n",
      "Val: 0.8837 MAE: 0.7216 RMSE: 0.9401\n",
      "Epoch 74 Step 38850: Train 0.3566 Reg: 0.4365\n",
      "Test: 0.9164 MAE: 0.7385 RMSE: 0.9573\n",
      "Val: 0.8845 MAE: 0.7215 RMSE: 0.9405\n",
      "Epoch 75 Step 39368: Train 0.3561 Reg: 0.4355\n",
      "Test: 0.9182 MAE: 0.7394 RMSE: 0.9582\n",
      "Val: 0.8853 MAE: 0.7218 RMSE: 0.9409\n",
      "Epoch 76 Step 39886: Train 0.3556 Reg: 0.4346\n",
      "Test: 0.9181 MAE: 0.7390 RMSE: 0.9582\n",
      "Val: 0.8860 MAE: 0.7220 RMSE: 0.9413\n",
      "Epoch 77 Step 40404: Train 0.3552 Reg: 0.4337\n",
      "Test: 0.9202 MAE: 0.7399 RMSE: 0.9593\n",
      "Val: 0.8867 MAE: 0.7223 RMSE: 0.9417\n",
      "Epoch 78 Step 40922: Train 0.3547 Reg: 0.4329\n",
      "Test: 0.9229 MAE: 0.7416 RMSE: 0.9607\n",
      "Val: 0.8878 MAE: 0.7230 RMSE: 0.9422\n",
      "Epoch 79 Step 41440: Train 0.3544 Reg: 0.4321\n",
      "Test: 0.9227 MAE: 0.7412 RMSE: 0.9606\n",
      "Val: 0.8881 MAE: 0.7229 RMSE: 0.9424\n",
      "Epoch 80 Step 41958: Train 0.3539 Reg: 0.4313\n",
      "Test: 0.9235 MAE: 0.7414 RMSE: 0.9610\n",
      "Val: 0.8887 MAE: 0.7231 RMSE: 0.9427\n",
      "Epoch 81 Step 42476: Train 0.3536 Reg: 0.4306\n",
      "Test: 0.9247 MAE: 0.7421 RMSE: 0.9616\n",
      "Val: 0.8893 MAE: 0.7234 RMSE: 0.9430\n",
      "Epoch 82 Step 42994: Train 0.3532 Reg: 0.4299\n",
      "Test: 0.9246 MAE: 0.7415 RMSE: 0.9616\n",
      "Val: 0.8900 MAE: 0.7234 RMSE: 0.9434\n",
      "Epoch 83 Step 43512: Train 0.3529 Reg: 0.4293\n",
      "Test: 0.9259 MAE: 0.7422 RMSE: 0.9622\n",
      "Val: 0.8905 MAE: 0.7237 RMSE: 0.9437\n",
      "Epoch 84 Step 44030: Train 0.3526 Reg: 0.4286\n",
      "Test: 0.9266 MAE: 0.7425 RMSE: 0.9626\n",
      "Val: 0.8910 MAE: 0.7240 RMSE: 0.9440\n",
      "Epoch 85 Step 44548: Train 0.3523 Reg: 0.4280\n",
      "Test: 0.9266 MAE: 0.7422 RMSE: 0.9626\n",
      "Val: 0.8914 MAE: 0.7239 RMSE: 0.9441\n",
      "Epoch 86 Step 45066: Train 0.3520 Reg: 0.4275\n",
      "Test: 0.9279 MAE: 0.7428 RMSE: 0.9633\n",
      "Val: 0.8919 MAE: 0.7242 RMSE: 0.9444\n",
      "Epoch 87 Step 45584: Train 0.3517 Reg: 0.4269\n",
      "Test: 0.9290 MAE: 0.7433 RMSE: 0.9639\n",
      "Val: 0.8923 MAE: 0.7244 RMSE: 0.9446\n",
      "Epoch 88 Step 46102: Train 0.3514 Reg: 0.4264\n",
      "Test: 0.9300 MAE: 0.7439 RMSE: 0.9644\n",
      "Val: 0.8928 MAE: 0.7246 RMSE: 0.9449\n",
      "Epoch 89 Step 46620: Train 0.3512 Reg: 0.4259\n",
      "Test: 0.9290 MAE: 0.7432 RMSE: 0.9638\n",
      "Val: 0.8931 MAE: 0.7245 RMSE: 0.9451\n",
      "Epoch 90 Step 47138: Train 0.3509 Reg: 0.4255\n",
      "Test: 0.9298 MAE: 0.7434 RMSE: 0.9643\n",
      "Val: 0.8936 MAE: 0.7247 RMSE: 0.9453\n",
      "Epoch 91 Step 47656: Train 0.3507 Reg: 0.4250\n",
      "Test: 0.9303 MAE: 0.7436 RMSE: 0.9645\n",
      "Val: 0.8938 MAE: 0.7248 RMSE: 0.9454\n",
      "Epoch 92 Step 48174: Train 0.3505 Reg: 0.4246\n",
      "Test: 0.9305 MAE: 0.7436 RMSE: 0.9646\n",
      "Val: 0.8942 MAE: 0.7249 RMSE: 0.9456\n",
      "Epoch 93 Step 48692: Train 0.3503 Reg: 0.4242\n",
      "Test: 0.9301 MAE: 0.7429 RMSE: 0.9644\n",
      "Val: 0.8947 MAE: 0.7248 RMSE: 0.9459\n",
      "Epoch 94 Step 49210: Train 0.3501 Reg: 0.4238\n",
      "Test: 0.9312 MAE: 0.7437 RMSE: 0.9650\n",
      "Val: 0.8949 MAE: 0.7250 RMSE: 0.9460\n",
      "Epoch 95 Step 49728: Train 0.3499 Reg: 0.4235\n",
      "Test: 0.9328 MAE: 0.7447 RMSE: 0.9658\n",
      "Val: 0.8953 MAE: 0.7254 RMSE: 0.9462\n",
      "Epoch 96 Step 50246: Train 0.3497 Reg: 0.4231\n",
      "Test: 0.9325 MAE: 0.7445 RMSE: 0.9657\n",
      "Val: 0.8955 MAE: 0.7254 RMSE: 0.9463\n",
      "Epoch 97 Step 50764: Train 0.3495 Reg: 0.4228\n",
      "Test: 0.9323 MAE: 0.7441 RMSE: 0.9656\n",
      "Val: 0.8957 MAE: 0.7253 RMSE: 0.9464\n",
      "Epoch 98 Step 51282: Train 0.3494 Reg: 0.4225\n",
      "Test: 0.9334 MAE: 0.7448 RMSE: 0.9661\n",
      "Val: 0.8960 MAE: 0.7255 RMSE: 0.9466\n",
      "Epoch 99 Step 51800: Train 0.3492 Reg: 0.4222\n",
      "Test: 0.9330 MAE: 0.7443 RMSE: 0.9659\n",
      "Val: 0.8963 MAE: 0.7255 RMSE: 0.9467\n",
      "-------Dataset Info--------\n",
      "split way [threshold] with threshold 30 training_ratio 1.0\n",
      "train set size: support/query 558297/416772\n",
      "test set size: support/query 2092/3139\n",
      "Epoch 0: TrainLoss 0.9303 RecLoss: 0.0000 (left: 2:59:55)\n",
      "TestLoss: 0.9364 MAE: 0.7702 RMSE: 0.9677\n",
      "ValLoss: 0.8684 MAE: 0.7444 RMSE: 0.9319\n",
      "Epoch 1: TrainLoss 0.8738 RecLoss: 0.0000 (left: 3:00:26)\n",
      "TestLoss: 0.9229 MAE: 0.7595 RMSE: 0.9607\n",
      "ValLoss: 0.8582 MAE: 0.7356 RMSE: 0.9264\n",
      "Epoch 2: TrainLoss 0.8681 RecLoss: 0.0000 (left: 2:58:52)\n",
      "TestLoss: 0.9320 MAE: 0.7505 RMSE: 0.9654\n",
      "ValLoss: 0.8716 MAE: 0.7312 RMSE: 0.9336\n",
      "Epoch 3: TrainLoss 0.8657 RecLoss: 0.0000 (left: 2:55:48)\n",
      "TestLoss: 0.9138 MAE: 0.7438 RMSE: 0.9559\n",
      "ValLoss: 0.8598 MAE: 0.7285 RMSE: 0.9273\n",
      "Epoch 4: TrainLoss 0.8653 RecLoss: 0.0000 (left: 2:53:31)\n",
      "TestLoss: 0.9126 MAE: 0.7494 RMSE: 0.9553\n",
      "ValLoss: 0.8521 MAE: 0.7294 RMSE: 0.9231\n",
      "Epoch 5: TrainLoss 0.8618 RecLoss: 0.0000 (left: 2:52:39)\n",
      "TestLoss: 0.9160 MAE: 0.7508 RMSE: 0.9571\n",
      "ValLoss: 0.8509 MAE: 0.7283 RMSE: 0.9225\n",
      "Epoch 6: TrainLoss 0.8609 RecLoss: 0.0000 (left: 2:50:17)\n",
      "TestLoss: 0.9171 MAE: 0.7484 RMSE: 0.9577\n",
      "ValLoss: 0.8567 MAE: 0.7265 RMSE: 0.9256\n",
      "Epoch 7: TrainLoss 0.8599 RecLoss: 0.0000 (left: 2:48:20)\n",
      "TestLoss: 0.9104 MAE: 0.7489 RMSE: 0.9541\n",
      "ValLoss: 0.8516 MAE: 0.7291 RMSE: 0.9228\n",
      "Epoch 8: TrainLoss 0.8593 RecLoss: 0.0000 (left: 2:46:43)\n",
      "TestLoss: 0.9147 MAE: 0.7569 RMSE: 0.9564\n",
      "ValLoss: 0.8503 MAE: 0.7318 RMSE: 0.9221\n",
      "Epoch 9: TrainLoss 0.8594 RecLoss: 0.0000 (left: 2:45:22)\n",
      "TestLoss: 0.9172 MAE: 0.7571 RMSE: 0.9577\n",
      "ValLoss: 0.8526 MAE: 0.7342 RMSE: 0.9234\n",
      "Epoch 10: TrainLoss 0.8574 RecLoss: 0.0000 (left: 2:43:43)\n",
      "TestLoss: 0.9096 MAE: 0.7497 RMSE: 0.9537\n",
      "ValLoss: 0.8495 MAE: 0.7287 RMSE: 0.9217\n",
      "Epoch 11: TrainLoss 0.8575 RecLoss: 0.0000 (left: 2:41:54)\n",
      "TestLoss: 0.9196 MAE: 0.7589 RMSE: 0.9589\n",
      "ValLoss: 0.8512 MAE: 0.7336 RMSE: 0.9226\n",
      "Epoch 12: TrainLoss 0.8582 RecLoss: 0.0000 (left: 2:40:12)\n",
      "TestLoss: 0.9075 MAE: 0.7490 RMSE: 0.9526\n",
      "ValLoss: 0.8494 MAE: 0.7287 RMSE: 0.9216\n",
      "Epoch 13: TrainLoss 0.8562 RecLoss: 0.0000 (left: 2:38:42)\n",
      "TestLoss: 0.9037 MAE: 0.7497 RMSE: 0.9506\n",
      "ValLoss: 0.8480 MAE: 0.7296 RMSE: 0.9209\n",
      "Epoch 14: TrainLoss 0.8555 RecLoss: 0.0000 (left: 2:36:49)\n",
      "TestLoss: 0.9105 MAE: 0.7478 RMSE: 0.9542\n",
      "ValLoss: 0.8521 MAE: 0.7273 RMSE: 0.9231\n",
      "Epoch 15: TrainLoss 0.8558 RecLoss: 0.0000 (left: 2:34:39)\n",
      "TestLoss: 0.9082 MAE: 0.7506 RMSE: 0.9530\n",
      "ValLoss: 0.8467 MAE: 0.7276 RMSE: 0.9201\n",
      "Epoch 16: TrainLoss 0.8549 RecLoss: 0.0000 (left: 2:32:10)\n",
      "TestLoss: 0.9114 MAE: 0.7572 RMSE: 0.9547\n",
      "ValLoss: 0.8514 MAE: 0.7340 RMSE: 0.9227\n",
      "Epoch 17: TrainLoss 0.8550 RecLoss: 0.0000 (left: 2:29:08)\n",
      "TestLoss: 0.9104 MAE: 0.7482 RMSE: 0.9541\n",
      "ValLoss: 0.8532 MAE: 0.7266 RMSE: 0.9237\n",
      "Epoch 18: TrainLoss 0.8543 RecLoss: 0.0000 (left: 2:26:32)\n",
      "TestLoss: 0.9052 MAE: 0.7473 RMSE: 0.9514\n",
      "ValLoss: 0.8491 MAE: 0.7268 RMSE: 0.9215\n",
      "Epoch 19: TrainLoss 0.8537 RecLoss: 0.0000 (left: 2:23:38)\n",
      "TestLoss: 0.9140 MAE: 0.7553 RMSE: 0.9560\n",
      "ValLoss: 0.8482 MAE: 0.7292 RMSE: 0.9210\n",
      "Epoch 20: TrainLoss 0.8536 RecLoss: 0.0000 (left: 2:21:15)\n",
      "TestLoss: 0.9129 MAE: 0.7592 RMSE: 0.9554\n",
      "ValLoss: 0.8487 MAE: 0.7341 RMSE: 0.9213\n",
      "Epoch 21: TrainLoss 0.8534 RecLoss: 0.0000 (left: 2:18:54)\n",
      "TestLoss: 0.9087 MAE: 0.7506 RMSE: 0.9533\n",
      "ValLoss: 0.8457 MAE: 0.7273 RMSE: 0.9196\n",
      "Epoch 22: TrainLoss 0.8526 RecLoss: 0.0000 (left: 2:16:21)\n",
      "TestLoss: 0.9138 MAE: 0.7592 RMSE: 0.9559\n",
      "ValLoss: 0.8492 MAE: 0.7326 RMSE: 0.9215\n",
      "Epoch 23: TrainLoss 0.8525 RecLoss: 0.0000 (left: 2:14:14)\n",
      "TestLoss: 0.9089 MAE: 0.7512 RMSE: 0.9533\n",
      "ValLoss: 0.8477 MAE: 0.7288 RMSE: 0.9207\n",
      "Epoch 24: TrainLoss 0.8523 RecLoss: 0.0000 (left: 2:12:12)\n",
      "TestLoss: 0.9048 MAE: 0.7487 RMSE: 0.9512\n",
      "ValLoss: 0.8449 MAE: 0.7258 RMSE: 0.9192\n",
      "Epoch 25: TrainLoss 0.8522 RecLoss: 0.0000 (left: 2:09:58)\n",
      "TestLoss: 0.9090 MAE: 0.7533 RMSE: 0.9534\n",
      "ValLoss: 0.8466 MAE: 0.7283 RMSE: 0.9201\n",
      "Epoch 26: TrainLoss 0.8521 RecLoss: 0.0000 (left: 2:07:54)\n",
      "TestLoss: 0.9031 MAE: 0.7430 RMSE: 0.9503\n",
      "ValLoss: 0.8515 MAE: 0.7245 RMSE: 0.9228\n",
      "Epoch 27: TrainLoss 0.8520 RecLoss: 0.0000 (left: 2:05:48)\n",
      "TestLoss: 0.9065 MAE: 0.7492 RMSE: 0.9521\n",
      "ValLoss: 0.8459 MAE: 0.7271 RMSE: 0.9197\n",
      "Epoch 28: TrainLoss 0.8521 RecLoss: 0.0000 (left: 2:03:44)\n",
      "TestLoss: 0.9061 MAE: 0.7542 RMSE: 0.9519\n",
      "ValLoss: 0.8458 MAE: 0.7297 RMSE: 0.9197\n",
      "Epoch 29: TrainLoss 0.8512 RecLoss: 0.0000 (left: 2:01:41)\n",
      "TestLoss: 0.9108 MAE: 0.7546 RMSE: 0.9544\n",
      "ValLoss: 0.8468 MAE: 0.7304 RMSE: 0.9202\n",
      "Epoch 30: TrainLoss 0.8506 RecLoss: 0.0000 (left: 1:59:36)\n",
      "TestLoss: 0.9064 MAE: 0.7484 RMSE: 0.9520\n",
      "ValLoss: 0.8456 MAE: 0.7250 RMSE: 0.9195\n",
      "Epoch 31: TrainLoss 0.8507 RecLoss: 0.0000 (left: 1:57:37)\n",
      "TestLoss: 0.9205 MAE: 0.7644 RMSE: 0.9594\n",
      "ValLoss: 0.8497 MAE: 0.7347 RMSE: 0.9218\n",
      "Epoch 32: TrainLoss 0.8513 RecLoss: 0.0000 (left: 1:55:37)\n",
      "TestLoss: 0.9046 MAE: 0.7448 RMSE: 0.9511\n",
      "ValLoss: 0.8494 MAE: 0.7239 RMSE: 0.9216\n",
      "Epoch 33: TrainLoss 0.8508 RecLoss: 0.0000 (left: 1:53:41)\n",
      "TestLoss: 0.9120 MAE: 0.7596 RMSE: 0.9550\n",
      "ValLoss: 0.8505 MAE: 0.7350 RMSE: 0.9222\n",
      "Epoch 34: TrainLoss 0.8503 RecLoss: 0.0000 (left: 1:51:41)\n",
      "TestLoss: 0.9082 MAE: 0.7539 RMSE: 0.9530\n",
      "ValLoss: 0.8454 MAE: 0.7282 RMSE: 0.9195\n",
      "Epoch 35: TrainLoss 0.8501 RecLoss: 0.0000 (left: 1:49:38)\n",
      "TestLoss: 0.9148 MAE: 0.7611 RMSE: 0.9565\n",
      "ValLoss: 0.8496 MAE: 0.7340 RMSE: 0.9217\n",
      "Epoch 36: TrainLoss 0.8493 RecLoss: 0.0000 (left: 1:47:43)\n",
      "TestLoss: 0.9087 MAE: 0.7561 RMSE: 0.9533\n",
      "ValLoss: 0.8483 MAE: 0.7314 RMSE: 0.9210\n",
      "Epoch 37: TrainLoss 0.8487 RecLoss: 0.0000 (left: 1:45:45)\n",
      "TestLoss: 0.9042 MAE: 0.7523 RMSE: 0.9509\n",
      "ValLoss: 0.8451 MAE: 0.7293 RMSE: 0.9193\n",
      "Epoch 38: TrainLoss 0.8486 RecLoss: 0.0000 (left: 1:43:53)\n",
      "TestLoss: 0.9023 MAE: 0.7494 RMSE: 0.9499\n",
      "ValLoss: 0.8439 MAE: 0.7265 RMSE: 0.9186\n",
      "Epoch 39: TrainLoss 0.8488 RecLoss: 0.0000 (left: 1:41:55)\n",
      "TestLoss: 0.9080 MAE: 0.7484 RMSE: 0.9529\n",
      "ValLoss: 0.8486 MAE: 0.7252 RMSE: 0.9212\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train-1m.py\", line 157, in <module>\n",
      "    loss_r, loss_rec = train(model, optimizer, i)\n",
      "  File \"train-1m.py\", line 86, in train\n",
      "    train_set_his_i = [torch.tensor(\n",
      "  File \"train-1m.py\", line 87, in <listcomp>\n",
      "    sequence_adjust( user_his_dic[train_set_que_i[k][0].item()] ),\n",
      "  File \"train-1m.py\", line 74, in sequence_adjust\n",
      "    random.shuffle(seq)\n",
      "  File \"/raid/home/jayantkalani/miniconda3/envs/ncf2/lib/python3.8/random.py\", line 306, in shuffle\n",
      "    j = randbelow(i+1)\n",
      "  File \"/raid/home/jayantkalani/miniconda3/envs/ncf2/lib/python3.8/random.py\", line 254, in _randbelow_with_getrandbits\n",
      "    k = n.bit_length()  # don't use (n-1) here because n can be 1\n",
      "KeyboardInterrupt\n",
      "Extra : False\n",
      "-------Dataset Info--------\n",
      "split way [threshold] with threshold 30 training_ratio 1.0\n",
      "train set size: support/query 558297/416772\n",
      "test set size: support/query 2092/3139\n",
      "USER HIS DICT: 6040\n",
      "NUM IS: 6040\n",
      "Key Test Result: MAE: 0.6700 RMSE: 0.8416 NDCG: 0.0000\n",
      "CORE IS SELECTED:\n",
      "USER HIS DICT: 6040\n",
      "NUM IS: 6040\n",
      "Que Test Result: MAE: 0.7481 RMSE: 0.9480 NDCG: 0.0000\n",
      "All Test Result: MAE: 0.7169 RMSE: 0.9070 NDCG: 0.0000\n"
     ]
    }
   ],
   "source": [
    "!python pretrain-1m.py\n",
    "!python train-1m.py\n",
    "!python test-1m.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 users as core users in support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Dataset Info--------\n",
      "split way [threshold] with threshold 30 training_ratio 1.0\n",
      "train set size: support/query 233956/741113\n",
      "test set size: support/query 523/4708\n",
      "Epoch 0 Step 218: Train 2.8406 Reg: 0.4863\n",
      "Test: 0.7799 MAE: 0.6897 RMSE: 0.8831\n",
      "Val: 0.7763 MAE: 0.6991 RMSE: 0.8811\n",
      "Epoch 1 Step 436: Train 0.7594 Reg: 0.4383\n",
      "Test: 0.7483 MAE: 0.6838 RMSE: 0.8651\n",
      "Val: 0.7558 MAE: 0.6919 RMSE: 0.8694\n",
      "Epoch 2 Step 654: Train 0.7488 Reg: 0.3935\n",
      "Test: 0.7275 MAE: 0.6753 RMSE: 0.8529\n",
      "Val: 0.7509 MAE: 0.6880 RMSE: 0.8666\n",
      "Epoch 3 Step 872: Train 0.7458 Reg: 0.3592\n",
      "Test: 0.7188 MAE: 0.6698 RMSE: 0.8478\n",
      "Val: 0.7447 MAE: 0.6853 RMSE: 0.8630\n",
      "Epoch 4 Step 1090: Train 0.7431 Reg: 0.3314\n",
      "Test: 0.7091 MAE: 0.6640 RMSE: 0.8421\n",
      "Val: 0.7440 MAE: 0.6841 RMSE: 0.8626\n",
      "Epoch 5 Step 1308: Train 0.7402 Reg: 0.3110\n",
      "Test: 0.7181 MAE: 0.6626 RMSE: 0.8474\n",
      "Val: 0.7444 MAE: 0.6834 RMSE: 0.8628\n",
      "Epoch 6 Step 1526: Train 0.7369 Reg: 0.2950\n",
      "Test: 0.7186 MAE: 0.6720 RMSE: 0.8477\n",
      "Val: 0.7392 MAE: 0.6796 RMSE: 0.8598\n",
      "Epoch 7 Step 1744: Train 0.7335 Reg: 0.2818\n",
      "Test: 0.7244 MAE: 0.6684 RMSE: 0.8511\n",
      "Val: 0.7345 MAE: 0.6786 RMSE: 0.8570\n",
      "Epoch 8 Step 1962: Train 0.7289 Reg: 0.2719\n",
      "Test: 0.6985 MAE: 0.6596 RMSE: 0.8358\n",
      "Val: 0.7304 MAE: 0.6783 RMSE: 0.8546\n",
      "Epoch 9 Step 2180: Train 0.7238 Reg: 0.2663\n",
      "Test: 0.7007 MAE: 0.6598 RMSE: 0.8371\n",
      "Val: 0.7237 MAE: 0.6734 RMSE: 0.8507\n",
      "Epoch 10 Step 2398: Train 0.7189 Reg: 0.2605\n",
      "Test: 0.7003 MAE: 0.6577 RMSE: 0.8368\n",
      "Val: 0.7189 MAE: 0.6699 RMSE: 0.8479\n",
      "Epoch 11 Step 2616: Train 0.7134 Reg: 0.2573\n",
      "Test: 0.7127 MAE: 0.6658 RMSE: 0.8442\n",
      "Val: 0.7163 MAE: 0.6681 RMSE: 0.8463\n",
      "Epoch 12 Step 2834: Train 0.7074 Reg: 0.2558\n",
      "Test: 0.7038 MAE: 0.6628 RMSE: 0.8389\n",
      "Val: 0.7091 MAE: 0.6643 RMSE: 0.8421\n",
      "Epoch 13 Step 3052: Train 0.6971 Reg: 0.2569\n",
      "Test: 0.6924 MAE: 0.6532 RMSE: 0.8321\n",
      "Val: 0.7024 MAE: 0.6601 RMSE: 0.8381\n",
      "Epoch 14 Step 3270: Train 0.6852 Reg: 0.2601\n",
      "Test: 0.6921 MAE: 0.6505 RMSE: 0.8319\n",
      "Val: 0.6950 MAE: 0.6558 RMSE: 0.8337\n",
      "Epoch 15 Step 3488: Train 0.6755 Reg: 0.2619\n",
      "Test: 0.6871 MAE: 0.6533 RMSE: 0.8289\n",
      "Val: 0.6915 MAE: 0.6534 RMSE: 0.8316\n",
      "Epoch 16 Step 3706: Train 0.6694 Reg: 0.2609\n",
      "Test: 0.6892 MAE: 0.6530 RMSE: 0.8302\n",
      "Val: 0.6891 MAE: 0.6509 RMSE: 0.8301\n",
      "Epoch 17 Step 3924: Train 0.6634 Reg: 0.2598\n",
      "Test: 0.6825 MAE: 0.6539 RMSE: 0.8261\n",
      "Val: 0.6885 MAE: 0.6519 RMSE: 0.8298\n",
      "Epoch 18 Step 4142: Train 0.6580 Reg: 0.2588\n",
      "Test: 0.6821 MAE: 0.6491 RMSE: 0.8259\n",
      "Val: 0.6877 MAE: 0.6514 RMSE: 0.8293\n",
      "Epoch 19 Step 4360: Train 0.6515 Reg: 0.2602\n",
      "Test: 0.6830 MAE: 0.6528 RMSE: 0.8264\n",
      "Val: 0.6866 MAE: 0.6513 RMSE: 0.8286\n",
      "Epoch 20 Step 4578: Train 0.6424 Reg: 0.2641\n",
      "Test: 0.6870 MAE: 0.6547 RMSE: 0.8289\n",
      "Val: 0.6849 MAE: 0.6498 RMSE: 0.8276\n",
      "Epoch 21 Step 4796: Train 0.6285 Reg: 0.2710\n",
      "Test: 0.6722 MAE: 0.6475 RMSE: 0.8199\n",
      "Val: 0.6836 MAE: 0.6454 RMSE: 0.8268\n",
      "Epoch 22 Step 5014: Train 0.6095 Reg: 0.2821\n",
      "Test: 0.6770 MAE: 0.6509 RMSE: 0.8228\n",
      "Val: 0.6776 MAE: 0.6452 RMSE: 0.8231\n",
      "Epoch 23 Step 5232: Train 0.5869 Reg: 0.2941\n",
      "Test: 0.6656 MAE: 0.6483 RMSE: 0.8158\n",
      "Val: 0.6776 MAE: 0.6446 RMSE: 0.8232\n",
      "Epoch 24 Step 5450: Train 0.5670 Reg: 0.3045\n",
      "Test: 0.6617 MAE: 0.6462 RMSE: 0.8135\n",
      "Val: 0.6791 MAE: 0.6446 RMSE: 0.8241\n",
      "Epoch 25 Step 5668: Train 0.5505 Reg: 0.3117\n",
      "Test: 0.6602 MAE: 0.6461 RMSE: 0.8125\n",
      "Val: 0.6835 MAE: 0.6466 RMSE: 0.8268\n",
      "Epoch 26 Step 5886: Train 0.5358 Reg: 0.3182\n",
      "Test: 0.6548 MAE: 0.6404 RMSE: 0.8092\n",
      "Val: 0.6883 MAE: 0.6465 RMSE: 0.8296\n",
      "Epoch 27 Step 6104: Train 0.5209 Reg: 0.3255\n",
      "Test: 0.6501 MAE: 0.6424 RMSE: 0.8063\n",
      "Val: 0.6948 MAE: 0.6488 RMSE: 0.8336\n",
      "Epoch 28 Step 6322: Train 0.5051 Reg: 0.3335\n",
      "Test: 0.6618 MAE: 0.6493 RMSE: 0.8135\n",
      "Val: 0.6995 MAE: 0.6517 RMSE: 0.8364\n",
      "Epoch 29 Step 6540: Train 0.4887 Reg: 0.3413\n",
      "Test: 0.6633 MAE: 0.6467 RMSE: 0.8144\n",
      "Val: 0.7066 MAE: 0.6543 RMSE: 0.8406\n",
      "Epoch 30 Step 6758: Train 0.4732 Reg: 0.3487\n",
      "Test: 0.6706 MAE: 0.6486 RMSE: 0.8189\n",
      "Val: 0.7143 MAE: 0.6576 RMSE: 0.8452\n",
      "Epoch 31 Step 6976: Train 0.4589 Reg: 0.3550\n",
      "Test: 0.6873 MAE: 0.6599 RMSE: 0.8290\n",
      "Val: 0.7248 MAE: 0.6628 RMSE: 0.8514\n",
      "Epoch 32 Step 7194: Train 0.4460 Reg: 0.3604\n",
      "Test: 0.6860 MAE: 0.6531 RMSE: 0.8283\n",
      "Val: 0.7363 MAE: 0.6647 RMSE: 0.8581\n",
      "Epoch 33 Step 7412: Train 0.4342 Reg: 0.3654\n",
      "Test: 0.7000 MAE: 0.6636 RMSE: 0.8367\n",
      "Val: 0.7446 MAE: 0.6701 RMSE: 0.8629\n",
      "Epoch 34 Step 7630: Train 0.4231 Reg: 0.3696\n",
      "Test: 0.7030 MAE: 0.6621 RMSE: 0.8385\n",
      "Val: 0.7538 MAE: 0.6729 RMSE: 0.8682\n",
      "Epoch 35 Step 7848: Train 0.4133 Reg: 0.3731\n",
      "Test: 0.7162 MAE: 0.6696 RMSE: 0.8463\n",
      "Val: 0.7640 MAE: 0.6781 RMSE: 0.8741\n",
      "Epoch 36 Step 8066: Train 0.4044 Reg: 0.3763\n",
      "Test: 0.7223 MAE: 0.6715 RMSE: 0.8499\n",
      "Val: 0.7720 MAE: 0.6804 RMSE: 0.8786\n",
      "Epoch 37 Step 8284: Train 0.3957 Reg: 0.3791\n",
      "Test: 0.7286 MAE: 0.6696 RMSE: 0.8536\n",
      "Val: 0.7824 MAE: 0.6846 RMSE: 0.8846\n",
      "Epoch 38 Step 8502: Train 0.3873 Reg: 0.3818\n",
      "Test: 0.7342 MAE: 0.6728 RMSE: 0.8569\n",
      "Val: 0.7902 MAE: 0.6877 RMSE: 0.8889\n",
      "Epoch 39 Step 8720: Train 0.3792 Reg: 0.3841\n",
      "Test: 0.7368 MAE: 0.6720 RMSE: 0.8584\n",
      "Val: 0.7994 MAE: 0.6910 RMSE: 0.8941\n",
      "Epoch 40 Step 8938: Train 0.3713 Reg: 0.3866\n",
      "Test: 0.7467 MAE: 0.6768 RMSE: 0.8641\n",
      "Val: 0.8087 MAE: 0.6953 RMSE: 0.8993\n",
      "Epoch 41 Step 9156: Train 0.3638 Reg: 0.3885\n",
      "Test: 0.7513 MAE: 0.6780 RMSE: 0.8668\n",
      "Val: 0.8169 MAE: 0.6984 RMSE: 0.9038\n",
      "Epoch 42 Step 9374: Train 0.3570 Reg: 0.3901\n",
      "Test: 0.7602 MAE: 0.6826 RMSE: 0.8719\n",
      "Val: 0.8236 MAE: 0.7012 RMSE: 0.9075\n",
      "Epoch 43 Step 9592: Train 0.3511 Reg: 0.3912\n",
      "Test: 0.7606 MAE: 0.6819 RMSE: 0.8721\n",
      "Val: 0.8304 MAE: 0.7035 RMSE: 0.9113\n",
      "Epoch 44 Step 9810: Train 0.3459 Reg: 0.3918\n",
      "Test: 0.7667 MAE: 0.6847 RMSE: 0.8756\n",
      "Val: 0.8381 MAE: 0.7065 RMSE: 0.9155\n",
      "Epoch 45 Step 10028: Train 0.3413 Reg: 0.3922\n",
      "Test: 0.7743 MAE: 0.6889 RMSE: 0.8799\n",
      "Val: 0.8447 MAE: 0.7092 RMSE: 0.9191\n",
      "Epoch 46 Step 10246: Train 0.3374 Reg: 0.3921\n",
      "Test: 0.7793 MAE: 0.6907 RMSE: 0.8828\n",
      "Val: 0.8508 MAE: 0.7117 RMSE: 0.9224\n",
      "Epoch 47 Step 10464: Train 0.3339 Reg: 0.3920\n",
      "Test: 0.7815 MAE: 0.6918 RMSE: 0.8840\n",
      "Val: 0.8561 MAE: 0.7137 RMSE: 0.9252\n",
      "Epoch 48 Step 10682: Train 0.3309 Reg: 0.3917\n",
      "Test: 0.7871 MAE: 0.6940 RMSE: 0.8872\n",
      "Val: 0.8607 MAE: 0.7154 RMSE: 0.9277\n",
      "Epoch 49 Step 10900: Train 0.3281 Reg: 0.3913\n",
      "Test: 0.7902 MAE: 0.6961 RMSE: 0.8889\n",
      "Val: 0.8654 MAE: 0.7172 RMSE: 0.9303\n",
      "Epoch 50 Step 11118: Train 0.3256 Reg: 0.3906\n",
      "Test: 0.7949 MAE: 0.6984 RMSE: 0.8916\n",
      "Val: 0.8712 MAE: 0.7196 RMSE: 0.9334\n",
      "Epoch 51 Step 11336: Train 0.3234 Reg: 0.3901\n",
      "Test: 0.7984 MAE: 0.6993 RMSE: 0.8936\n",
      "Val: 0.8742 MAE: 0.7207 RMSE: 0.9350\n",
      "Epoch 52 Step 11554: Train 0.3213 Reg: 0.3895\n",
      "Test: 0.8031 MAE: 0.7017 RMSE: 0.8962\n",
      "Val: 0.8784 MAE: 0.7224 RMSE: 0.9372\n",
      "Epoch 53 Step 11772: Train 0.3195 Reg: 0.3887\n",
      "Test: 0.8026 MAE: 0.7018 RMSE: 0.8959\n",
      "Val: 0.8814 MAE: 0.7233 RMSE: 0.9388\n",
      "Epoch 54 Step 11990: Train 0.3177 Reg: 0.3881\n",
      "Test: 0.8067 MAE: 0.7029 RMSE: 0.8982\n",
      "Val: 0.8849 MAE: 0.7245 RMSE: 0.9407\n",
      "Epoch 55 Step 12208: Train 0.3162 Reg: 0.3874\n",
      "Test: 0.8090 MAE: 0.7053 RMSE: 0.8994\n",
      "Val: 0.8872 MAE: 0.7257 RMSE: 0.9419\n",
      "Epoch 56 Step 12426: Train 0.3147 Reg: 0.3868\n",
      "Test: 0.8129 MAE: 0.7070 RMSE: 0.9016\n",
      "Val: 0.8905 MAE: 0.7269 RMSE: 0.9437\n",
      "Epoch 57 Step 12644: Train 0.3133 Reg: 0.3861\n",
      "Test: 0.8156 MAE: 0.7087 RMSE: 0.9031\n",
      "Val: 0.8938 MAE: 0.7283 RMSE: 0.9454\n",
      "Epoch 58 Step 12862: Train 0.3120 Reg: 0.3854\n",
      "Test: 0.8171 MAE: 0.7096 RMSE: 0.9040\n",
      "Val: 0.8963 MAE: 0.7289 RMSE: 0.9467\n",
      "Epoch 59 Step 13080: Train 0.3108 Reg: 0.3848\n",
      "Test: 0.8182 MAE: 0.7099 RMSE: 0.9045\n",
      "Val: 0.8988 MAE: 0.7299 RMSE: 0.9481\n",
      "Epoch 60 Step 13298: Train 0.3097 Reg: 0.3842\n",
      "Test: 0.8238 MAE: 0.7127 RMSE: 0.9076\n",
      "Val: 0.9015 MAE: 0.7311 RMSE: 0.9495\n",
      "Epoch 61 Step 13516: Train 0.3086 Reg: 0.3835\n",
      "Test: 0.8226 MAE: 0.7122 RMSE: 0.9070\n",
      "Val: 0.9034 MAE: 0.7315 RMSE: 0.9505\n",
      "Epoch 62 Step 13734: Train 0.3077 Reg: 0.3829\n",
      "Test: 0.8275 MAE: 0.7154 RMSE: 0.9097\n",
      "Val: 0.9061 MAE: 0.7330 RMSE: 0.9519\n",
      "Epoch 63 Step 13952: Train 0.3067 Reg: 0.3824\n",
      "Test: 0.8264 MAE: 0.7140 RMSE: 0.9091\n",
      "Val: 0.9075 MAE: 0.7330 RMSE: 0.9526\n",
      "Epoch 64 Step 14170: Train 0.3058 Reg: 0.3818\n",
      "Test: 0.8304 MAE: 0.7162 RMSE: 0.9112\n",
      "Val: 0.9095 MAE: 0.7338 RMSE: 0.9537\n",
      "Epoch 65 Step 14388: Train 0.3050 Reg: 0.3812\n",
      "Test: 0.8316 MAE: 0.7165 RMSE: 0.9119\n",
      "Val: 0.9108 MAE: 0.7342 RMSE: 0.9543\n",
      "Epoch 66 Step 14606: Train 0.3042 Reg: 0.3807\n",
      "Test: 0.8347 MAE: 0.7189 RMSE: 0.9136\n",
      "Val: 0.9128 MAE: 0.7354 RMSE: 0.9554\n",
      "Epoch 67 Step 14824: Train 0.3035 Reg: 0.3802\n",
      "Test: 0.8331 MAE: 0.7175 RMSE: 0.9128\n",
      "Val: 0.9140 MAE: 0.7354 RMSE: 0.9561\n",
      "Epoch 68 Step 15042: Train 0.3027 Reg: 0.3798\n",
      "Test: 0.8367 MAE: 0.7195 RMSE: 0.9147\n",
      "Val: 0.9157 MAE: 0.7362 RMSE: 0.9569\n",
      "Epoch 69 Step 15260: Train 0.3021 Reg: 0.3793\n",
      "Test: 0.8358 MAE: 0.7184 RMSE: 0.9142\n",
      "Val: 0.9167 MAE: 0.7364 RMSE: 0.9575\n",
      "Epoch 70 Step 15478: Train 0.3014 Reg: 0.3789\n",
      "Test: 0.8383 MAE: 0.7205 RMSE: 0.9156\n",
      "Val: 0.9183 MAE: 0.7372 RMSE: 0.9583\n",
      "Epoch 71 Step 15696: Train 0.3008 Reg: 0.3784\n",
      "Test: 0.8390 MAE: 0.7206 RMSE: 0.9160\n",
      "Val: 0.9195 MAE: 0.7375 RMSE: 0.9589\n",
      "Epoch 72 Step 15914: Train 0.3002 Reg: 0.3780\n",
      "Test: 0.8402 MAE: 0.7212 RMSE: 0.9166\n",
      "Val: 0.9202 MAE: 0.7377 RMSE: 0.9593\n",
      "Epoch 73 Step 16132: Train 0.2997 Reg: 0.3777\n",
      "Test: 0.8414 MAE: 0.7215 RMSE: 0.9173\n",
      "Val: 0.9216 MAE: 0.7382 RMSE: 0.9600\n",
      "Epoch 74 Step 16350: Train 0.2992 Reg: 0.3773\n",
      "Test: 0.8420 MAE: 0.7217 RMSE: 0.9176\n",
      "Val: 0.9228 MAE: 0.7385 RMSE: 0.9606\n",
      "Epoch 75 Step 16568: Train 0.2987 Reg: 0.3769\n",
      "Test: 0.8429 MAE: 0.7223 RMSE: 0.9181\n",
      "Val: 0.9239 MAE: 0.7389 RMSE: 0.9612\n",
      "Epoch 76 Step 16786: Train 0.2982 Reg: 0.3766\n",
      "Test: 0.8457 MAE: 0.7237 RMSE: 0.9196\n",
      "Val: 0.9253 MAE: 0.7396 RMSE: 0.9619\n",
      "Epoch 77 Step 17004: Train 0.2978 Reg: 0.3763\n",
      "Test: 0.8445 MAE: 0.7229 RMSE: 0.9190\n",
      "Val: 0.9256 MAE: 0.7395 RMSE: 0.9621\n",
      "Epoch 78 Step 17222: Train 0.2974 Reg: 0.3760\n",
      "Test: 0.8479 MAE: 0.7249 RMSE: 0.9208\n",
      "Val: 0.9270 MAE: 0.7402 RMSE: 0.9628\n",
      "Epoch 79 Step 17440: Train 0.2970 Reg: 0.3757\n",
      "Test: 0.8462 MAE: 0.7239 RMSE: 0.9199\n",
      "Val: 0.9272 MAE: 0.7401 RMSE: 0.9629\n",
      "Epoch 80 Step 17658: Train 0.2966 Reg: 0.3754\n",
      "Test: 0.8480 MAE: 0.7247 RMSE: 0.9208\n",
      "Val: 0.9284 MAE: 0.7406 RMSE: 0.9635\n",
      "Epoch 81 Step 17876: Train 0.2962 Reg: 0.3751\n",
      "Test: 0.8495 MAE: 0.7253 RMSE: 0.9217\n",
      "Val: 0.9292 MAE: 0.7409 RMSE: 0.9639\n",
      "Epoch 82 Step 18094: Train 0.2959 Reg: 0.3749\n",
      "Test: 0.8495 MAE: 0.7253 RMSE: 0.9217\n",
      "Val: 0.9296 MAE: 0.7410 RMSE: 0.9642\n",
      "Epoch 83 Step 18312: Train 0.2956 Reg: 0.3746\n",
      "Test: 0.8503 MAE: 0.7258 RMSE: 0.9221\n",
      "Val: 0.9305 MAE: 0.7413 RMSE: 0.9646\n",
      "Epoch 84 Step 18530: Train 0.2953 Reg: 0.3744\n",
      "Test: 0.8514 MAE: 0.7263 RMSE: 0.9227\n",
      "Val: 0.9311 MAE: 0.7416 RMSE: 0.9649\n",
      "Epoch 85 Step 18748: Train 0.2950 Reg: 0.3742\n",
      "Test: 0.8522 MAE: 0.7268 RMSE: 0.9231\n",
      "Val: 0.9317 MAE: 0.7419 RMSE: 0.9653\n",
      "Epoch 86 Step 18966: Train 0.2947 Reg: 0.3739\n",
      "Test: 0.8513 MAE: 0.7261 RMSE: 0.9226\n",
      "Val: 0.9320 MAE: 0.7418 RMSE: 0.9654\n",
      "Epoch 87 Step 19184: Train 0.2944 Reg: 0.3738\n",
      "Test: 0.8536 MAE: 0.7274 RMSE: 0.9239\n",
      "Val: 0.9329 MAE: 0.7423 RMSE: 0.9658\n",
      "Epoch 88 Step 19402: Train 0.2942 Reg: 0.3736\n",
      "Test: 0.8530 MAE: 0.7270 RMSE: 0.9236\n",
      "Val: 0.9331 MAE: 0.7422 RMSE: 0.9659\n",
      "Epoch 89 Step 19620: Train 0.2939 Reg: 0.3734\n",
      "Test: 0.8546 MAE: 0.7279 RMSE: 0.9245\n",
      "Val: 0.9338 MAE: 0.7427 RMSE: 0.9664\n",
      "Epoch 90 Step 19838: Train 0.2937 Reg: 0.3732\n",
      "Test: 0.8551 MAE: 0.7281 RMSE: 0.9247\n",
      "Val: 0.9343 MAE: 0.7428 RMSE: 0.9666\n",
      "Epoch 91 Step 20056: Train 0.2935 Reg: 0.3730\n",
      "Test: 0.8549 MAE: 0.7279 RMSE: 0.9246\n",
      "Val: 0.9346 MAE: 0.7428 RMSE: 0.9668\n",
      "Epoch 92 Step 20274: Train 0.2933 Reg: 0.3729\n",
      "Test: 0.8552 MAE: 0.7280 RMSE: 0.9248\n",
      "Val: 0.9350 MAE: 0.7429 RMSE: 0.9670\n",
      "Epoch 93 Step 20492: Train 0.2931 Reg: 0.3727\n",
      "Test: 0.8560 MAE: 0.7284 RMSE: 0.9252\n",
      "Val: 0.9354 MAE: 0.7431 RMSE: 0.9672\n",
      "Epoch 94 Step 20710: Train 0.2929 Reg: 0.3726\n",
      "Test: 0.8564 MAE: 0.7285 RMSE: 0.9254\n",
      "Val: 0.9358 MAE: 0.7432 RMSE: 0.9674\n",
      "Epoch 95 Step 20928: Train 0.2927 Reg: 0.3725\n",
      "Test: 0.8563 MAE: 0.7284 RMSE: 0.9253\n",
      "Val: 0.9361 MAE: 0.7433 RMSE: 0.9675\n",
      "Epoch 96 Step 21146: Train 0.2926 Reg: 0.3723\n",
      "Test: 0.8568 MAE: 0.7287 RMSE: 0.9256\n",
      "Val: 0.9365 MAE: 0.7434 RMSE: 0.9677\n",
      "Epoch 97 Step 21364: Train 0.2924 Reg: 0.3722\n",
      "Test: 0.8575 MAE: 0.7291 RMSE: 0.9260\n",
      "Val: 0.9369 MAE: 0.7436 RMSE: 0.9679\n",
      "Epoch 98 Step 21582: Train 0.2923 Reg: 0.3721\n",
      "Test: 0.8573 MAE: 0.7289 RMSE: 0.9259\n",
      "Val: 0.9371 MAE: 0.7436 RMSE: 0.9680\n",
      "Epoch 99 Step 21800: Train 0.2921 Reg: 0.3720\n",
      "Test: 0.8579 MAE: 0.7292 RMSE: 0.9262\n",
      "Val: 0.9374 MAE: 0.7437 RMSE: 0.9682\n",
      "-------Dataset Info--------\n",
      "split way [threshold] with threshold 30 training_ratio 1.0\n",
      "train set size: support/query 233956/741113\n",
      "test set size: support/query 523/4708\n",
      "Epoch 0: TrainLoss 0.8755 RecLoss: 0.0000 (left: 5:49:53)\n",
      "TestLoss: 0.9208 MAE: 0.7647 RMSE: 0.9596\n",
      "ValLoss: 0.8471 MAE: 0.7346 RMSE: 0.9204\n",
      "Epoch 1: TrainLoss 0.8427 RecLoss: 0.0000 (left: 5:59:50)\n",
      "TestLoss: 0.9123 MAE: 0.7601 RMSE: 0.9551\n",
      "ValLoss: 0.8462 MAE: 0.7342 RMSE: 0.9199\n",
      "Epoch 2: TrainLoss 0.8403 RecLoss: 0.0000 (left: 5:46:20)\n",
      "TestLoss: 0.9075 MAE: 0.7475 RMSE: 0.9526\n",
      "ValLoss: 0.8416 MAE: 0.7214 RMSE: 0.9174\n",
      "Epoch 3: TrainLoss 0.8373 RecLoss: 0.0000 (left: 5:32:29)\n",
      "TestLoss: 0.8990 MAE: 0.7474 RMSE: 0.9482\n",
      "ValLoss: 0.8380 MAE: 0.7246 RMSE: 0.9154\n",
      "Epoch 4: TrainLoss 0.8360 RecLoss: 0.0000 (left: 5:23:08)\n",
      "TestLoss: 0.9036 MAE: 0.7519 RMSE: 0.9506\n",
      "ValLoss: 0.8376 MAE: 0.7262 RMSE: 0.9152\n",
      "Epoch 5: TrainLoss 0.8351 RecLoss: 0.0000 (left: 5:16:55)\n",
      "TestLoss: 0.9056 MAE: 0.7468 RMSE: 0.9517\n",
      "ValLoss: 0.8365 MAE: 0.7187 RMSE: 0.9146\n",
      "Epoch 6: TrainLoss 0.8341 RecLoss: 0.0000 (left: 5:12:22)\n",
      "TestLoss: 0.9076 MAE: 0.7535 RMSE: 0.9527\n",
      "ValLoss: 0.8326 MAE: 0.7229 RMSE: 0.9125\n",
      "Epoch 7: TrainLoss 0.8340 RecLoss: 0.0000 (left: 5:08:51)\n",
      "TestLoss: 0.8983 MAE: 0.7489 RMSE: 0.9478\n",
      "ValLoss: 0.8325 MAE: 0.7220 RMSE: 0.9124\n",
      "Epoch 8: TrainLoss 0.8329 RecLoss: 0.0000 (left: 5:05:19)\n",
      "TestLoss: 0.9028 MAE: 0.7512 RMSE: 0.9501\n",
      "ValLoss: 0.8316 MAE: 0.7229 RMSE: 0.9119\n",
      "Epoch 9: TrainLoss 0.8319 RecLoss: 0.0000 (left: 5:01:40)\n",
      "TestLoss: 0.8999 MAE: 0.7425 RMSE: 0.9486\n",
      "ValLoss: 0.8366 MAE: 0.7186 RMSE: 0.9146\n",
      "Epoch 10: TrainLoss 0.8315 RecLoss: 0.0000 (left: 4:57:58)\n",
      "TestLoss: 0.9046 MAE: 0.7567 RMSE: 0.9511\n",
      "ValLoss: 0.8343 MAE: 0.7282 RMSE: 0.9134\n",
      "Epoch 11: TrainLoss 0.8312 RecLoss: 0.0000 (left: 4:53:53)\n",
      "TestLoss: 0.8986 MAE: 0.7448 RMSE: 0.9480\n",
      "ValLoss: 0.8347 MAE: 0.7204 RMSE: 0.9136\n",
      "Epoch 12: TrainLoss 0.8307 RecLoss: 0.0000 (left: 4:49:51)\n",
      "TestLoss: 0.8961 MAE: 0.7411 RMSE: 0.9466\n",
      "ValLoss: 0.8353 MAE: 0.7181 RMSE: 0.9139\n",
      "Epoch 13: TrainLoss 0.8305 RecLoss: 0.0000 (left: 4:44:43)\n",
      "TestLoss: 0.9018 MAE: 0.7522 RMSE: 0.9496\n",
      "ValLoss: 0.8321 MAE: 0.7246 RMSE: 0.9122\n",
      "Epoch 14: TrainLoss 0.8305 RecLoss: 0.0000 (left: 4:40:02)\n",
      "TestLoss: 0.9055 MAE: 0.7552 RMSE: 0.9516\n",
      "ValLoss: 0.8322 MAE: 0.7247 RMSE: 0.9122\n",
      "Epoch 15: TrainLoss 0.8292 RecLoss: 0.0000 (left: 4:35:29)\n",
      "TestLoss: 0.9006 MAE: 0.7511 RMSE: 0.9490\n",
      "ValLoss: 0.8322 MAE: 0.7241 RMSE: 0.9123\n",
      "Epoch 16: TrainLoss 0.8288 RecLoss: 0.0000 (left: 4:31:09)\n",
      "TestLoss: 0.8912 MAE: 0.7437 RMSE: 0.9440\n",
      "ValLoss: 0.8318 MAE: 0.7199 RMSE: 0.9120\n",
      "Epoch 17: TrainLoss 0.8284 RecLoss: 0.0000 (left: 4:26:56)\n",
      "TestLoss: 0.8987 MAE: 0.7445 RMSE: 0.9480\n",
      "ValLoss: 0.8320 MAE: 0.7181 RMSE: 0.9121\n",
      "Epoch 18: TrainLoss 0.8281 RecLoss: 0.0000 (left: 4:22:56)\n",
      "TestLoss: 0.8928 MAE: 0.7460 RMSE: 0.9449\n",
      "ValLoss: 0.8311 MAE: 0.7196 RMSE: 0.9116\n",
      "Epoch 19: TrainLoss 0.8282 RecLoss: 0.0000 (left: 4:19:23)\n",
      "TestLoss: 0.8943 MAE: 0.7426 RMSE: 0.9457\n",
      "ValLoss: 0.8313 MAE: 0.7167 RMSE: 0.9118\n",
      "Epoch 20: TrainLoss 0.8276 RecLoss: 0.0000 (left: 4:17:16)\n",
      "TestLoss: 0.8935 MAE: 0.7446 RMSE: 0.9452\n",
      "ValLoss: 0.8305 MAE: 0.7184 RMSE: 0.9113\n",
      "Epoch 21: TrainLoss 0.8281 RecLoss: 0.0000 (left: 4:13:32)\n",
      "TestLoss: 0.8959 MAE: 0.7469 RMSE: 0.9465\n",
      "ValLoss: 0.8301 MAE: 0.7196 RMSE: 0.9111\n",
      "Epoch 22: TrainLoss 0.8273 RecLoss: 0.0000 (left: 4:09:35)\n",
      "TestLoss: 0.8927 MAE: 0.7447 RMSE: 0.9448\n",
      "ValLoss: 0.8287 MAE: 0.7186 RMSE: 0.9103\n",
      "Epoch 23: TrainLoss 0.8269 RecLoss: 0.0000 (left: 4:05:43)\n",
      "TestLoss: 0.8955 MAE: 0.7453 RMSE: 0.9463\n",
      "ValLoss: 0.8274 MAE: 0.7179 RMSE: 0.9096\n",
      "Epoch 24: TrainLoss 0.8264 RecLoss: 0.0000 (left: 4:01:57)\n",
      "TestLoss: 0.8954 MAE: 0.7419 RMSE: 0.9463\n",
      "ValLoss: 0.8304 MAE: 0.7158 RMSE: 0.9113\n",
      "Epoch 25: TrainLoss 0.8261 RecLoss: 0.0000 (left: 3:58:09)\n",
      "TestLoss: 0.8952 MAE: 0.7501 RMSE: 0.9461\n",
      "ValLoss: 0.8284 MAE: 0.7225 RMSE: 0.9102\n",
      "Epoch 26: TrainLoss 0.8259 RecLoss: 0.0000 (left: 3:54:27)\n",
      "TestLoss: 0.8917 MAE: 0.7506 RMSE: 0.9443\n",
      "ValLoss: 0.8291 MAE: 0.7237 RMSE: 0.9105\n",
      "Epoch 27: TrainLoss 0.8260 RecLoss: 0.0000 (left: 3:50:55)\n",
      "TestLoss: 0.8943 MAE: 0.7418 RMSE: 0.9457\n",
      "ValLoss: 0.8293 MAE: 0.7162 RMSE: 0.9107\n",
      "Epoch 28: TrainLoss 0.8253 RecLoss: 0.0000 (left: 3:47:23)\n",
      "TestLoss: 0.8893 MAE: 0.7426 RMSE: 0.9430\n",
      "ValLoss: 0.8261 MAE: 0.7176 RMSE: 0.9089\n",
      "Epoch 29: TrainLoss 0.8251 RecLoss: 0.0000 (left: 3:43:51)\n",
      "TestLoss: 0.8960 MAE: 0.7519 RMSE: 0.9466\n",
      "ValLoss: 0.8302 MAE: 0.7249 RMSE: 0.9112\n",
      "Epoch 30: TrainLoss 0.8248 RecLoss: 0.0000 (left: 3:41:07)\n",
      "TestLoss: 0.8961 MAE: 0.7470 RMSE: 0.9466\n",
      "ValLoss: 0.8287 MAE: 0.7214 RMSE: 0.9103\n",
      "Epoch 31: TrainLoss 0.8247 RecLoss: 0.0000 (left: 3:37:33)\n",
      "TestLoss: 0.8968 MAE: 0.7476 RMSE: 0.9470\n",
      "ValLoss: 0.8279 MAE: 0.7174 RMSE: 0.9099\n",
      "Epoch 32: TrainLoss 0.8241 RecLoss: 0.0000 (left: 3:34:01)\n",
      "TestLoss: 0.8930 MAE: 0.7499 RMSE: 0.9450\n",
      "ValLoss: 0.8307 MAE: 0.7238 RMSE: 0.9114\n",
      "Epoch 33: TrainLoss 0.8245 RecLoss: 0.0000 (left: 3:30:35)\n",
      "TestLoss: 0.9152 MAE: 0.7641 RMSE: 0.9567\n",
      "ValLoss: 0.8388 MAE: 0.7310 RMSE: 0.9158\n",
      "Epoch 34: TrainLoss 0.8237 RecLoss: 0.0000 (left: 3:27:05)\n",
      "TestLoss: 0.8890 MAE: 0.7467 RMSE: 0.9429\n",
      "ValLoss: 0.8259 MAE: 0.7211 RMSE: 0.9088\n",
      "Epoch 35: TrainLoss 0.8236 RecLoss: 0.0000 (left: 3:23:42)\n",
      "TestLoss: 0.8912 MAE: 0.7468 RMSE: 0.9441\n",
      "ValLoss: 0.8264 MAE: 0.7192 RMSE: 0.9091\n",
      "Epoch 36: TrainLoss 0.8234 RecLoss: 0.0000 (left: 3:20:26)\n",
      "TestLoss: 0.8983 MAE: 0.7539 RMSE: 0.9478\n",
      "ValLoss: 0.8284 MAE: 0.7236 RMSE: 0.9101\n",
      "Epoch 37: TrainLoss 0.8230 RecLoss: 0.0000 (left: 3:16:57)\n",
      "TestLoss: 0.8852 MAE: 0.7395 RMSE: 0.9409\n",
      "ValLoss: 0.8320 MAE: 0.7163 RMSE: 0.9122\n",
      "Epoch 38: TrainLoss 0.8235 RecLoss: 0.0000 (left: 3:13:30)\n",
      "TestLoss: 0.8851 MAE: 0.7431 RMSE: 0.9408\n",
      "ValLoss: 0.8262 MAE: 0.7181 RMSE: 0.9090\n",
      "Epoch 39: TrainLoss 0.8225 RecLoss: 0.0000 (left: 3:10:09)\n",
      "TestLoss: 0.8913 MAE: 0.7462 RMSE: 0.9441\n",
      "ValLoss: 0.8260 MAE: 0.7188 RMSE: 0.9088\n",
      "Epoch 40: TrainLoss 0.8222 RecLoss: 0.0000 (left: 3:06:48)\n",
      "TestLoss: 0.8924 MAE: 0.7403 RMSE: 0.9447\n",
      "ValLoss: 0.8294 MAE: 0.7157 RMSE: 0.9107\n",
      "Epoch 41: TrainLoss 0.8227 RecLoss: 0.0000 (left: 3:03:29)\n",
      "TestLoss: 0.8924 MAE: 0.7488 RMSE: 0.9447\n",
      "ValLoss: 0.8264 MAE: 0.7219 RMSE: 0.9091\n",
      "Epoch 42: TrainLoss 0.8219 RecLoss: 0.0000 (left: 3:00:10)\n",
      "TestLoss: 0.8913 MAE: 0.7497 RMSE: 0.9441\n",
      "ValLoss: 0.8259 MAE: 0.7221 RMSE: 0.9088\n",
      "Epoch 43: TrainLoss 0.8225 RecLoss: 0.0000 (left: 2:56:53)\n",
      "TestLoss: 0.8842 MAE: 0.7410 RMSE: 0.9403\n",
      "ValLoss: 0.8238 MAE: 0.7167 RMSE: 0.9076\n",
      "Epoch 44: TrainLoss 0.8215 RecLoss: 0.0000 (left: 2:53:34)\n",
      "TestLoss: 0.8893 MAE: 0.7468 RMSE: 0.9430\n",
      "ValLoss: 0.8254 MAE: 0.7204 RMSE: 0.9085\n",
      "Epoch 45: TrainLoss 0.8211 RecLoss: 0.0000 (left: 2:50:21)\n",
      "TestLoss: 0.8852 MAE: 0.7405 RMSE: 0.9409\n",
      "ValLoss: 0.8261 MAE: 0.7165 RMSE: 0.9089\n",
      "Epoch 46: TrainLoss 0.8211 RecLoss: 0.0000 (left: 2:47:09)\n",
      "TestLoss: 0.8848 MAE: 0.7401 RMSE: 0.9406\n",
      "ValLoss: 0.8236 MAE: 0.7174 RMSE: 0.9075\n",
      "Epoch 47: TrainLoss 0.8208 RecLoss: 0.0000 (left: 2:43:50)\n",
      "TestLoss: 0.8892 MAE: 0.7432 RMSE: 0.9430\n",
      "ValLoss: 0.8270 MAE: 0.7172 RMSE: 0.9094\n",
      "Epoch 48: TrainLoss 0.8212 RecLoss: 0.0000 (left: 2:40:36)\n",
      "TestLoss: 0.8815 MAE: 0.7382 RMSE: 0.9389\n",
      "ValLoss: 0.8287 MAE: 0.7164 RMSE: 0.9103\n",
      "Epoch 49: TrainLoss 0.8207 RecLoss: 0.0000 (left: 2:37:23)\n",
      "TestLoss: 0.8908 MAE: 0.7456 RMSE: 0.9438\n",
      "ValLoss: 0.8252 MAE: 0.7201 RMSE: 0.9084\n",
      "Epoch 50: TrainLoss 0.8201 RecLoss: 0.0000 (left: 2:34:12)\n",
      "TestLoss: 0.8860 MAE: 0.7430 RMSE: 0.9413\n",
      "ValLoss: 0.8253 MAE: 0.7183 RMSE: 0.9085\n",
      "Epoch 51: TrainLoss 0.8204 RecLoss: 0.0000 (left: 2:31:05)\n",
      "TestLoss: 0.8866 MAE: 0.7422 RMSE: 0.9416\n",
      "ValLoss: 0.8249 MAE: 0.7185 RMSE: 0.9082\n",
      "Epoch 52: TrainLoss 0.8200 RecLoss: 0.0000 (left: 2:27:51)\n",
      "TestLoss: 0.8834 MAE: 0.7418 RMSE: 0.9399\n",
      "ValLoss: 0.8267 MAE: 0.7183 RMSE: 0.9092\n",
      "Epoch 53: TrainLoss 0.8202 RecLoss: 0.0000 (left: 2:24:45)\n",
      "TestLoss: 0.8865 MAE: 0.7415 RMSE: 0.9415\n",
      "ValLoss: 0.8231 MAE: 0.7163 RMSE: 0.9073\n",
      "Epoch 54: TrainLoss 0.8196 RecLoss: 0.0000 (left: 2:21:47)\n",
      "TestLoss: 0.8819 MAE: 0.7409 RMSE: 0.9391\n",
      "ValLoss: 0.8230 MAE: 0.7175 RMSE: 0.9072\n",
      "Epoch 55: TrainLoss 0.8192 RecLoss: 0.0000 (left: 2:18:38)\n",
      "TestLoss: 0.8870 MAE: 0.7473 RMSE: 0.9418\n",
      "ValLoss: 0.8249 MAE: 0.7205 RMSE: 0.9082\n",
      "Epoch 56: TrainLoss 0.8192 RecLoss: 0.0000 (left: 2:15:27)\n",
      "TestLoss: 0.8859 MAE: 0.7446 RMSE: 0.9412\n",
      "ValLoss: 0.8258 MAE: 0.7208 RMSE: 0.9087\n",
      "Epoch 57: TrainLoss 0.8190 RecLoss: 0.0000 (left: 2:12:17)\n",
      "TestLoss: 0.8853 MAE: 0.7418 RMSE: 0.9409\n",
      "ValLoss: 0.8238 MAE: 0.7167 RMSE: 0.9076\n",
      "Epoch 58: TrainLoss 0.8187 RecLoss: 0.0000 (left: 2:09:06)\n",
      "TestLoss: 0.8813 MAE: 0.7392 RMSE: 0.9388\n",
      "ValLoss: 0.8238 MAE: 0.7157 RMSE: 0.9076\n",
      "Epoch 59: TrainLoss 0.8186 RecLoss: 0.0000 (left: 2:05:54)\n",
      "TestLoss: 0.8838 MAE: 0.7427 RMSE: 0.9401\n",
      "ValLoss: 0.8252 MAE: 0.7179 RMSE: 0.9084\n",
      "Epoch 60: TrainLoss 0.8187 RecLoss: 0.0000 (left: 2:02:44)\n",
      "TestLoss: 0.8877 MAE: 0.7486 RMSE: 0.9422\n",
      "ValLoss: 0.8246 MAE: 0.7214 RMSE: 0.9081\n",
      "Epoch 61: TrainLoss 0.8183 RecLoss: 0.0000 (left: 1:59:37)\n",
      "TestLoss: 0.8881 MAE: 0.7438 RMSE: 0.9424\n",
      "ValLoss: 0.8249 MAE: 0.7202 RMSE: 0.9082\n",
      "Epoch 62: TrainLoss 0.8180 RecLoss: 0.0000 (left: 1:56:31)\n",
      "TestLoss: 0.8868 MAE: 0.7455 RMSE: 0.9417\n",
      "ValLoss: 0.8234 MAE: 0.7192 RMSE: 0.9074\n",
      "Epoch 63: TrainLoss 0.8177 RecLoss: 0.0000 (left: 1:53:24)\n",
      "TestLoss: 0.8851 MAE: 0.7463 RMSE: 0.9408\n",
      "ValLoss: 0.8254 MAE: 0.7217 RMSE: 0.9085\n",
      "Epoch 64: TrainLoss 0.8176 RecLoss: 0.0000 (left: 1:50:18)\n",
      "TestLoss: 0.8826 MAE: 0.7447 RMSE: 0.9394\n",
      "ValLoss: 0.8248 MAE: 0.7199 RMSE: 0.9082\n",
      "Epoch 65: TrainLoss 0.8174 RecLoss: 0.0000 (left: 1:47:11)\n",
      "TestLoss: 0.8764 MAE: 0.7395 RMSE: 0.9362\n",
      "ValLoss: 0.8231 MAE: 0.7180 RMSE: 0.9073\n",
      "Epoch 66: TrainLoss 0.8171 RecLoss: 0.0000 (left: 1:44:05)\n",
      "TestLoss: 0.8868 MAE: 0.7431 RMSE: 0.9417\n",
      "ValLoss: 0.8224 MAE: 0.7176 RMSE: 0.9069\n",
      "Epoch 67: TrainLoss 0.8173 RecLoss: 0.0000 (left: 1:40:58)\n",
      "TestLoss: 0.8837 MAE: 0.7428 RMSE: 0.9401\n",
      "ValLoss: 0.8253 MAE: 0.7190 RMSE: 0.9085\n",
      "Epoch 68: TrainLoss 0.8171 RecLoss: 0.0000 (left: 1:37:51)\n",
      "TestLoss: 0.8819 MAE: 0.7409 RMSE: 0.9391\n",
      "ValLoss: 0.8231 MAE: 0.7170 RMSE: 0.9072\n",
      "Epoch 69: TrainLoss 0.8167 RecLoss: 0.0000 (left: 1:34:44)\n",
      "TestLoss: 0.8830 MAE: 0.7445 RMSE: 0.9397\n",
      "ValLoss: 0.8227 MAE: 0.7194 RMSE: 0.9070\n",
      "Epoch 70: TrainLoss 0.8167 RecLoss: 0.0000 (left: 1:31:37)\n",
      "TestLoss: 0.8839 MAE: 0.7449 RMSE: 0.9401\n",
      "ValLoss: 0.8218 MAE: 0.7189 RMSE: 0.9065\n",
      "Epoch 71: TrainLoss 0.8165 RecLoss: 0.0000 (left: 1:28:31)\n",
      "TestLoss: 0.8839 MAE: 0.7427 RMSE: 0.9402\n",
      "ValLoss: 0.8227 MAE: 0.7178 RMSE: 0.9071\n",
      "Epoch 72: TrainLoss 0.8166 RecLoss: 0.0000 (left: 1:25:24)\n",
      "TestLoss: 0.8879 MAE: 0.7454 RMSE: 0.9423\n",
      "ValLoss: 0.8234 MAE: 0.7201 RMSE: 0.9074\n",
      "Epoch 73: TrainLoss 0.8158 RecLoss: 0.0000 (left: 1:22:18)\n",
      "TestLoss: 0.8844 MAE: 0.7435 RMSE: 0.9404\n",
      "ValLoss: 0.8227 MAE: 0.7191 RMSE: 0.9070\n",
      "Epoch 74: TrainLoss 0.8162 RecLoss: 0.0000 (left: 1:19:13)\n",
      "TestLoss: 0.8809 MAE: 0.7383 RMSE: 0.9386\n",
      "ValLoss: 0.8222 MAE: 0.7149 RMSE: 0.9068\n",
      "Epoch 75: TrainLoss 0.8159 RecLoss: 0.0000 (left: 1:16:09)\n",
      "TestLoss: 0.8807 MAE: 0.7381 RMSE: 0.9385\n",
      "ValLoss: 0.8240 MAE: 0.7159 RMSE: 0.9077\n",
      "Epoch 76: TrainLoss 0.8156 RecLoss: 0.0000 (left: 1:13:05)\n",
      "TestLoss: 0.8831 MAE: 0.7431 RMSE: 0.9398\n",
      "ValLoss: 0.8212 MAE: 0.7167 RMSE: 0.9062\n",
      "Epoch 77: TrainLoss 0.8154 RecLoss: 0.0000 (left: 1:10:01)\n",
      "TestLoss: 0.8823 MAE: 0.7414 RMSE: 0.9393\n",
      "ValLoss: 0.8206 MAE: 0.7161 RMSE: 0.9059\n",
      "Epoch 78: TrainLoss 0.8154 RecLoss: 0.0000 (left: 1:06:58)\n",
      "TestLoss: 0.8811 MAE: 0.7420 RMSE: 0.9387\n",
      "ValLoss: 0.8209 MAE: 0.7175 RMSE: 0.9060\n",
      "Epoch 79: TrainLoss 0.8149 RecLoss: 0.0000 (left: 1:03:54)\n",
      "TestLoss: 0.8796 MAE: 0.7398 RMSE: 0.9379\n",
      "ValLoss: 0.8205 MAE: 0.7159 RMSE: 0.9058\n",
      "Epoch 80: TrainLoss 0.8150 RecLoss: 0.0000 (left: 1:00:50)\n",
      "TestLoss: 0.8766 MAE: 0.7377 RMSE: 0.9363\n",
      "ValLoss: 0.8216 MAE: 0.7156 RMSE: 0.9064\n",
      "Epoch 81: TrainLoss 0.8148 RecLoss: 0.0000 (left: 0:57:45)\n",
      "TestLoss: 0.8909 MAE: 0.7504 RMSE: 0.9439\n",
      "ValLoss: 0.8261 MAE: 0.7238 RMSE: 0.9089\n",
      "Epoch 82: TrainLoss 0.8147 RecLoss: 0.0000 (left: 0:54:41)\n",
      "TestLoss: 0.8825 MAE: 0.7421 RMSE: 0.9394\n",
      "ValLoss: 0.8222 MAE: 0.7176 RMSE: 0.9067\n",
      "Epoch 83: TrainLoss 0.8147 RecLoss: 0.0000 (left: 0:51:38)\n",
      "TestLoss: 0.8787 MAE: 0.7429 RMSE: 0.9374\n",
      "ValLoss: 0.8224 MAE: 0.7191 RMSE: 0.9069\n",
      "Epoch 84: TrainLoss 0.8142 RecLoss: 0.0000 (left: 0:48:35)\n",
      "TestLoss: 0.8843 MAE: 0.7456 RMSE: 0.9404\n",
      "ValLoss: 0.8230 MAE: 0.7214 RMSE: 0.9072\n",
      "Epoch 85: TrainLoss 0.8142 RecLoss: 0.0000 (left: 0:45:32)\n",
      "TestLoss: 0.8789 MAE: 0.7385 RMSE: 0.9375\n",
      "ValLoss: 0.8202 MAE: 0.7157 RMSE: 0.9057\n",
      "Epoch 86: TrainLoss 0.8142 RecLoss: 0.0000 (left: 0:42:29)\n",
      "TestLoss: 0.8800 MAE: 0.7380 RMSE: 0.9381\n",
      "ValLoss: 0.8230 MAE: 0.7148 RMSE: 0.9072\n",
      "Epoch 87: TrainLoss 0.8140 RecLoss: 0.0000 (left: 0:39:26)\n",
      "TestLoss: 0.8749 MAE: 0.7385 RMSE: 0.9354\n",
      "ValLoss: 0.8211 MAE: 0.7169 RMSE: 0.9061\n",
      "Epoch 88: TrainLoss 0.8137 RecLoss: 0.0000 (left: 0:36:24)\n",
      "TestLoss: 0.8904 MAE: 0.7486 RMSE: 0.9436\n",
      "ValLoss: 0.8253 MAE: 0.7221 RMSE: 0.9085\n",
      "Epoch 89: TrainLoss 0.8140 RecLoss: 0.0000 (left: 0:33:22)\n",
      "TestLoss: 0.8841 MAE: 0.7456 RMSE: 0.9403\n",
      "ValLoss: 0.8233 MAE: 0.7209 RMSE: 0.9073\n",
      "Epoch 90: TrainLoss 0.8136 RecLoss: 0.0000 (left: 0:30:19)\n",
      "TestLoss: 0.8744 MAE: 0.7338 RMSE: 0.9351\n",
      "ValLoss: 0.8245 MAE: 0.7143 RMSE: 0.9080\n",
      "Epoch 91: TrainLoss 0.8133 RecLoss: 0.0000 (left: 0:27:17)\n",
      "TestLoss: 0.8816 MAE: 0.7444 RMSE: 0.9389\n",
      "ValLoss: 0.8232 MAE: 0.7202 RMSE: 0.9073\n",
      "Epoch 92: TrainLoss 0.8133 RecLoss: 0.0000 (left: 0:24:14)\n",
      "TestLoss: 0.8834 MAE: 0.7358 RMSE: 0.9399\n",
      "ValLoss: 0.8258 MAE: 0.7128 RMSE: 0.9087\n",
      "Epoch 93: TrainLoss 0.8130 RecLoss: 0.0000 (left: 0:21:12)\n",
      "TestLoss: 0.8828 MAE: 0.7453 RMSE: 0.9396\n",
      "ValLoss: 0.8225 MAE: 0.7194 RMSE: 0.9069\n",
      "Epoch 94: TrainLoss 0.8130 RecLoss: 0.0000 (left: 0:18:10)\n",
      "TestLoss: 0.8910 MAE: 0.7500 RMSE: 0.9439\n",
      "ValLoss: 0.8243 MAE: 0.7219 RMSE: 0.9079\n",
      "Epoch 95: TrainLoss 0.8131 RecLoss: 0.0000 (left: 0:15:08)\n",
      "TestLoss: 0.8769 MAE: 0.7387 RMSE: 0.9365\n",
      "ValLoss: 0.8212 MAE: 0.7160 RMSE: 0.9062\n",
      "Epoch 96: TrainLoss 0.8125 RecLoss: 0.0000 (left: 0:12:06)\n",
      "TestLoss: 0.8821 MAE: 0.7435 RMSE: 0.9392\n",
      "ValLoss: 0.8204 MAE: 0.7182 RMSE: 0.9058\n",
      "Epoch 97: TrainLoss 0.8127 RecLoss: 0.0000 (left: 0:09:04)\n",
      "TestLoss: 0.8791 MAE: 0.7376 RMSE: 0.9376\n",
      "ValLoss: 0.8210 MAE: 0.7146 RMSE: 0.9061\n",
      "Epoch 98: TrainLoss 0.8128 RecLoss: 0.0000 (left: 0:06:02)\n",
      "TestLoss: 0.8809 MAE: 0.7432 RMSE: 0.9386\n",
      "ValLoss: 0.8197 MAE: 0.7170 RMSE: 0.9054\n",
      "Epoch 99: TrainLoss 0.8123 RecLoss: 0.0000 (left: 0:03:01)\n",
      "TestLoss: 0.8766 MAE: 0.7400 RMSE: 0.9363\n",
      "ValLoss: 0.8201 MAE: 0.7161 RMSE: 0.9056\n",
      "Extra : False\n",
      "-------Dataset Info--------\n",
      "split way [threshold] with threshold 30 training_ratio 1.0\n",
      "train set size: support/query 233956/741113\n",
      "test set size: support/query 523/4708\n",
      "USER HIS DICT: 6040\n",
      "NUM IS: 6040\n",
      "Key Test Result: MAE: 0.6509 RMSE: 0.8228 NDCG: 0.0000\n",
      "CORE IS SELECTED:\n",
      "USER HIS DICT: 6040\n",
      "NUM IS: 6040\n",
      "Que Test Result: MAE: 0.7423 RMSE: 0.9382 NDCG: 0.0000\n",
      "All Test Result: MAE: 0.7332 RMSE: 0.9273 NDCG: 0.0000\n"
     ]
    }
   ],
   "source": [
    "!python pretrain-1m.py\n",
    "!python train-1m.py\n",
    "!python test-1m.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 60 % core users in support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Dataset Info--------\n",
      "split way [threshold] with threshold 30 training_ratio 1.0\n",
      "train set size: support/query 708509/266560\n",
      "test set size: support/query 3138/2093\n",
      "Epoch 0 Step 658: Train 1.5459 Reg: 0.6121\n",
      "Test: 0.9011 MAE: 0.7494 RMSE: 0.9493\n",
      "Val: 0.8290 MAE: 0.7239 RMSE: 0.9105\n",
      "Epoch 1 Step 1316: Train 0.8090 Reg: 0.4047\n",
      "Test: 0.9057 MAE: 0.7540 RMSE: 0.9517\n",
      "Val: 0.8244 MAE: 0.7215 RMSE: 0.9080\n",
      "Epoch 2 Step 1974: Train 0.8048 Reg: 0.3479\n",
      "Test: 0.9090 MAE: 0.7535 RMSE: 0.9534\n",
      "Val: 0.8220 MAE: 0.7200 RMSE: 0.9066\n",
      "Epoch 3 Step 2632: Train 0.7988 Reg: 0.3164\n",
      "Test: 0.8792 MAE: 0.7415 RMSE: 0.9377\n",
      "Val: 0.8157 MAE: 0.7180 RMSE: 0.9032\n",
      "Epoch 4 Step 3290: Train 0.7905 Reg: 0.2901\n",
      "Test: 0.8816 MAE: 0.7445 RMSE: 0.9389\n",
      "Val: 0.8047 MAE: 0.7118 RMSE: 0.8971\n",
      "Epoch 5 Step 3948: Train 0.7781 Reg: 0.2756\n",
      "Test: 0.8707 MAE: 0.7311 RMSE: 0.9331\n",
      "Val: 0.7929 MAE: 0.7026 RMSE: 0.8904\n",
      "Epoch 6 Step 4606: Train 0.7613 Reg: 0.2828\n",
      "Test: 0.8426 MAE: 0.7280 RMSE: 0.9179\n",
      "Val: 0.7693 MAE: 0.6952 RMSE: 0.8771\n",
      "Epoch 7 Step 5264: Train 0.7400 Reg: 0.3179\n",
      "Test: 0.8274 MAE: 0.7212 RMSE: 0.9096\n",
      "Val: 0.7571 MAE: 0.6899 RMSE: 0.8701\n",
      "Epoch 8 Step 5922: Train 0.7288 Reg: 0.3196\n",
      "Test: 0.8173 MAE: 0.7143 RMSE: 0.9040\n",
      "Val: 0.7490 MAE: 0.6839 RMSE: 0.8654\n",
      "Epoch 9 Step 6580: Train 0.7168 Reg: 0.3322\n",
      "Test: 0.8081 MAE: 0.7084 RMSE: 0.8990\n",
      "Val: 0.7356 MAE: 0.6748 RMSE: 0.8577\n",
      "Epoch 10 Step 7238: Train 0.7001 Reg: 0.3620\n",
      "Test: 0.7909 MAE: 0.7052 RMSE: 0.8893\n",
      "Val: 0.7222 MAE: 0.6709 RMSE: 0.8498\n",
      "Epoch 11 Step 7896: Train 0.6848 Reg: 0.3871\n",
      "Test: 0.7762 MAE: 0.6967 RMSE: 0.8810\n",
      "Val: 0.7154 MAE: 0.6646 RMSE: 0.8458\n",
      "Epoch 12 Step 8554: Train 0.6680 Reg: 0.4181\n",
      "Test: 0.7696 MAE: 0.6922 RMSE: 0.8772\n",
      "Val: 0.7054 MAE: 0.6592 RMSE: 0.8399\n",
      "Epoch 13 Step 9212: Train 0.6499 Reg: 0.4567\n",
      "Test: 0.7637 MAE: 0.6925 RMSE: 0.8739\n",
      "Val: 0.6970 MAE: 0.6541 RMSE: 0.8349\n",
      "Epoch 14 Step 9870: Train 0.6293 Reg: 0.4898\n",
      "Test: 0.7523 MAE: 0.6861 RMSE: 0.8674\n",
      "Val: 0.6928 MAE: 0.6522 RMSE: 0.8323\n",
      "Epoch 15 Step 10528: Train 0.6105 Reg: 0.5158\n",
      "Test: 0.7539 MAE: 0.6829 RMSE: 0.8683\n",
      "Val: 0.6925 MAE: 0.6508 RMSE: 0.8321\n",
      "Epoch 16 Step 11186: Train 0.5937 Reg: 0.5405\n",
      "Test: 0.7557 MAE: 0.6845 RMSE: 0.8693\n",
      "Val: 0.6947 MAE: 0.6522 RMSE: 0.8335\n",
      "Epoch 17 Step 11844: Train 0.5762 Reg: 0.5657\n",
      "Test: 0.7563 MAE: 0.6865 RMSE: 0.8697\n",
      "Val: 0.7000 MAE: 0.6558 RMSE: 0.8367\n",
      "Epoch 18 Step 12502: Train 0.5585 Reg: 0.5851\n",
      "Test: 0.7648 MAE: 0.6876 RMSE: 0.8745\n",
      "Val: 0.7071 MAE: 0.6571 RMSE: 0.8409\n",
      "Epoch 19 Step 13160: Train 0.5433 Reg: 0.5987\n",
      "Test: 0.7712 MAE: 0.6943 RMSE: 0.8782\n",
      "Val: 0.7123 MAE: 0.6599 RMSE: 0.8440\n",
      "Epoch 20 Step 13818: Train 0.5305 Reg: 0.6071\n",
      "Test: 0.7773 MAE: 0.6932 RMSE: 0.8817\n",
      "Val: 0.7202 MAE: 0.6614 RMSE: 0.8486\n",
      "Epoch 21 Step 14476: Train 0.5189 Reg: 0.6141\n",
      "Test: 0.7760 MAE: 0.6949 RMSE: 0.8809\n",
      "Val: 0.7279 MAE: 0.6660 RMSE: 0.8532\n",
      "Epoch 22 Step 15134: Train 0.5082 Reg: 0.6189\n",
      "Test: 0.7873 MAE: 0.6964 RMSE: 0.8873\n",
      "Val: 0.7374 MAE: 0.6696 RMSE: 0.8587\n",
      "Epoch 23 Step 15792: Train 0.4986 Reg: 0.6205\n",
      "Test: 0.7920 MAE: 0.6990 RMSE: 0.8899\n",
      "Val: 0.7449 MAE: 0.6725 RMSE: 0.8631\n",
      "Epoch 24 Step 16450: Train 0.4903 Reg: 0.6197\n",
      "Test: 0.7972 MAE: 0.7006 RMSE: 0.8929\n",
      "Val: 0.7520 MAE: 0.6748 RMSE: 0.8672\n",
      "Epoch 25 Step 17108: Train 0.4838 Reg: 0.6181\n",
      "Test: 0.8066 MAE: 0.7055 RMSE: 0.8981\n",
      "Val: 0.7556 MAE: 0.6777 RMSE: 0.8693\n",
      "Epoch 26 Step 17766: Train 0.4760 Reg: 0.6183\n",
      "Test: 0.8037 MAE: 0.7029 RMSE: 0.8965\n",
      "Val: 0.7632 MAE: 0.6798 RMSE: 0.8736\n",
      "Epoch 27 Step 18424: Train 0.4684 Reg: 0.6179\n",
      "Test: 0.8192 MAE: 0.7060 RMSE: 0.9051\n",
      "Val: 0.7717 MAE: 0.6817 RMSE: 0.8785\n",
      "Epoch 28 Step 19082: Train 0.4612 Reg: 0.6161\n",
      "Test: 0.8319 MAE: 0.7125 RMSE: 0.9121\n",
      "Val: 0.7784 MAE: 0.6859 RMSE: 0.8822\n",
      "Epoch 29 Step 19740: Train 0.4551 Reg: 0.6128\n",
      "Test: 0.8364 MAE: 0.7120 RMSE: 0.9145\n",
      "Val: 0.7853 MAE: 0.6870 RMSE: 0.8862\n",
      "Epoch 30 Step 20398: Train 0.4500 Reg: 0.6079\n",
      "Test: 0.8414 MAE: 0.7146 RMSE: 0.9173\n",
      "Val: 0.7911 MAE: 0.6906 RMSE: 0.8894\n",
      "Epoch 31 Step 21056: Train 0.4457 Reg: 0.6024\n",
      "Test: 0.8513 MAE: 0.7181 RMSE: 0.9226\n",
      "Val: 0.7992 MAE: 0.6936 RMSE: 0.8940\n",
      "Epoch 32 Step 21714: Train 0.4415 Reg: 0.5972\n",
      "Test: 0.8544 MAE: 0.7185 RMSE: 0.9243\n",
      "Val: 0.8024 MAE: 0.6954 RMSE: 0.8958\n",
      "Epoch 33 Step 22372: Train 0.4379 Reg: 0.5915\n",
      "Test: 0.8610 MAE: 0.7178 RMSE: 0.9279\n",
      "Val: 0.8079 MAE: 0.6954 RMSE: 0.8988\n",
      "Epoch 34 Step 23030: Train 0.4345 Reg: 0.5857\n",
      "Test: 0.8712 MAE: 0.7220 RMSE: 0.9334\n",
      "Val: 0.8121 MAE: 0.6963 RMSE: 0.9012\n",
      "Epoch 35 Step 23688: Train 0.4312 Reg: 0.5802\n",
      "Test: 0.8720 MAE: 0.7237 RMSE: 0.9338\n",
      "Val: 0.8153 MAE: 0.6984 RMSE: 0.9029\n",
      "Epoch 36 Step 24346: Train 0.4282 Reg: 0.5746\n",
      "Test: 0.8814 MAE: 0.7268 RMSE: 0.9388\n",
      "Val: 0.8222 MAE: 0.7010 RMSE: 0.9068\n",
      "Epoch 37 Step 25004: Train 0.4251 Reg: 0.5689\n",
      "Test: 0.8843 MAE: 0.7266 RMSE: 0.9404\n",
      "Val: 0.8259 MAE: 0.7017 RMSE: 0.9088\n",
      "Epoch 38 Step 25662: Train 0.4224 Reg: 0.5636\n",
      "Test: 0.8878 MAE: 0.7274 RMSE: 0.9422\n",
      "Val: 0.8287 MAE: 0.7031 RMSE: 0.9103\n",
      "Epoch 39 Step 26320: Train 0.4197 Reg: 0.5582\n",
      "Test: 0.8950 MAE: 0.7299 RMSE: 0.9460\n",
      "Val: 0.8335 MAE: 0.7051 RMSE: 0.9130\n",
      "Epoch 40 Step 26978: Train 0.4172 Reg: 0.5531\n",
      "Test: 0.9037 MAE: 0.7328 RMSE: 0.9506\n",
      "Val: 0.8364 MAE: 0.7064 RMSE: 0.9145\n",
      "Epoch 41 Step 27636: Train 0.4147 Reg: 0.5481\n",
      "Test: 0.9050 MAE: 0.7323 RMSE: 0.9513\n",
      "Val: 0.8420 MAE: 0.7073 RMSE: 0.9176\n",
      "Epoch 42 Step 28294: Train 0.4124 Reg: 0.5431\n",
      "Test: 0.9120 MAE: 0.7374 RMSE: 0.9550\n",
      "Val: 0.8457 MAE: 0.7112 RMSE: 0.9196\n",
      "Epoch 43 Step 28952: Train 0.4102 Reg: 0.5385\n",
      "Test: 0.9138 MAE: 0.7356 RMSE: 0.9559\n",
      "Val: 0.8474 MAE: 0.7098 RMSE: 0.9205\n",
      "Epoch 44 Step 29610: Train 0.4079 Reg: 0.5338\n",
      "Test: 0.9169 MAE: 0.7368 RMSE: 0.9575\n",
      "Val: 0.8515 MAE: 0.7120 RMSE: 0.9228\n",
      "Epoch 45 Step 30268: Train 0.4059 Reg: 0.5293\n",
      "Test: 0.9219 MAE: 0.7397 RMSE: 0.9601\n",
      "Val: 0.8556 MAE: 0.7142 RMSE: 0.9250\n",
      "Epoch 46 Step 30926: Train 0.4039 Reg: 0.5252\n",
      "Test: 0.9240 MAE: 0.7382 RMSE: 0.9613\n",
      "Val: 0.8571 MAE: 0.7134 RMSE: 0.9258\n",
      "Epoch 47 Step 31584: Train 0.4019 Reg: 0.5209\n",
      "Test: 0.9315 MAE: 0.7419 RMSE: 0.9651\n",
      "Val: 0.8601 MAE: 0.7149 RMSE: 0.9274\n",
      "Epoch 48 Step 32242: Train 0.4001 Reg: 0.5169\n",
      "Test: 0.9335 MAE: 0.7415 RMSE: 0.9662\n",
      "Val: 0.8637 MAE: 0.7157 RMSE: 0.9294\n",
      "Epoch 49 Step 32900: Train 0.3984 Reg: 0.5131\n",
      "Test: 0.9358 MAE: 0.7420 RMSE: 0.9674\n",
      "Val: 0.8663 MAE: 0.7168 RMSE: 0.9307\n",
      "Epoch 50 Step 33558: Train 0.3967 Reg: 0.5093\n",
      "Test: 0.9404 MAE: 0.7428 RMSE: 0.9697\n",
      "Val: 0.8694 MAE: 0.7172 RMSE: 0.9324\n",
      "Epoch 51 Step 34216: Train 0.3950 Reg: 0.5058\n",
      "Test: 0.9448 MAE: 0.7442 RMSE: 0.9720\n",
      "Val: 0.8716 MAE: 0.7184 RMSE: 0.9336\n",
      "Epoch 52 Step 34874: Train 0.3934 Reg: 0.5025\n",
      "Test: 0.9472 MAE: 0.7456 RMSE: 0.9732\n",
      "Val: 0.8735 MAE: 0.7196 RMSE: 0.9346\n",
      "Epoch 53 Step 35532: Train 0.3919 Reg: 0.4992\n",
      "Test: 0.9504 MAE: 0.7465 RMSE: 0.9749\n",
      "Val: 0.8764 MAE: 0.7198 RMSE: 0.9362\n",
      "Epoch 54 Step 36190: Train 0.3904 Reg: 0.4961\n",
      "Test: 0.9532 MAE: 0.7469 RMSE: 0.9763\n",
      "Val: 0.8797 MAE: 0.7206 RMSE: 0.9379\n",
      "Epoch 55 Step 36848: Train 0.3890 Reg: 0.4931\n",
      "Test: 0.9552 MAE: 0.7489 RMSE: 0.9773\n",
      "Val: 0.8808 MAE: 0.7217 RMSE: 0.9385\n",
      "Epoch 56 Step 37506: Train 0.3877 Reg: 0.4902\n",
      "Test: 0.9597 MAE: 0.7510 RMSE: 0.9796\n",
      "Val: 0.8833 MAE: 0.7229 RMSE: 0.9398\n",
      "Epoch 57 Step 38164: Train 0.3864 Reg: 0.4874\n",
      "Test: 0.9633 MAE: 0.7510 RMSE: 0.9815\n",
      "Val: 0.8859 MAE: 0.7233 RMSE: 0.9412\n",
      "Epoch 58 Step 38822: Train 0.3851 Reg: 0.4849\n",
      "Test: 0.9660 MAE: 0.7516 RMSE: 0.9829\n",
      "Val: 0.8871 MAE: 0.7237 RMSE: 0.9418\n",
      "Epoch 59 Step 39480: Train 0.3840 Reg: 0.4823\n",
      "Test: 0.9691 MAE: 0.7522 RMSE: 0.9844\n",
      "Val: 0.8894 MAE: 0.7243 RMSE: 0.9431\n",
      "Epoch 60 Step 40138: Train 0.3828 Reg: 0.4798\n",
      "Test: 0.9718 MAE: 0.7523 RMSE: 0.9858\n",
      "Val: 0.8914 MAE: 0.7242 RMSE: 0.9442\n",
      "Epoch 61 Step 40796: Train 0.3817 Reg: 0.4776\n",
      "Test: 0.9750 MAE: 0.7546 RMSE: 0.9874\n",
      "Val: 0.8925 MAE: 0.7255 RMSE: 0.9447\n",
      "Epoch 62 Step 41454: Train 0.3807 Reg: 0.4754\n",
      "Test: 0.9769 MAE: 0.7552 RMSE: 0.9884\n",
      "Val: 0.8940 MAE: 0.7255 RMSE: 0.9455\n",
      "Epoch 63 Step 42112: Train 0.3796 Reg: 0.4733\n",
      "Test: 0.9785 MAE: 0.7551 RMSE: 0.9892\n",
      "Val: 0.8961 MAE: 0.7262 RMSE: 0.9466\n",
      "Epoch 64 Step 42770: Train 0.3786 Reg: 0.4713\n",
      "Test: 0.9809 MAE: 0.7567 RMSE: 0.9904\n",
      "Val: 0.8974 MAE: 0.7272 RMSE: 0.9473\n",
      "Epoch 65 Step 43428: Train 0.3776 Reg: 0.4693\n",
      "Test: 0.9826 MAE: 0.7568 RMSE: 0.9913\n",
      "Val: 0.8991 MAE: 0.7274 RMSE: 0.9482\n",
      "Epoch 66 Step 44086: Train 0.3768 Reg: 0.4676\n",
      "Test: 0.9841 MAE: 0.7569 RMSE: 0.9920\n",
      "Val: 0.9007 MAE: 0.7276 RMSE: 0.9490\n",
      "Epoch 67 Step 44744: Train 0.3759 Reg: 0.4658\n",
      "Test: 0.9869 MAE: 0.7580 RMSE: 0.9934\n",
      "Val: 0.9021 MAE: 0.7284 RMSE: 0.9498\n",
      "Epoch 68 Step 45402: Train 0.3751 Reg: 0.4641\n",
      "Test: 0.9885 MAE: 0.7588 RMSE: 0.9943\n",
      "Val: 0.9033 MAE: 0.7288 RMSE: 0.9504\n",
      "Epoch 69 Step 46060: Train 0.3743 Reg: 0.4625\n",
      "Test: 0.9898 MAE: 0.7596 RMSE: 0.9949\n",
      "Val: 0.9044 MAE: 0.7294 RMSE: 0.9510\n",
      "Epoch 70 Step 46718: Train 0.3735 Reg: 0.4610\n",
      "Test: 0.9927 MAE: 0.7606 RMSE: 0.9964\n",
      "Val: 0.9060 MAE: 0.7302 RMSE: 0.9518\n",
      "Epoch 71 Step 47376: Train 0.3728 Reg: 0.4595\n",
      "Test: 0.9936 MAE: 0.7603 RMSE: 0.9968\n",
      "Val: 0.9071 MAE: 0.7302 RMSE: 0.9524\n",
      "Epoch 72 Step 48034: Train 0.3721 Reg: 0.4581\n",
      "Test: 0.9951 MAE: 0.7602 RMSE: 0.9976\n",
      "Val: 0.9088 MAE: 0.7299 RMSE: 0.9533\n",
      "Epoch 73 Step 48692: Train 0.3715 Reg: 0.4568\n",
      "Test: 0.9959 MAE: 0.7611 RMSE: 0.9979\n",
      "Val: 0.9090 MAE: 0.7309 RMSE: 0.9534\n",
      "Epoch 74 Step 49350: Train 0.3708 Reg: 0.4555\n",
      "Test: 0.9974 MAE: 0.7617 RMSE: 0.9987\n",
      "Val: 0.9101 MAE: 0.7311 RMSE: 0.9540\n",
      "Epoch 75 Step 50008: Train 0.3702 Reg: 0.4543\n",
      "Test: 0.9989 MAE: 0.7620 RMSE: 0.9994\n",
      "Val: 0.9110 MAE: 0.7314 RMSE: 0.9545\n",
      "Epoch 76 Step 50666: Train 0.3696 Reg: 0.4531\n",
      "Test: 1.0006 MAE: 0.7624 RMSE: 1.0003\n",
      "Val: 0.9121 MAE: 0.7319 RMSE: 0.9551\n",
      "Epoch 77 Step 51324: Train 0.3691 Reg: 0.4520\n",
      "Test: 1.0018 MAE: 0.7629 RMSE: 1.0009\n",
      "Val: 0.9130 MAE: 0.7322 RMSE: 0.9555\n",
      "Epoch 78 Step 51982: Train 0.3685 Reg: 0.4510\n",
      "Test: 1.0032 MAE: 0.7638 RMSE: 1.0016\n",
      "Val: 0.9139 MAE: 0.7329 RMSE: 0.9560\n",
      "Epoch 79 Step 52640: Train 0.3680 Reg: 0.4500\n",
      "Test: 1.0040 MAE: 0.7637 RMSE: 1.0020\n",
      "Val: 0.9144 MAE: 0.7326 RMSE: 0.9562\n",
      "Epoch 80 Step 53298: Train 0.3675 Reg: 0.4490\n",
      "Test: 1.0061 MAE: 0.7646 RMSE: 1.0030\n",
      "Val: 0.9154 MAE: 0.7332 RMSE: 0.9568\n",
      "Epoch 81 Step 53956: Train 0.3671 Reg: 0.4481\n",
      "Test: 1.0068 MAE: 0.7644 RMSE: 1.0034\n",
      "Val: 0.9161 MAE: 0.7329 RMSE: 0.9571\n",
      "Epoch 82 Step 54614: Train 0.3666 Reg: 0.4473\n",
      "Test: 1.0082 MAE: 0.7653 RMSE: 1.0041\n",
      "Val: 0.9168 MAE: 0.7336 RMSE: 0.9575\n",
      "Epoch 83 Step 55272: Train 0.3662 Reg: 0.4465\n",
      "Test: 1.0089 MAE: 0.7652 RMSE: 1.0044\n",
      "Val: 0.9174 MAE: 0.7336 RMSE: 0.9578\n",
      "Epoch 84 Step 55930: Train 0.3658 Reg: 0.4456\n",
      "Test: 1.0098 MAE: 0.7657 RMSE: 1.0049\n",
      "Val: 0.9180 MAE: 0.7339 RMSE: 0.9581\n",
      "Epoch 85 Step 56588: Train 0.3654 Reg: 0.4449\n",
      "Test: 1.0105 MAE: 0.7658 RMSE: 1.0052\n",
      "Val: 0.9187 MAE: 0.7341 RMSE: 0.9585\n",
      "Epoch 86 Step 57246: Train 0.3651 Reg: 0.4442\n",
      "Test: 1.0114 MAE: 0.7657 RMSE: 1.0057\n",
      "Val: 0.9193 MAE: 0.7340 RMSE: 0.9588\n",
      "Epoch 87 Step 57904: Train 0.3647 Reg: 0.4435\n",
      "Test: 1.0121 MAE: 0.7661 RMSE: 1.0060\n",
      "Val: 0.9196 MAE: 0.7342 RMSE: 0.9590\n",
      "Epoch 88 Step 58562: Train 0.3644 Reg: 0.4429\n",
      "Test: 1.0130 MAE: 0.7665 RMSE: 1.0065\n",
      "Val: 0.9202 MAE: 0.7346 RMSE: 0.9593\n",
      "Epoch 89 Step 59220: Train 0.3641 Reg: 0.4423\n",
      "Test: 1.0137 MAE: 0.7667 RMSE: 1.0068\n",
      "Val: 0.9207 MAE: 0.7346 RMSE: 0.9595\n",
      "Epoch 90 Step 59878: Train 0.3638 Reg: 0.4417\n",
      "Test: 1.0144 MAE: 0.7666 RMSE: 1.0072\n",
      "Val: 0.9212 MAE: 0.7345 RMSE: 0.9598\n",
      "Epoch 91 Step 60536: Train 0.3635 Reg: 0.4411\n",
      "Test: 1.0152 MAE: 0.7672 RMSE: 1.0076\n",
      "Val: 0.9216 MAE: 0.7350 RMSE: 0.9600\n",
      "Epoch 92 Step 61194: Train 0.3632 Reg: 0.4406\n",
      "Test: 1.0157 MAE: 0.7672 RMSE: 1.0078\n",
      "Val: 0.9221 MAE: 0.7350 RMSE: 0.9602\n",
      "Epoch 93 Step 61852: Train 0.3629 Reg: 0.4401\n",
      "Test: 1.0165 MAE: 0.7674 RMSE: 1.0082\n",
      "Val: 0.9226 MAE: 0.7351 RMSE: 0.9605\n",
      "Epoch 94 Step 62510: Train 0.3627 Reg: 0.4396\n",
      "Test: 1.0171 MAE: 0.7674 RMSE: 1.0085\n",
      "Val: 0.9229 MAE: 0.7351 RMSE: 0.9607\n",
      "Epoch 95 Step 63168: Train 0.3624 Reg: 0.4392\n",
      "Test: 1.0176 MAE: 0.7674 RMSE: 1.0088\n",
      "Val: 0.9234 MAE: 0.7350 RMSE: 0.9609\n",
      "Epoch 96 Step 63826: Train 0.3622 Reg: 0.4388\n",
      "Test: 1.0181 MAE: 0.7679 RMSE: 1.0090\n",
      "Val: 0.9236 MAE: 0.7355 RMSE: 0.9610\n",
      "Epoch 97 Step 64484: Train 0.3620 Reg: 0.4383\n",
      "Test: 1.0186 MAE: 0.7678 RMSE: 1.0093\n",
      "Val: 0.9239 MAE: 0.7353 RMSE: 0.9612\n",
      "Epoch 98 Step 65142: Train 0.3618 Reg: 0.4379\n",
      "Test: 1.0193 MAE: 0.7682 RMSE: 1.0096\n",
      "Val: 0.9242 MAE: 0.7357 RMSE: 0.9614\n",
      "Epoch 99 Step 65800: Train 0.3616 Reg: 0.4376\n",
      "Test: 1.0196 MAE: 0.7682 RMSE: 1.0097\n",
      "Val: 0.9246 MAE: 0.7358 RMSE: 0.9615\n",
      "-------Dataset Info--------\n",
      "split way [threshold] with threshold 30 training_ratio 1.0\n",
      "train set size: support/query 708509/266560\n",
      "test set size: support/query 3138/2093\n",
      "Epoch 0: TrainLoss 0.9735 RecLoss: 0.0000 (left: 1:38:42)\n",
      "TestLoss: 0.9405 MAE: 0.7788 RMSE: 0.9698\n",
      "ValLoss: 0.9434 MAE: 0.7736 RMSE: 0.9713\n",
      "Epoch 1: TrainLoss 0.9045 RecLoss: 0.0000 (left: 1:37:52)\n",
      "TestLoss: 0.9128 MAE: 0.7643 RMSE: 0.9554\n",
      "ValLoss: 0.9149 MAE: 0.7591 RMSE: 0.9565\n",
      "Epoch 2: TrainLoss 0.8867 RecLoss: 0.0000 (left: 1:38:29)\n",
      "TestLoss: 0.9123 MAE: 0.7636 RMSE: 0.9551\n",
      "ValLoss: 0.9097 MAE: 0.7580 RMSE: 0.9538\n",
      "Epoch 3: TrainLoss 0.8852 RecLoss: 0.0000 (left: 1:37:30)\n",
      "TestLoss: 0.8971 MAE: 0.7432 RMSE: 0.9472\n",
      "ValLoss: 0.9050 MAE: 0.7390 RMSE: 0.9513\n",
      "Epoch 4: TrainLoss 0.8837 RecLoss: 0.0000 (left: 1:35:37)\n",
      "TestLoss: 0.8981 MAE: 0.7544 RMSE: 0.9477\n",
      "ValLoss: 0.8994 MAE: 0.7496 RMSE: 0.9483\n",
      "Epoch 5: TrainLoss 0.8810 RecLoss: 0.0000 (left: 1:34:52)\n",
      "TestLoss: 0.9008 MAE: 0.7559 RMSE: 0.9491\n",
      "ValLoss: 0.9007 MAE: 0.7514 RMSE: 0.9491\n",
      "Epoch 6: TrainLoss 0.8796 RecLoss: 0.0000 (left: 1:33:45)\n",
      "TestLoss: 0.9037 MAE: 0.7428 RMSE: 0.9506\n",
      "ValLoss: 0.9049 MAE: 0.7374 RMSE: 0.9513\n",
      "Epoch 7: TrainLoss 0.8795 RecLoss: 0.0000 (left: 1:33:18)\n",
      "TestLoss: 0.8953 MAE: 0.7409 RMSE: 0.9462\n",
      "ValLoss: 0.9014 MAE: 0.7374 RMSE: 0.9494\n",
      "Epoch 8: TrainLoss 0.8807 RecLoss: 0.0000 (left: 1:32:37)\n",
      "TestLoss: 0.8925 MAE: 0.7499 RMSE: 0.9447\n",
      "ValLoss: 0.8946 MAE: 0.7458 RMSE: 0.9458\n",
      "Epoch 9: TrainLoss 0.8759 RecLoss: 0.0000 (left: 1:32:03)\n",
      "TestLoss: 0.8862 MAE: 0.7413 RMSE: 0.9414\n",
      "ValLoss: 0.8910 MAE: 0.7377 RMSE: 0.9439\n",
      "Epoch 10: TrainLoss 0.8791 RecLoss: 0.0000 (left: 1:30:52)\n",
      "TestLoss: 0.9328 MAE: 0.7749 RMSE: 0.9658\n",
      "ValLoss: 0.9252 MAE: 0.7701 RMSE: 0.9619\n",
      "Epoch 11: TrainLoss 0.8757 RecLoss: 0.0000 (left: 1:29:55)\n",
      "TestLoss: 0.8907 MAE: 0.7424 RMSE: 0.9438\n",
      "ValLoss: 0.8913 MAE: 0.7384 RMSE: 0.9441\n",
      "Epoch 12: TrainLoss 0.8748 RecLoss: 0.0000 (left: 1:29:08)\n",
      "TestLoss: 0.8950 MAE: 0.7445 RMSE: 0.9461\n",
      "ValLoss: 0.8908 MAE: 0.7383 RMSE: 0.9438\n",
      "Epoch 13: TrainLoss 0.8729 RecLoss: 0.0000 (left: 1:28:22)\n",
      "TestLoss: 0.9065 MAE: 0.7552 RMSE: 0.9521\n",
      "ValLoss: 0.8962 MAE: 0.7489 RMSE: 0.9467\n",
      "Epoch 14: TrainLoss 0.8734 RecLoss: 0.0000 (left: 1:27:27)\n",
      "TestLoss: 0.8942 MAE: 0.7453 RMSE: 0.9456\n",
      "ValLoss: 0.8912 MAE: 0.7402 RMSE: 0.9440\n",
      "Epoch 15: TrainLoss 0.8734 RecLoss: 0.0000 (left: 1:26:39)\n",
      "TestLoss: 0.9032 MAE: 0.7518 RMSE: 0.9504\n",
      "ValLoss: 0.8897 MAE: 0.7452 RMSE: 0.9432\n",
      "Epoch 16: TrainLoss 0.8708 RecLoss: 0.0000 (left: 1:25:52)\n",
      "TestLoss: 0.8891 MAE: 0.7434 RMSE: 0.9429\n",
      "ValLoss: 0.8863 MAE: 0.7393 RMSE: 0.9414\n",
      "Epoch 17: TrainLoss 0.8711 RecLoss: 0.0000 (left: 1:24:34)\n",
      "TestLoss: 0.8883 MAE: 0.7414 RMSE: 0.9425\n",
      "ValLoss: 0.8845 MAE: 0.7371 RMSE: 0.9405\n",
      "Epoch 18: TrainLoss 0.8709 RecLoss: 0.0000 (left: 1:23:30)\n",
      "TestLoss: 0.8926 MAE: 0.7421 RMSE: 0.9448\n",
      "ValLoss: 0.8882 MAE: 0.7353 RMSE: 0.9425\n",
      "Epoch 19: TrainLoss 0.8707 RecLoss: 0.0000 (left: 1:22:22)\n",
      "TestLoss: 0.8985 MAE: 0.7537 RMSE: 0.9479\n",
      "ValLoss: 0.8932 MAE: 0.7493 RMSE: 0.9451\n",
      "Epoch 20: TrainLoss 0.8703 RecLoss: 0.0000 (left: 1:21:16)\n",
      "TestLoss: 0.8926 MAE: 0.7404 RMSE: 0.9448\n",
      "ValLoss: 0.8918 MAE: 0.7362 RMSE: 0.9444\n",
      "Epoch 21: TrainLoss 0.8698 RecLoss: 0.0000 (left: 1:20:00)\n",
      "TestLoss: 0.9018 MAE: 0.7555 RMSE: 0.9496\n",
      "ValLoss: 0.8906 MAE: 0.7479 RMSE: 0.9437\n",
      "Epoch 22: TrainLoss 0.8695 RecLoss: 0.0000 (left: 1:18:47)\n",
      "TestLoss: 0.8876 MAE: 0.7371 RMSE: 0.9421\n",
      "ValLoss: 0.8889 MAE: 0.7331 RMSE: 0.9428\n",
      "Epoch 23: TrainLoss 0.8685 RecLoss: 0.0000 (left: 1:17:34)\n",
      "TestLoss: 0.8908 MAE: 0.7461 RMSE: 0.9438\n",
      "ValLoss: 0.8871 MAE: 0.7390 RMSE: 0.9419\n",
      "Epoch 24: TrainLoss 0.8692 RecLoss: 0.0000 (left: 1:16:33)\n",
      "TestLoss: 0.8883 MAE: 0.7347 RMSE: 0.9425\n",
      "ValLoss: 0.8948 MAE: 0.7317 RMSE: 0.9459\n",
      "Epoch 25: TrainLoss 0.8681 RecLoss: 0.0000 (left: 1:15:30)\n",
      "TestLoss: 0.8848 MAE: 0.7426 RMSE: 0.9407\n",
      "ValLoss: 0.8901 MAE: 0.7399 RMSE: 0.9434\n",
      "Epoch 26: TrainLoss 0.8682 RecLoss: 0.0000 (left: 1:14:23)\n",
      "TestLoss: 0.8885 MAE: 0.7425 RMSE: 0.9426\n",
      "ValLoss: 0.8828 MAE: 0.7375 RMSE: 0.9396\n",
      "Epoch 27: TrainLoss 0.8683 RecLoss: 0.0000 (left: 1:13:16)\n",
      "TestLoss: 0.8870 MAE: 0.7411 RMSE: 0.9418\n",
      "ValLoss: 0.8876 MAE: 0.7383 RMSE: 0.9421\n",
      "Epoch 28: TrainLoss 0.8676 RecLoss: 0.0000 (left: 1:12:06)\n",
      "TestLoss: 0.8996 MAE: 0.7528 RMSE: 0.9485\n",
      "ValLoss: 0.8885 MAE: 0.7438 RMSE: 0.9426\n",
      "Epoch 29: TrainLoss 0.8671 RecLoss: 0.0000 (left: 1:10:56)\n",
      "TestLoss: 0.8889 MAE: 0.7417 RMSE: 0.9428\n",
      "ValLoss: 0.8827 MAE: 0.7353 RMSE: 0.9395\n",
      "Epoch 30: TrainLoss 0.8673 RecLoss: 0.0000 (left: 1:09:50)\n",
      "TestLoss: 0.8884 MAE: 0.7456 RMSE: 0.9425\n",
      "ValLoss: 0.8841 MAE: 0.7411 RMSE: 0.9403\n",
      "Epoch 31: TrainLoss 0.8665 RecLoss: 0.0000 (left: 1:08:48)\n",
      "TestLoss: 0.8911 MAE: 0.7423 RMSE: 0.9440\n",
      "ValLoss: 0.8838 MAE: 0.7356 RMSE: 0.9401\n",
      "Epoch 32: TrainLoss 0.8653 RecLoss: 0.0000 (left: 1:07:41)\n",
      "TestLoss: 0.8867 MAE: 0.7397 RMSE: 0.9416\n",
      "ValLoss: 0.8837 MAE: 0.7342 RMSE: 0.9400\n",
      "Epoch 33: TrainLoss 0.8664 RecLoss: 0.0000 (left: 1:06:38)\n",
      "TestLoss: 0.8933 MAE: 0.7463 RMSE: 0.9451\n",
      "ValLoss: 0.8827 MAE: 0.7385 RMSE: 0.9395\n",
      "Epoch 34: TrainLoss 0.8657 RecLoss: 0.0000 (left: 1:05:36)\n",
      "TestLoss: 0.8905 MAE: 0.7435 RMSE: 0.9436\n",
      "ValLoss: 0.8833 MAE: 0.7362 RMSE: 0.9398\n",
      "Epoch 35: TrainLoss 0.8644 RecLoss: 0.0000 (left: 1:04:30)\n",
      "TestLoss: 0.8870 MAE: 0.7440 RMSE: 0.9418\n",
      "ValLoss: 0.8835 MAE: 0.7403 RMSE: 0.9400\n",
      "Epoch 36: TrainLoss 0.8653 RecLoss: 0.0000 (left: 1:03:28)\n",
      "TestLoss: 0.8853 MAE: 0.7402 RMSE: 0.9409\n",
      "ValLoss: 0.8835 MAE: 0.7362 RMSE: 0.9399\n",
      "Epoch 37: TrainLoss 0.8640 RecLoss: 0.0000 (left: 1:02:29)\n",
      "TestLoss: 0.8885 MAE: 0.7439 RMSE: 0.9426\n",
      "ValLoss: 0.8848 MAE: 0.7381 RMSE: 0.9407\n",
      "Epoch 38: TrainLoss 0.8653 RecLoss: 0.0000 (left: 1:01:32)\n",
      "TestLoss: 0.8800 MAE: 0.7390 RMSE: 0.9381\n",
      "ValLoss: 0.8854 MAE: 0.7348 RMSE: 0.9409\n",
      "Epoch 39: TrainLoss 0.8635 RecLoss: 0.0000 (left: 1:00:32)\n",
      "TestLoss: 0.9010 MAE: 0.7534 RMSE: 0.9492\n",
      "ValLoss: 0.8861 MAE: 0.7436 RMSE: 0.9413\n",
      "Epoch 40: TrainLoss 0.8644 RecLoss: 0.0000 (left: 0:59:34)\n",
      "TestLoss: 0.8887 MAE: 0.7433 RMSE: 0.9427\n",
      "ValLoss: 0.8812 MAE: 0.7351 RMSE: 0.9387\n",
      "Epoch 41: TrainLoss 0.8640 RecLoss: 0.0000 (left: 0:58:32)\n",
      "TestLoss: 0.8910 MAE: 0.7457 RMSE: 0.9439\n",
      "ValLoss: 0.8836 MAE: 0.7402 RMSE: 0.9400\n",
      "Epoch 42: TrainLoss 0.8634 RecLoss: 0.0000 (left: 0:57:32)\n",
      "TestLoss: 0.8849 MAE: 0.7435 RMSE: 0.9407\n",
      "ValLoss: 0.8853 MAE: 0.7387 RMSE: 0.9409\n",
      "Epoch 43: TrainLoss 0.8637 RecLoss: 0.0000 (left: 0:56:36)\n",
      "TestLoss: 0.8801 MAE: 0.7393 RMSE: 0.9381\n",
      "ValLoss: 0.8798 MAE: 0.7365 RMSE: 0.9380\n",
      "Epoch 44: TrainLoss 0.8624 RecLoss: 0.0000 (left: 0:55:39)\n",
      "TestLoss: 0.8879 MAE: 0.7390 RMSE: 0.9423\n",
      "ValLoss: 0.8825 MAE: 0.7329 RMSE: 0.9394\n",
      "Epoch 45: TrainLoss 0.8620 RecLoss: 0.0000 (left: 0:54:39)\n",
      "TestLoss: 0.8852 MAE: 0.7426 RMSE: 0.9409\n",
      "ValLoss: 0.8831 MAE: 0.7380 RMSE: 0.9397\n",
      "Epoch 46: TrainLoss 0.8627 RecLoss: 0.0000 (left: 0:53:42)\n",
      "TestLoss: 0.8869 MAE: 0.7431 RMSE: 0.9418\n",
      "ValLoss: 0.8832 MAE: 0.7400 RMSE: 0.9398\n",
      "Epoch 47: TrainLoss 0.8618 RecLoss: 0.0000 (left: 0:53:01)\n",
      "TestLoss: 0.8833 MAE: 0.7417 RMSE: 0.9399\n",
      "ValLoss: 0.8819 MAE: 0.7385 RMSE: 0.9391\n",
      "Epoch 48: TrainLoss 0.8614 RecLoss: 0.0000 (left: 0:51:58)\n",
      "TestLoss: 0.8868 MAE: 0.7440 RMSE: 0.9417\n",
      "ValLoss: 0.8853 MAE: 0.7392 RMSE: 0.9409\n",
      "Epoch 49: TrainLoss 0.8617 RecLoss: 0.0000 (left: 0:50:56)\n",
      "TestLoss: 0.8895 MAE: 0.7441 RMSE: 0.9431\n",
      "ValLoss: 0.8853 MAE: 0.7395 RMSE: 0.9409\n",
      "Epoch 50: TrainLoss 0.8605 RecLoss: 0.0000 (left: 0:49:55)\n",
      "TestLoss: 0.8817 MAE: 0.7370 RMSE: 0.9390\n",
      "ValLoss: 0.8817 MAE: 0.7331 RMSE: 0.9390\n",
      "Epoch 51: TrainLoss 0.8612 RecLoss: 0.0000 (left: 0:48:52)\n",
      "TestLoss: 0.8788 MAE: 0.7354 RMSE: 0.9374\n",
      "ValLoss: 0.8839 MAE: 0.7346 RMSE: 0.9401\n",
      "Epoch 52: TrainLoss 0.8604 RecLoss: 0.0000 (left: 0:47:50)\n",
      "TestLoss: 0.8836 MAE: 0.7403 RMSE: 0.9400\n",
      "ValLoss: 0.8816 MAE: 0.7374 RMSE: 0.9389\n",
      "Epoch 53: TrainLoss 0.8594 RecLoss: 0.0000 (left: 0:46:49)\n",
      "TestLoss: 0.8779 MAE: 0.7366 RMSE: 0.9370\n",
      "ValLoss: 0.8800 MAE: 0.7332 RMSE: 0.9381\n",
      "Epoch 54: TrainLoss 0.8607 RecLoss: 0.0000 (left: 0:45:49)\n",
      "TestLoss: 0.8785 MAE: 0.7371 RMSE: 0.9373\n",
      "ValLoss: 0.8825 MAE: 0.7357 RMSE: 0.9394\n",
      "Epoch 55: TrainLoss 0.8602 RecLoss: 0.0000 (left: 0:44:49)\n",
      "TestLoss: 0.8816 MAE: 0.7385 RMSE: 0.9389\n",
      "ValLoss: 0.8824 MAE: 0.7340 RMSE: 0.9393\n",
      "Epoch 56: TrainLoss 0.8593 RecLoss: 0.0000 (left: 0:43:47)\n",
      "TestLoss: 0.8816 MAE: 0.7374 RMSE: 0.9390\n",
      "ValLoss: 0.8829 MAE: 0.7341 RMSE: 0.9396\n",
      "Epoch 57: TrainLoss 0.8593 RecLoss: 0.0000 (left: 0:42:47)\n",
      "TestLoss: 0.8815 MAE: 0.7416 RMSE: 0.9389\n",
      "ValLoss: 0.8793 MAE: 0.7365 RMSE: 0.9377\n",
      "Epoch 58: TrainLoss 0.8587 RecLoss: 0.0000 (left: 0:41:46)\n",
      "TestLoss: 0.8811 MAE: 0.7405 RMSE: 0.9387\n",
      "ValLoss: 0.8771 MAE: 0.7348 RMSE: 0.9365\n",
      "Epoch 59: TrainLoss 0.8582 RecLoss: 0.0000 (left: 0:40:43)\n",
      "TestLoss: 0.8850 MAE: 0.7391 RMSE: 0.9407\n",
      "ValLoss: 0.8795 MAE: 0.7335 RMSE: 0.9378\n",
      "Epoch 60: TrainLoss 0.8581 RecLoss: 0.0000 (left: 0:39:43)\n",
      "TestLoss: 0.8806 MAE: 0.7367 RMSE: 0.9384\n",
      "ValLoss: 0.8828 MAE: 0.7337 RMSE: 0.9396\n",
      "Epoch 61: TrainLoss 0.8581 RecLoss: 0.0000 (left: 0:38:43)\n",
      "TestLoss: 0.8922 MAE: 0.7496 RMSE: 0.9446\n",
      "ValLoss: 0.8848 MAE: 0.7423 RMSE: 0.9407\n",
      "Epoch 62: TrainLoss 0.8574 RecLoss: 0.0000 (left: 0:37:42)\n",
      "TestLoss: 0.8876 MAE: 0.7383 RMSE: 0.9421\n",
      "ValLoss: 0.8858 MAE: 0.7323 RMSE: 0.9412\n",
      "Epoch 63: TrainLoss 0.8571 RecLoss: 0.0000 (left: 0:36:42)\n",
      "TestLoss: 0.8761 MAE: 0.7369 RMSE: 0.9360\n",
      "ValLoss: 0.8789 MAE: 0.7371 RMSE: 0.9375\n",
      "Epoch 64: TrainLoss 0.8570 RecLoss: 0.0000 (left: 0:35:41)\n",
      "TestLoss: 0.8849 MAE: 0.7427 RMSE: 0.9407\n",
      "ValLoss: 0.8811 MAE: 0.7380 RMSE: 0.9387\n",
      "Epoch 65: TrainLoss 0.8570 RecLoss: 0.0000 (left: 0:34:40)\n",
      "TestLoss: 0.8806 MAE: 0.7397 RMSE: 0.9384\n",
      "ValLoss: 0.8820 MAE: 0.7343 RMSE: 0.9392\n",
      "Epoch 66: TrainLoss 0.8580 RecLoss: 0.0000 (left: 0:33:39)\n",
      "TestLoss: 0.8901 MAE: 0.7468 RMSE: 0.9434\n",
      "ValLoss: 0.8835 MAE: 0.7414 RMSE: 0.9399\n",
      "Epoch 67: TrainLoss 0.8566 RecLoss: 0.0000 (left: 0:32:38)\n",
      "TestLoss: 0.8878 MAE: 0.7434 RMSE: 0.9422\n",
      "ValLoss: 0.8786 MAE: 0.7376 RMSE: 0.9373\n",
      "Epoch 68: TrainLoss 0.8563 RecLoss: 0.0000 (left: 0:31:38)\n",
      "TestLoss: 0.8820 MAE: 0.7417 RMSE: 0.9391\n",
      "ValLoss: 0.8790 MAE: 0.7364 RMSE: 0.9376\n",
      "Epoch 69: TrainLoss 0.8564 RecLoss: 0.0000 (left: 0:30:38)\n",
      "TestLoss: 0.8894 MAE: 0.7499 RMSE: 0.9431\n",
      "ValLoss: 0.8829 MAE: 0.7441 RMSE: 0.9396\n",
      "Epoch 70: TrainLoss 0.8561 RecLoss: 0.0000 (left: 0:29:38)\n",
      "TestLoss: 0.8825 MAE: 0.7420 RMSE: 0.9394\n",
      "ValLoss: 0.8789 MAE: 0.7367 RMSE: 0.9375\n",
      "Epoch 71: TrainLoss 0.8553 RecLoss: 0.0000 (left: 0:28:38)\n",
      "TestLoss: 0.8758 MAE: 0.7350 RMSE: 0.9358\n",
      "ValLoss: 0.8797 MAE: 0.7329 RMSE: 0.9379\n",
      "Epoch 72: TrainLoss 0.8558 RecLoss: 0.0000 (left: 0:27:38)\n",
      "TestLoss: 0.8783 MAE: 0.7340 RMSE: 0.9372\n",
      "ValLoss: 0.8811 MAE: 0.7323 RMSE: 0.9387\n",
      "Epoch 73: TrainLoss 0.8551 RecLoss: 0.0000 (left: 0:26:38)\n",
      "TestLoss: 0.8778 MAE: 0.7381 RMSE: 0.9369\n",
      "ValLoss: 0.8797 MAE: 0.7357 RMSE: 0.9379\n",
      "Epoch 74: TrainLoss 0.8545 RecLoss: 0.0000 (left: 0:25:37)\n",
      "TestLoss: 0.8829 MAE: 0.7331 RMSE: 0.9396\n",
      "ValLoss: 0.8869 MAE: 0.7315 RMSE: 0.9417\n",
      "Epoch 75: TrainLoss 0.8547 RecLoss: 0.0000 (left: 0:24:37)\n",
      "TestLoss: 0.8799 MAE: 0.7411 RMSE: 0.9380\n",
      "ValLoss: 0.8789 MAE: 0.7383 RMSE: 0.9375\n",
      "Epoch 76: TrainLoss 0.8550 RecLoss: 0.0000 (left: 0:23:37)\n",
      "TestLoss: 0.8783 MAE: 0.7384 RMSE: 0.9372\n",
      "ValLoss: 0.8801 MAE: 0.7339 RMSE: 0.9381\n",
      "Epoch 77: TrainLoss 0.8540 RecLoss: 0.0000 (left: 0:22:38)\n",
      "TestLoss: 0.8755 MAE: 0.7367 RMSE: 0.9357\n",
      "ValLoss: 0.8780 MAE: 0.7330 RMSE: 0.9370\n",
      "Epoch 78: TrainLoss 0.8539 RecLoss: 0.0000 (left: 0:21:39)\n",
      "TestLoss: 0.8781 MAE: 0.7375 RMSE: 0.9370\n",
      "ValLoss: 0.8768 MAE: 0.7330 RMSE: 0.9364\n",
      "Epoch 79: TrainLoss 0.8545 RecLoss: 0.0000 (left: 0:20:39)\n",
      "TestLoss: 0.8800 MAE: 0.7375 RMSE: 0.9381\n",
      "ValLoss: 0.8801 MAE: 0.7358 RMSE: 0.9381\n",
      "Epoch 80: TrainLoss 0.8535 RecLoss: 0.0000 (left: 0:19:39)\n",
      "TestLoss: 0.8754 MAE: 0.7371 RMSE: 0.9356\n",
      "ValLoss: 0.8786 MAE: 0.7349 RMSE: 0.9373\n",
      "Epoch 81: TrainLoss 0.8535 RecLoss: 0.0000 (left: 0:18:39)\n",
      "TestLoss: 0.8749 MAE: 0.7356 RMSE: 0.9354\n",
      "ValLoss: 0.8790 MAE: 0.7343 RMSE: 0.9375\n",
      "Epoch 82: TrainLoss 0.8527 RecLoss: 0.0000 (left: 0:17:39)\n",
      "TestLoss: 0.8829 MAE: 0.7420 RMSE: 0.9396\n",
      "ValLoss: 0.8801 MAE: 0.7401 RMSE: 0.9381\n",
      "Epoch 83: TrainLoss 0.8526 RecLoss: 0.0000 (left: 0:16:40)\n",
      "TestLoss: 0.8697 MAE: 0.7326 RMSE: 0.9326\n",
      "ValLoss: 0.8768 MAE: 0.7308 RMSE: 0.9364\n",
      "Epoch 84: TrainLoss 0.8526 RecLoss: 0.0000 (left: 0:15:40)\n",
      "TestLoss: 0.8794 MAE: 0.7365 RMSE: 0.9378\n",
      "ValLoss: 0.8782 MAE: 0.7326 RMSE: 0.9371\n",
      "Epoch 85: TrainLoss 0.8533 RecLoss: 0.0000 (left: 0:14:40)\n",
      "TestLoss: 0.8749 MAE: 0.7355 RMSE: 0.9354\n",
      "ValLoss: 0.8781 MAE: 0.7349 RMSE: 0.9371\n",
      "Epoch 86: TrainLoss 0.8528 RecLoss: 0.0000 (left: 0:13:41)\n",
      "TestLoss: 0.8779 MAE: 0.7354 RMSE: 0.9370\n",
      "ValLoss: 0.8799 MAE: 0.7350 RMSE: 0.9380\n",
      "Epoch 87: TrainLoss 0.8521 RecLoss: 0.0000 (left: 0:12:41)\n",
      "TestLoss: 0.8782 MAE: 0.7389 RMSE: 0.9371\n",
      "ValLoss: 0.8764 MAE: 0.7358 RMSE: 0.9362\n",
      "Epoch 88: TrainLoss 0.8517 RecLoss: 0.0000 (left: 0:11:42)\n",
      "TestLoss: 0.8744 MAE: 0.7358 RMSE: 0.9351\n",
      "ValLoss: 0.8751 MAE: 0.7332 RMSE: 0.9354\n",
      "Epoch 89: TrainLoss 0.8513 RecLoss: 0.0000 (left: 0:10:43)\n",
      "TestLoss: 0.8861 MAE: 0.7433 RMSE: 0.9413\n",
      "ValLoss: 0.8791 MAE: 0.7381 RMSE: 0.9376\n",
      "Epoch 90: TrainLoss 0.8526 RecLoss: 0.0000 (left: 0:09:44)\n",
      "TestLoss: 0.8854 MAE: 0.7425 RMSE: 0.9410\n",
      "ValLoss: 0.8782 MAE: 0.7371 RMSE: 0.9371\n",
      "Epoch 91: TrainLoss 0.8505 RecLoss: 0.0000 (left: 0:08:45)\n",
      "TestLoss: 0.8793 MAE: 0.7414 RMSE: 0.9377\n",
      "ValLoss: 0.8788 MAE: 0.7378 RMSE: 0.9374\n",
      "Epoch 92: TrainLoss 0.8504 RecLoss: 0.0000 (left: 0:07:46)\n",
      "TestLoss: 0.8760 MAE: 0.7415 RMSE: 0.9359\n",
      "ValLoss: 0.8798 MAE: 0.7371 RMSE: 0.9380\n",
      "Epoch 93: TrainLoss 0.8508 RecLoss: 0.0000 (left: 0:06:48)\n",
      "TestLoss: 0.8764 MAE: 0.7363 RMSE: 0.9361\n",
      "ValLoss: 0.8756 MAE: 0.7326 RMSE: 0.9357\n",
      "Epoch 94: TrainLoss 0.8509 RecLoss: 0.0000 (left: 0:05:49)\n",
      "TestLoss: 0.8721 MAE: 0.7343 RMSE: 0.9339\n",
      "ValLoss: 0.8773 MAE: 0.7333 RMSE: 0.9366\n",
      "Epoch 95: TrainLoss 0.8508 RecLoss: 0.0000 (left: 0:04:51)\n",
      "TestLoss: 0.8801 MAE: 0.7407 RMSE: 0.9382\n",
      "ValLoss: 0.8756 MAE: 0.7372 RMSE: 0.9357\n",
      "Epoch 96: TrainLoss 0.8511 RecLoss: 0.0000 (left: 0:03:52)\n",
      "TestLoss: 0.8798 MAE: 0.7361 RMSE: 0.9380\n",
      "ValLoss: 0.8764 MAE: 0.7321 RMSE: 0.9362\n",
      "Epoch 97: TrainLoss 0.8499 RecLoss: 0.0000 (left: 0:02:54)\n",
      "TestLoss: 0.8761 MAE: 0.7364 RMSE: 0.9360\n",
      "ValLoss: 0.8775 MAE: 0.7352 RMSE: 0.9368\n",
      "Epoch 98: TrainLoss 0.8506 RecLoss: 0.0000 (left: 0:01:56)\n",
      "TestLoss: 0.8844 MAE: 0.7450 RMSE: 0.9404\n",
      "ValLoss: 0.8804 MAE: 0.7411 RMSE: 0.9383\n",
      "Epoch 99: TrainLoss 0.8502 RecLoss: 0.0000 (left: 0:00:58)\n",
      "TestLoss: 0.8873 MAE: 0.7483 RMSE: 0.9420\n",
      "ValLoss: 0.8857 MAE: 0.7465 RMSE: 0.9411\n",
      "Extra : False\n",
      "-------Dataset Info--------\n",
      "split way [threshold] with threshold 30 training_ratio 1.0\n",
      "train set size: support/query 708509/266560\n",
      "test set size: support/query 3138/2093\n",
      "USER HIS DICT: 6040\n",
      "NUM IS: 6040\n",
      "Key Test Result: MAE: 0.6829 RMSE: 0.8683 NDCG: 0.0000\n",
      "CORE IS SELECTED:\n",
      "USER HIS DICT: 6040\n",
      "NUM IS: 6040\n",
      "Que Test Result: MAE: 0.7361 RMSE: 0.9360 NDCG: 0.0000\n",
      "All Test Result: MAE: 0.7042 RMSE: 0.8960 NDCG: 0.0000\n"
     ]
    }
   ],
   "source": [
    "!python pretrain-1m.py\n",
    "!python train-1m.py\n",
    "!python test-1m.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_axis = [10, 20, 40, 60]\n",
    "y_axis = [0.9310,0.9122,0.8853,0.8966]\n",
    "x1_axis = [10, 20, 40, 60]\n",
    "y1_axis = [0.8959, 0.8959, 0.8959, 0.8959]\n",
    "plt.plot(x1_axis, y1_axis, label=\"base line(100% support)\")\n",
    "plt.plot(x_axis, y_axis, label=\"% of core users\")\n",
    "plt.title('RMSE VS % of core user in support')\n",
    "plt.xlabel('% of core users in support')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABA6klEQVR4nO3deXwURf7/8dcbAgTkUpGIgoIaUQQECUFElEMET1zXCy8EBVHwvvBaXXdRvH4Kisu6yqKogLeofEVEgqIohyDKoSJnBFfkjtzw+f3RnTgMk2NChpDk83w85jHdXdXdVRmYz1R1d5XMDOecc66gyhV3AZxzzpUsHjicc87FxQOHc865uHjgcM45FxcPHM455+LigcM551xcPHA4lw9J/5T0u6Rfi7ssxU1SW0k/FHc5XPHywFEGSFosaZOkLEm/ShouqWpE+nBJJuncqP2eDrdfFa5XlPSkpMzwWIskPZXLebJfz8YoT7cwr6K2J0n6TdLZ4fo94TmywnOOzqOOT0taI2mKpEMjtl8maVAh/mzZ+9cDbgMamdnBhT1OaWFmn5tZw+IuRyKF/zZPK+5y7Ms8cJQd55hZVaAZ0By4Oyr9R6B79oqkJOBC4OeIPHcDaUA6UA1oD8yMdZ6IV78YZXkHqAmcGrW9C2DAR5K6A1cAp4XlTgMmxKqYpHSgBXAwMDm7bpJqALcDf4u1XwEdDqwys9/24Bj5Cv/e+5R9sUyJVNbquyc8cJQxZvYrMI4ggER6H2gjaf9wvQswG4jsnmkJvGNmyy2w2MxeLkQZNgOvA1dGJV0JvGpm28NzjTOzn7PLbWbP53LIBsBkM9tCEFyOCLcPAB43s3V5lUdSDUkvS1opaYmk+ySVC391jgcOCVs9w3PZv6ukWZLWS/pZUpdw+yGSxkhaLWmBpF4R+zwo6U1Jr0haD1wVluNFSSsk/RJ2kZXP5ZzDJf0zYr2dpMyI9bvCY2yQ9IOkjuH2cpL6h+VcJel1SQeEafXDFubVkpYCn8Y4b/R5Fku6XdJsSeskjZaUnEuZj5I0Kcz3e3YLMuK8SRF5MyRdEy5fJekLSc+E+87Prk9E3kckTQ3T38uuU5h+rqQ5ktaGeY+NKv9dkmYDf0gaCRwGvB9+5nfGqktZ54GjjJFUFzgDWBCVtBkYA1wSrl8JRAeFr4BbJV0vqUl0V1OcXgIukFQ5LFcN4JyIc34FXCnpDklpuX2BhuYAbcNjdQTmSEoDGprZawUoyzNADYKAcypB3XuY2ScEf6vlYevpqugdw9bOy8AdBK2oU4DFYfJIIBM4BLgAeDjyCw/oCrwZ7vdq+DfZDhxF0Co8HbimAOWPLlNDoB/Q0syqAZ0jynQjcF5Yz0OANcCQqEOcChwb7lcQFxH80GgANAWuyiXfP4CPgf2BugR/94JqBSwEagEPAG9HBgeCz6wnQZ22A4MBJB1N8DncDBwEjCUIChUj9u0GnAXUNLNuwFL+bDk/FkcZyw4z81cpfxF8aWQBGwi6giYQ/CfJTh8O/BM4GZhC8CX6P6AyQdfPVWG+8kBf4AtgC7Ac6B7jPGsjXr3yKNdPwKXhci/g26j0y4BPgD+AVUD/PI51C/AtMJrgy+ULgi+/G4HPCL6Ya8bYr3xYl0YR264FMsLldkBmHuf9N/BUjO31gB1AtYhtjwDDw+UHgc8i0lLCclSO2NYNmJjLeYcD/4xYzyknQeD5DTgNqBC13zygY8R6HWAbkATUD/99HJFHfXf5e4Sf+eUR648BQ3PZ92XgeaBu1Pbs8yZFbMsArgmXrwr/rSkifSpwRUTegRFpjYCt4Wd7P/B6RFo54BegXUT5e8b4/3Jacf+/3Zdf3uIoO86z4NdnO+AYgi/XXZjZZIJfZfcBH5jZpqj0HWY2xMzaEPxKHgAMi2z6h+epGfH6Tx5lepk/u6uuIPjFHXm+V83stPBcfYCHJMX8FWxmT5nZ8WZ2MXAx8DnBl0RvglbIPKB/jF1rARWBJRHblgCHxsgbSz12vQ6U7RBgtZltyOO4yyKWDwcqACvCLpW1BEGpdgHLkcPMFhD8wn4Q+E3SKEmHRJznnYhzzCMIcCm5lKsgIrszNwJVc8l3JyBgath11DOOc/xi4bd6aAnB3zjbsqi0CgSf7SFEfLZmtjPMm9vn4ArAA0cZY2aTCH6tPpFLllcI7iLK89qFmW0ysyEEXR2NClmcl4GOkloDJwIxu5XMbJuZvUFwzaVxXgeUlELQYngozDvbzLYB0wi6UaL9TvCL+/CIbYcR/CotiGXAkTG2LwcOkFQtj+NGfhEuI2hx1IoIutXN7LhczvsHUCVifZc7vszsNTM7maBeBjwacZ4zooJ7spnlVq4iY8F1ql5mdgjBZ/ScpKPCupBXfYBDo7pGDyP4G2erF5W2jeCzXU7EZxseox65fw6x1l0UDxxl09NAJ0nNYqQNBjoRdO/sQtLN4cXRygpune1OcHdV9J1VBWJmSwi6wkYC4y24cJ99rqsknSWpWnhB9wzgOODrfA77/4AHzGwjsAhoqeDW43YEfeTRZdhBcKF+QHiuw4FbCQJoQbwI9JDUMSznoZKOMbNlwJfAI5KSJTUFriboMov1t1hB0P//pKTq4bGOlBR951m2WcCZkg6QdDBBCwMIrnFI6iCpEsG1q00ErQqAoWFdDw/zHiSpawHrukckXRheY4PgB4cBO8xsJcEX+eWSyoctkehgXBu4UVIFSRcSdEOOjUi/XFIjSVUIfjS8GfHZnhV+PhUIfhRtIfhscvM//rzBwsXggaMMCv+jvkzQ/xudttrMJkR1C2TbBDxJ0DXxO8H1jr+aWeQXcvbdKNmvd/IpzksEvwijWzjrgXsILlSuJeg7vy7sTotJUnuC6xjvhHWZCnxI8Cu7PTAwl11vIPjVu5AgkL0GDMun3EScowfwFLAOmMSfv3C7EfTfLye4BfkBMxufx+GuJOg2m0vwxfomwTWIWEYQXNNZTBBwIp9xqURQ198JPqvaBH9LgEEEN0F8LGkDwU0IrQpS1yLQEvhaUlZYhpvMbFGY1ovgBoNVBD8Qor/YvwZSCeo0ALjAzFZFpI8gaEn/CiQTXNvCzH4ALie4EP87wQ0Y55jZ1jzK+QhwX9idd3vhqlq6Kfb3g3PO7RsUPIB6Tdj1Fis9A3jFzF7Ym+Uqy7zF4ZxzLi4eOJxzzsUlYYFD0jAF4w59n0u6JA1W8ETtbEknRKR1UfC06wJJ/SO2Px4+NTpb0juSaiaq/M65fYOZDc+tmypMb+fdVHtXIlscwwmeJs3NGQQXu1IJ7rX/F0D4hPCQML0R0E1S9u2e44HGZtaUYGyl6PGWnHPOJVjCBvUys88k1c8jS1fg5fDuna8k1ZRUh+AulAXZd+pIGhXmnWtmH0fs/xXBMA75qlWrltWvn1dRcvfHH3+w3377FWrfksrrXDZ4ncuGPanzjBkzfjezg6K3F+dokIey6xObmeG2WNtj3S7Yk11vQdyFpN4ELRlSUlJ44oncnnfLW1ZWFlWr5vYgbOnkdS4bvM5lw57UuX379ktibS/OwBFrgDzLY/ufO0r3EgxkFvNhKgALRlJ9HiAtLc3atWtXqEJmZGRQ2H1LKq9z2eB1LhsSUefiDByZ7DpMQF2CB6Uq5rIdgPBp5bMJBmrzh1Ccc24vK87bcccQDJstSScC68JhF6YBqZIahEMfXxLmRcE8B3cB54ZDSjjnnNvLEtbiUDAhSjugloKJXx4gGLESMxtKMM7MmQTzQmwkGLYBM9suqR/BZEPlgWFmNic87LMEwymMD8c7+8rM+iSqDs7tbdu2bSMzM5PNmzcn/Fw1atRg3rx5CT/PvsTrHFtycjJ169alQoUKBTpmIu+q6pZPuhGMdRQrbSy7DmCWvf2ooimdc/umzMxMqlWrRv369dmzebLyt2HDBqpVq5Z/xlLE67w7M2PVqlVkZmbSoEGDAh3Tnxx3bh+yefNmDjzwwIQHDeeySeLAAw+Mq5XrgcO5fYwHDbe3xftvzgNHXhZO4tDM92HHtuIuiXPO7TM8cORl3hhSF7wAz7WGH8eB3/3rSrnFixfTuHGekywm5FzTp0/nxhtvLPSxzIwOHTqwfv16AHr27Ent2rV3q8vq1avp1KkTqampdOrUiTVr1uSkPfLIIxx11FE0bNiQcePGAbBlyxa6dOlC48aNee6553Ly9u7dm5kzCzV/2V6zePFiXnvtz0k1v/vuO6666qoiObYHjryc+QSzm9wPGLx2EbxyPvxWtu7IcG5vSEtLY/DgwYXef+zYsRx//PFUr14dgKuuuoqPPvpot3xPPfUUHTt25KeffqJjx44MHBjM7TV37lxGjRrFnDlz+Oijj7j++uvZsWMH48aNo0WLFsyePZvnn38egG+//ZadO3fSvHnzQpc30bZv375b4GjSpAmZmZksXbp0j4/vgSMvEqsPTIPrpkCXgfDLDPhXG/jwNvhjVf77O1cCbd++ne7du9O0aVMuuOACNm4MHpl66KGHaNmyJY0bN6Z3795kP387ePBgGjVqRNOmTbnkkkuAYHyknj170rJlS5o3b857772X5zkzMjI4++yzAXjwwQfp2bMn7dq144gjjtgloLzyyiukp6fTrFkzrr32WnbsCGbEffXVV+na9c8ZcE855RQOOOCA3c7z4Ycf0r17dwC6d+/Ou+++C8B7773HJZdcQqVKlWjQoAFHHXUUU6dOpUKFCmzatInt27fnHOP+++/noYceyrUukyZNolmzZjRr1ozmzZuzYcOGXeoH0K9fP4YPHw5A/fr1ueuuu0hPTyc9PZ0FCxYAQfDr06cPbdu25eijj+aDDz4AghsoevToQZMmTWjevDkTJ04EYPjw4Vx44YWcc845nH766fTv35/PP/+cNm3a8NRTTwFwzjnnMGrUqDw/i4IozifHS46kinDiddD0YsgYCNNegNlvwKl3QnrvIN25Ivb39+cwd/n6Ij1mo0Oq88A5x+WZ54cffuDFF1+kTZs29OzZk+eee47bb7+dfv368be//Q2AK664gg8++IBzzjmHgQMHsmjRIipVqsTatWsBGDBgAB06dGDYsGGsXbuW9PR0TjvttAIPtjd//nwmTpzIhg0baNiwIddddx0LFixg9OjRfPHFF1SoUIHrr7+eV199lSuvvJIvvviCf//73/ked+XKldSpE8zGW6dOHX777TcAfvnlF0488cScfHXr1uWXX37hvPPOY8SIEbRq1Yo777yTMWPG0KJFCw455JBcz/HEE08wZMgQ2rRpQ1ZWFsnJyfmWq3r16kydOpWXX36Zm2++OSdILF68mEmTJvHzzz/Tvn17FixYwJAhQ4Cg62n+/Pmcfvrp/PjjjwBMmTKF2bNnc8ABB5CRkcETTzzByJEjc27HTUtLY+DAgdx55535likv3uKIR5UD4MzH4PopUC8dPr4XnmsF8z/06x+u1KhXrx5t2rQB4PLLL2fy5GCa94kTJ9KqVSuaNGnCp59+ypw5wXO5TZs25bLLLuOVV14hKSn4Lfrxxx8zcOBAmjVrRrt27di8eXNcXSRnnXUWlSpVolatWtSuXZv//e9/TJgwgRkzZtCyZUuaNWvGhAkTWLgwmO5+9erVe/R8RqzRiySRlJTEa6+9xsyZM7nwwgt5+umnue2227j11lu54IILGDNmzG77tWnThltvvZXBgwezdu3anL9JXrp165bzPmXKlJztF110EeXKlSM1NZUjjjiC+fPnM3nyZK644goAjjnmGA4//PCcwNGpU6eYLa1stWvXZvny5bmmF5S3OArjoIZw+Zvw03gYdy+MuhQanAKdH4GD986FRVf65dcySJToWzMlsXnzZq6//nqmT59OvXr1ePDBB3Pu+//www/57LPPGDNmDP/4xz+YM2cOZsZbb71Fw4YNC1WGSpUq5SyXL1+e7du3Y2Z0796dRx55ZLf8SUlJ7Ny5k3Ll8v4tfNBBB7FixQrq1KnDihUrqF27NhC0MJYt+3NQ7szMzN1aFc899xzdu3dnypQpVKxYkdGjR9O6dWvOPffcXfL179+fs846i7Fjx3LiiSfyySef5JQvW/QzE5F/89yWs9fzGqIvvxbd5s2bqVy5cp55CsJbHHsitRNc9wWc+QT8+h38uy28fxNkrSzukjlXaEuXLs351Tty5EhOPvnknC+6WrVqkZWVxZtvvgnAzp07WbZsGe3bt+exxx5j7dq1ZGVl0blzZ5555pmcL7miuAOpY8eOvPnmmzndS6tXr2bJkmDU74YNG+a0PvJy5pln8tJLLwHw0ksv5VwXOffccxk1ahRbtmxh0aJF/PTTT6Snp+fst2bNGj744AOuvPJKNm7cSLly5XICarSff/6ZJk2acNddd5GWlsb8+fM5/PDDmTt3Llu2bGHdunVMmDBhl31Gjx6d8966deuc7W+88QY7d+7k559/ZuHChTRs2JBTTjmFV18NBgb/8ccfWbp0acwAXa1aNTZs2LDLth9//LFI7przwLGnyleA9F5w40xo1QdmvgKDm8Pkp2H7luIunXNxO/bYY3nppZdo2rQpq1ev5rrrrqNmzZr06tWLJk2acN5559GyZUsAduzYweWXX55zofaWW26hZs2a3H///Wzbto2mTZvSuHFj7r///j0uV6NGjfjnP//J6aefTtOmTenUqRMrVqwAgq6tjIyMnLzdunWjdevW/PDDD9StW5cXX3wRgFtuuYXx48eTmprK+PHj6d8/mJn6uOOO46KLLqJRo0Z06dKFIUOGUL58+ZzjPfTQQ9x3331IonPnzkyfPp0mTZrQq1ev3cr59NNP07hxY44//ngqV67MGWecQb169bjoootyuvWi78jasmULrVq1YtCgQTkXsiEIiKeeeipnnHEGQ4cOJTk5OeeOryZNmnDxxRczfPjwXVpo2Zo2bUpSUhInnXRSzjEnTpzIWWedVchPIIKZlfpXixYtrLAmTpwY3w4rfzR79WKzB6qbPd3UbM57Zjt3Fvr8xSHuOpcC+0qd586du9fOtX79+r12rkRbvny5nXbaafnm2xfrfPjhh9vKlSt32969e3d744039vj42XXevHmztWrVyrZt2xYzX6x/e8B0i/Gd6i2OolYrFS4dBVe8AxWqwOtXwPCzYcW3xV0y50qtOnXq0KtXr5wHAN3uli5dysCBAwt0sT4/fnE8UY7sANd+Dt+8BBMHwL9PheaXQYf7odrBxV0650qdiy66qLiLUCiLFy+OuT37OY+ikpqaSmpqapEcy1sciVQ+CVpeDTd8Ayf1g29HwzMt4PMnYVvi51twzrlE8MCxN1SuCaf/E/p+DUe0gwkPwbMt4fu3/fkP51yJ44FjbzrwSLjkVej+PiTXgDd7wLAu8Ms3xV0y55wrsIQFDknDJP0m6ftc0iVpsKQFkmZLOiEirYukH8K0/hHbD5A0XtJP4fv+iSp/QjU4Ba6dBOcMhtU/w3/awzt9YP2eP9HpnHOJlsgWx3CgSx7pZwCp4as38C8ASeWBIWF6I6CbpEbhPv2BCWaWCkwI10umcuWhRffg+sfJt8D3bwXXPyY9Bls3FnfpXBm1cuVKTj75ZBo3bpwzACBA165d4x6qYuXKlbRq1YrmzZvz+eefF3FJXXFKWOAws8+A1Xlk6Qq8HN4u/BVQU1IdIB1YYGYLzWwrMCrMm73PS+HyS8B5CSn83pRcHU57EPpODZ5EnzgAnk0LBlH06x9uLxs5cmTOsBqPP/44AO+//z4nnHBCngP7xTJhwgSOOeYYZs6cSdu2bYukfJGj1O5txXnufU1x3o57KLAsYj0z3BZre6twOcXMVgCY2QpJtXM7uKTeBC0ZUlJSdnmqNB5ZWVmF3jdutXtSo2IrjlrwItXevoZ1nzzOz0dezfoahRvvp7D2ap33EftKnWvUqLHbMBGJsmPHjt3OtWPHDtasWcOqVaswM9asWcOTTz7J6NGjcy3X0qVL6du3L7///ju1atXiueeeY82aNdxxxx1s2rSJpk2b8sknn+wyRtKMGTO466672LhxIxUrVuT999+nQoUK3HLLLcycOZOkpCQefvjhnOE1xo0bx+bNm9m4cSOjR4/mjjvuYM6cOezYsYO77757t6ehP//8cwYPHswbb7wBwG233cYJJ5zAJZdcwq233srYsWNJSkqiQ4cODBgwgN9//52bb745Z7yqRx99lBNPPJGHH36YX3/9lSVLlnDggQdyxx13cN1117Ft2zZ27tzJiBEjOOqoo4ryYylysT7nWDZv3lzg/wPFGThiTXJreWyPi5k9DzwPkJaWZu3atYv3EEAwT0Bh9y2cdrDzOvh2JDUm/J0TZt4JTS4MWiU16u6VEuz9Ohe/faXO8+bN+3OU1//rH4yBVpQObgJnBJMXbdiwYbcRZXv27Mmll17K66+/zqOPPsqIESPo0aMHKSkpuR6yf//+9OjRg+7duzNs2DDuuece3n33Xf7xj38wffp0nn322V3yb926lZ49ezJ69GhatmzJ+vXrqVKlCoMGDaJChQrMmTNnl+HCk5OTmTZtWs5w4ffccw+dO3dmxIgROUO2n3POObsM8FelShWSkpJy6lexYkWSk5NZt24dH374IfPnz0cSa9eupVq1alx77bXccccdnHzyySxdupTOnTszb948KlWqxOzZs5k8eTKVK1fmhhtu4NZbb+Wyyy5j69at7Nixo0gGDUykWJ9zLMnJyQWenKo4A0cmUC9ivS6wHKiYy3aA/0mqE7Y26gC/7ZWS7m3lygUPCzbqCpOfginPwrwPoM2N0OYmqFiwOQ2ci1eNGjX48MMPgWBgv0cffZS3336bXr16sWbNGm677bZdBuGDYA6It99+Gwjm6chvrocffviBOnXq5Ix3lT1r3+TJk7nhhhuAvIcL//jjjxkzZgxPPPEEQM6Q7ccee2y+9atevTrJyclcc801nHXWWTmTK33yySfMnTs3J9/69etzfqWfe+65OcGhdevWDBgwgMzMTM4///wie6CupCnOwDEG6CdpFEFX1LowIKwEUiU1AH4BLgEujdinOzAwfM97WrGSrlJV6Hh/cBH9kwdh0qPwzcvQ8YFgUql8hpB2JVzYMiguDz30EPfeey8jR46kRYsWXHrppXTt2jVnxrncRA8FHs3MYuaxAg4XbgUYsj23YcyTkpKYOnUqEyZMYNSoUTz77LN8+umn7Ny5kylTpsRsPUSe+9JLL6VVq1Z8+OGHdO7cmRdeeIEOHTrkWd/SKJG3444EpgANJWVKulpSH0l9wixjgYXAAuA/wPUAZrYd6AeMA+YBr5vZnHCfgUAnST8BncL10q/mYXDBMOg5DqrVgXf7wAsdYOlXxV0yV0r99NNPLF++nFNPPTXfYcRPOumknOlIX331VU4++eQ8j33MMcewfPlypk2bBgRdKdu3by/wcOEFGbI9t2HMs7KyWLduHWeeeSZPP/00s2bNAuD000/fpUste3u0hQsXcsQRR3DjjTdy7rnnMnv27DzrWlolrMVhZt3ySTegby5pYwkCS/T2VUDHIilgSXTYiXDNBPjujaAFMqwzHPcXOO3vsP/hxV06V4rce++9DBgwAAiGKD/vvPMYNGhQzLm2Bw8eTM+ePXn88cc56KCD+O9//5vnsbMnQbrhhhvYtGkTlStX5pNPPuH666+nT58+NGnShKSkpFyHC7///vu5+eabadq0KWZG/fr1c6ZazRY5jHlqampO331WVhaXXXYZmzdvxsxyhhsfPHgwffv2pWnTpjlBbOjQobude/To0bzyyitUqFCBgw8+OGcq3bJGeTUPS4u0tDSbPn16ofbdVy6a7mbrH/DFYPhiENjOYCysk2+BSoWfPjPbPlvnBNpX6jxv3rwC9dUXhYJeNC1NvM65i/VvT9IMM0uLzuud5CVVxf2g/d1ww/TgIvrnTwYPEH4zAnbuKO7SOedKMQ8cJV2NuvDX/wRdWDUPgzH94Pl2sHhycZfMOVdKeeAoLeqmwdXj4a8vwsbVMPwsGH0FrF5U3CVzcSoL3cdu3xLvvzkPHKWJBE0ugH7ToP19sOATGJIO4/8Gm31mtJIgOTk556lt5/YGM2PVqlUkJycXeB+fAbA0qlgFTr0Dml8ezP3xxSCY9Rp0uA+aXxEMsOj2SXXr1iUzM5OVK1cm/FybN2+O68uiNPA6x5acnEzdugUfmcIDR2lWvQ785V+Q3gvG3QPv3wRT/wOdH4YjTi3u0rkYKlSoQIMGDfbKuTIyMgo8xERp4XUuGt5VVRYcegL0+D+4cHjQZfXyuTDyUlj1c3GXzDlXAnngKCuk4GHBftOCIUsWTYIhrWDcvbBpbXGXzjlXgnjgKGsqJEPbW4MJpI6/BKYMgWdOgGkvwA6fb8A5lz8PHGVVtRTo+mwwhe1Bx8KHt8HQk2HBhOIumXNuH+eBo6yrczxc9QFcNAK2b4JXzqfJ7Ifgt/nFXTLn3D7KA4cLrn80OjeYvrbTQ9RYNw/+dRJ8cAtkJf62UOdcyeK347o/JVWCNjfx9R/1abPtc5g+LJj7vO0tcOL1UGHfnunMObd3eIvD7WZbxRpw1hNw/VdQ/+TgIcJnWwZBJGJyHOdc2eSBw+XuoKPh0lHQ/X2ovD+8fQ280BGWTCnukjnnilFCA4ekLpJ+kLRAUv8Y6ftLekfSbElTJTWOSLtJ0veS5ki6OWJ7M0lfSZolabqk9ETWwQENToHek+C8f8GGX+G/XWD05f4AoXNlVCKnji0PDAHOABoB3SQ1isp2DzDLzJoCVwKDwn0bA72AdOB44GxJ2bPCPwb83cyaAX8L112ilSsHzS6FG2ZA+3thwafBA4Qf3R2MxuucKzMS2eJIBxaY2UIz2wqMArpG5WkETAAws/lAfUkpwLHAV2a2MZyDfBLwl3AfA6qHyzWA5Qmsg4tWsQqceifc+A006wZfD4XBzYMHCbdvLe7SOef2gkQGjkOBZRHrmeG2SN8C5wOEXU6HA3WB74FTJB0oqQpwJlAv3Odm4HFJy4AngLsTVQGXh2oHw7nPwLWfB2NhjbsnGMJ97nvgQ4I7V6olbM5xSRcCnc3smnD9CiDdzG6IyFOdoHuqOfAdcAxwjZl9K+lqoC+QBcwFNpnZLZIGA5PM7C1JFwG9zey0GOfvDfQGSElJaTFq1KhC1SMrK4uqVasWat+SqjB1PmDVNxz583/Zb+NS1tZoxM9H9mBD9aMTVMKi559z2eB1jk/79u1jzjmOmSXkBbQGxkWs3w3cnUd+AYuB6jHSHgauD5fX8WfAE7A+v7K0aNHCCmvixImF3rekKnSdt28zmzbM7LEjzR6obvZGT7M1S4q0bInin3PZ4HWODzDdYnynJrKrahqQKqmBpIrAJcCYyAySaoZpANcAn5nZ+jCtdvh+GEF31sgw33IgezKJDsBPCayDi0f5JEjrATfOhLa3w/wP4Jk0GP8AbF5X3KVzzhWRhD05bmbbJfUDxgHlgWFmNkdSnzB9KMFF8Jcl7SDojro64hBvSToQ2Ab0NbM14fZewCBJScBmwu4otw+pVA063h8EkQkPwRdPw8xXoF1/aNEjCDDOuRIrof+DzWwsMDZq29CI5SlAavR+YVrbXLZPBloUYTFdotSoC+c/D636wMf3wdjbYerz0OkfcHTnYIws51yJ40+Ou8Q79AS46kO4+FXYuQNGXhzMQrhidnGXzDlXCB443N4hwbFnB+NfdXkUfv0O/n0KvHs9rPdHcZwrSTxwuL0rqSKc2Ce4gN66L3z3BjzTAiY+Alv/KO7SOecKwAOHKx6V94fOA4I5QFJPh0kDYfAJ8M2IoDvLObfP8sDhitcBDeCil6Dnx1CzHozpF3Rh/TyxuEvmnMuFBw63bzisFVw9Hi4YBlvWw4jz4NULfQpb5/ZBHjjcvkOCxn+FvtOg00Ow9Cufwta5fZAHDrfvqZAMbW4KLqCn9YQZLwUj8H7+JGzbVNylc67M88Dh9l371QqmsO37NTRoGzGF7es+ha1zxcgDh9v31UqFbiMjprDtFU5h+2Vxl8y5MskDhys5cqawHRpOYXsGjLrMp7B1bi/zwOFKlnLlgpkHs6ew/XmiT2Hr3F7mgcOVTDGnsG0GXz4L27cUd+mcK9U8cLiSLXsK2z6T4dAW8PG9wRS2c971KWydSxAPHK50SDkOrngHLnsLkirDG91hWBfInF7cJXOu1PHA4UqX1NOC1sfZT8Pqn4O7r968GtYuLe6SOVdqeOBwpY9PYetcQiU0cEjqIukHSQsk9Y+Rvr+kdyTNljRVUuOItJskfS9pjqSbo/a7ITzuHEmPJbIOrgTLnsL2hhlw3F+CKWwHN4ep/4Ed24u7dM6VWAkLHJLKA0OAM4BGQDdJjaKy3QPMMrOmwJXAoHDfxgRzi6cDxwNnS0oN09oDXYGmZnYc8ESi6uBKiRp14fx/Q+8MOOjYYArbf7WGHz7yC+jOFUIiWxzpwAIzW2hmW4FRBF/4kRoBEwDMbD5QX1IKcCzwlZltNLPtwCTgL+E+1wEDzWxLuN9vCayDK00OaQ5XfQCXvOZT2Dq3B2QJ+sUl6QKgi5ldE65fAbQys34ReR4Gks3sVknpwJdAK2Aj8B7QGthEEFymm9kNkmaFaV2AzcDtZjYtxvl7A70BUlJSWowaNapQ9cjKyqJq1aqF2rekKgt11s5tHLL8I+ovHk3S9iwyD2zLsqOvYmulA4u7aHtNWfico3md49O+ffsZZpYWvT1pj0uVO8XYFh2lBgKDwmDwHTAT2G5m8yQ9CowHsoBvgexO6SRgf+BEoCXwuqQjLCoCmtnzwPMAaWlp1q5du0JVIiMjg8LuW1KVnTp3gk33wWdPcOhXQ6k3fTqcdAOcdCNUKv1fLmXnc/6T17loJLKrKhOoF7FeF1gemcHM1ptZDzNrRnCN4yBgUZj2opmdYGanAKuBnyKO+7YFpgI7gVoJrIcrzcIpbKemD4GjO8OkR4M50L952aewdS4XiQwc04BUSQ0kVQQuAcZEZpBUM0wDuAb4zMzWh2m1w/fDgPOBkWG+d4EOYdrRQEXg9wTWw5UBmysfDBcOj5jC9oZwCttPi7tozu1zEhY4wova/YBxwDzgdTObI6mPpD5htmOBOZLmE9x9dVPEId6SNBd4H+hrZmvC7cOAIyR9T3DBvXt0N5VzhbbbFLZ/8SlsnYuSyGscmNlYYGzUtqERy1OA1Fz2bZvL9q3A5UVYTOd2lT2FbcOzYOq/4bMngylsW3SHdvdA1YOKu4TOFSt/cty53EROYdvyap/C1rmQBw7n8rPfgXDm4z6FrXMhDxzOFVTMKWw7+BS2rszxwOFcvHaZwvZ/PoWtK3M8cDhXGLtMYXtfOIVtOvxff5/C1pV6Hjic2xMVq8CpdwQX0JtdFtyF5VPYulLOA4dzRaFaCpw7OJzCNs2nsHWlmgcO54pSynFwxds+ha0r1TxwOJcIMaew7QlrlhR3yZzbYx44nEuU3aaw/TB4/mP833wKW1eieeBwLtF2m8J2UMQUttuKu3TOxc0Dh3N7S8wpbE/yKWxdieOBw7m9LXIKW9vpU9i6EscDh3PFQYJjzoLrv4IzHoNfvw/m/3j3eli/PP/9nStGeQYOSR0ilhtEpZ2fqEI5V2aUrwCtrg0uoJ/UD757I5iBcOLDsCWruEvnXEz5tTieiFh+KyrtviIui3NlV+WacPo/oe9Un8LW7fPyCxzKZTnWunNuTx3QwKewdfu8/AKH5bIca303krpI+kHSAkn9Y6TvL+kdSbMlTZXUOCLtJknfS5oj6eYY+94uySTVyq8czpU4saawfeUC+G1ecZfMuXwDxxGSxkh6P2I5e71BXjtKKg8MIZhLvBHQTVKjqGz3ALPMrClwJTAo3Lcx0AtIB44HzpaUGnHsekAnYGkB6+lcyZM9hW3fadDpH7BsanD77vs3Q9ZvxV06V4blN+d414jlJ6LSotejpQMLzGwhgKRR4fHmRuRpBDwCYGbzJdWXlAIcC3xlZhvDfScBfwEeC/d7CrgTeC+fMjhX8lVIhjY3BqPvTnoUpr8I370JJ98MrftChcrFXUJXxsjiePBIUgWgMfCLmeX5k0fSBUAXM7smXL8CaGVm/SLyPAwkm9mtktKBL4FWwEaCoNAa2ARMAKab2Q2SzgU6mtlNkhYDaWb2e4zz9wZ6A6SkpLQYNWpUgesZKSsri6pVqxZq35LK67xvq7wxkyN/folaq6ayuVItFjW4gv+lnAKK7+76klTnouJ1jk/79u1nmFnabglmlusLGAocFy7XIGgtfAf8AnTLZ98LgRci1q8AnonKUx34LzALGAFMA44P064GvgE+C8vxFFAF+BqoEeZZDNTKqxxmRosWLaywJk6cWOh9Syqvcwmx8DOzoW3NHqhu9u9TzRZ/EdfuJbLOe8jrHB+CH+y7fafm9xOlrZnNCZd7AD+aWROgBUFXUV4ygXoR63WBXZ5sMrP1ZtbDzJoRXOM4CFgUpr1oZieY2SnAauAn4EiCayvfhq2NusA3kg7OpyzOlT4N2kKvDJ/C1u11+QWOrRHLnYB3Aczs1wIcexqQKqmBpIrAJcCYyAySaoZpANcAn5nZ+jCtdvh+GHA+MNLMvjOz2mZW38zqEwSnEwpYHudKn1ynsL3Lp7B1CZNf4Fgr6WxJzYE2wEcAkpKAPK/Imdl2oB8wDpgHvG5mcyT1kdQnzHYsMEfSfIK7r26KOMRbkuYC7wN9zWxNnHVzruzYbQrb530KW5cw+d1VdS0wGDgYuDnil31H4MP8Dm5mY4GxUduGRixPAVKj9wvT2hbg+PXzy+NcmZI9hW2ra+Hj+4MpbKf9B077OzTqGtzi69weyrPFYWY/mlkXM2tmZsMjto8zs9sSXjrnXOFkT2F7+VtQoYpPYeuKVJ4tDkmD80o3sxuLtjjOuSJ11GnQoB3MegU+HRBMYdv4r9DxgeIumSvB8uuq6gN8D7xOcEeUt3OdK2nKJ0GLq4KA8cWg4LrHvA9oVvUo+P1YqF4Hqh2y63vVgyGpYr6HdmVTfoGjDsHzGBcD24HRwFt+odq5EqhSNehwH7ToAZP/H/z4JfwyHeatgB0xLqDvdxBUqwPVD4l6jwgwyTX9ukkZlGfgMLNVBA/fDZV0KNCN4C6ou8xsxN4ooHOuiNU4FM56kln7ZdCuXbtg2tpNa4IJpDas2P193S+QOQ02rtr9WEmVo1osMQJNtYODeUdcqZFfiwMASScQBI1OwP8BMxJZKOfcXiRBlQOC18GNc8+3fUsYUFbAhuXhe0SAWTY1eN+xNWpHBa2XWF1ikQEmuYa3XkqI/C6O/x04m+A5jFHA3eHzGc65siapEuxfP3jlxix48DAnsES9r1sGy76GTTEeTqxQJfcusZxrLyneetkH5NfiuB9YSDC0+fHAwwp+EQgwC4ZDd865gAT7HRi8Dm6Se75tm4PWyS5dYxEBZtlXsOHX2K2XqrXzDzCVqnvrJYHyCxx5zrnhnHOFUiE5mO3wgDy+YsyC6yq5XXtZswSWTgmuz+x2/P1iXnOptXI1LNvvzzvHyheot95Fye/i+JJY28NJmi4BYqY759wek2C/WsGrTh6dG9s2RbRYYgSYJVOC953baAwwZ2D2Cbz1Ukj5XeOoDvQFDiUYoHA8wfhTtxMMhf5qgsvnnHN5q1AZDjgieOVm507YuIrpE98n7ehDdr/2smYxLPkSNq+NcfzYrZdd3qumlKnWS341HQGsAaYQjF57B1AR6GpmsxJbNOecKyLlykHVg8iqdgQ0bJd7vq0bI669xLi4v+TLsPUSdY+QysF+tfO/NTm5ekKrubfkFziOCOffQNILwO/AYWa2IeElc865va1iFTjwyOCVm507YePvMa69hAFm9UJY8kXs1kvFqrl3iUXeOVaufMKqWBTyCxzbshfMbIekRR40nHNlWrlywbWRqrWBZrnny7f18kXurZeqKQW49lItkbXMU36B43hJ68NlAZXD9ezbcUtHu8s554paUbReVv0Miz+HzetiHL9aHtdewgBTtXZCqpbfXVX7dnvJOedKsgK3Xv4InmvJ7dbkRZ9D1q8xWi/lOaDxvUC7Ii12Qm8DkNQFGASUB14ws4FR6fsDwwjmEt8M9DSz78O0m4BeBK2b/5jZ0+H2x4FzCKa1/RnoYWZrE1kP55wrVhX3K1jr5Y+Vu3WJbdx6aJEXJ7+pYwstfNZjCMGUsI2AbpIaRWW7B5gVPoF+JUGQQVJjgqCRTvDE+tmSsmcKHA80Dvf5Ebg7UXVwzrkSo1y5YAbIQ5rDMWdCy2ug4/1srnxw0Z+qyI/4p3RggZktNLOtBGNddY3K0wiYAGBm84H6klII5iL/ysw2hmNjTQL+Eub7OGK8rK+Augmsg3POuSiJDByHAssi1jPDbZG+Bc4HkJQOHE4QCL4HTpF0oKQqwJlAvRjn6EkwWq9zzrm9JJHXOGI9o29R6wOBQZJmAd8BM4HtZjZP0qME3VJZBAFml6s+ku4Nt8V8el1Sb6A3QEpKChkZGYWqRFZWVqH3Lam8zmWD17lsSEidzSwhL6A1MC5i/W6CYdlzyy9gMVA9RtrDwPUR690JnmavUpCytGjRwgpr4sSJhd63pPI6lw1e57JhT+oMTLcY36mJ7KqaBqRKaiCpIsGgiGMiM0iqGaZBMKTJZ2a2PkyrHb4fRtCdNTJc7wLcBZxrZhsTWH7nnHMxJKyrysy2S+oHjCO4HXeYmc2R1CdMH0pwEfxlSTuAucDVEYd4S9KBBE+v97U/5zl/FqgEjA/nBvnKzPokqh7OOed2ldDnOMxsLDA2atvQiOUpQGr0fmFa21y2H1WUZXTOORefRHZVOeecK4U8cDjnnIuLBw7nnHNx8cDhnHMuLh44nHPOxcUDh3POubh44HDOORcXDxzOOefi4oHDOedcXDxwOOeci4sHDuecc3HxwOGccy4uHjicc87FxQOHc865uHjgcM45FxcPHM455+LigcM551xcEho4JHWR9IOkBZL6x0jfX9I7kmZLmiqpcUTaTZK+lzRH0s0R2w+QNF7ST+H7/omsg3POuV0lLHBIKg8MAc4AGgHdJDWKynYPMMvMmgJXAoPCfRsDvYB04HjgbEnZU8z2ByaYWSowIVx3zjm3lySyxZEOLDCzhWa2FRgFdI3K04jgyx8zmw/Ul5QCHAt8ZWYbzWw7MAn4S7hPV+ClcPkl4LwE1sE551yUpAQe+1BgWcR6JtAqKs+3wPnAZEnpwOFAXeB7YICkA4FNwJnA9HCfFDNbAWBmKyTVjnVySb2B3gApKSlkZGQUqhJZWVmF3rek8jqXDV7nsiERdU5k4FCMbRa1PhAYJGkW8B0wE9huZvMkPQqMB7IIAsz2eE5uZs8DzwOkpaVZu3bt4ip8toyMDAq7b0nldS4bvM5lQyLqnMjAkQnUi1ivCyyPzGBm64EeAJIELApfmNmLwIth2sPh8QD+J6lO2NqoA/yWwDo455yLkshrHNOAVEkNJFUELgHGRGaQVDNMA7gG+CwMJmR3QUk6jKA7a2SYbwzQPVzuDryXwDo455yLkrAWh5ltl9QPGAeUB4aZ2RxJfcL0oQQXwV+WtAOYC1wdcYi3wmsc24C+ZrYm3D4QeF3S1cBS4MJE1cE559zuEtlVhZmNBcZGbRsasTwFSI3eL0xrm8v2VUDHIiymc865OPiT48455+LigcM551xcPHA455yLiwcO55xzcfHA4ZxzLi4eOJxzzsXFA4dzzrm4eOBwzjkXFw8czjnn4uKBwznnXFw8cDjnnIuLBw7nnHNx8cDhnHMuLh44nHPOxcUDh3POubh44HDOORcXDxzOOefiktDAIamLpB8kLZDUP0b6/pLekTRb0lRJjSPSbpE0R9L3kkZKSg63N5P0laRZkqZLSk9kHZxzzu0qYYFDUnlgCHAG0AjoJqlRVLZ7gFlm1hS4EhgU7nsocCOQZmaNCeYsvyTc5zHg72bWDPhbuO6cc24vSWSLIx1YYGYLzWwrMAroGpWnETABwMzmA/UlpYRpSUBlSUlAFWB5uN2A6uFyjYjtzjnn9gKZWWIOLF0AdDGza8L1K4BWZtYvIs/DQLKZ3Rp2OX0Z5pkh6SZgALAJ+NjMLgv3ORYYB4gg8J1kZktinL830BsgJSWlxahRowpVj6ysLKpWrVqofUsqr3PZ4HUuG/akzu3bt59hZmm7JZhZQl7AhcALEetXAM9E5akO/BeYBYwApgHHA/sDnwIHARWAd4HLw30GA38Nly8CPsmvLC1atLDCmjhxYqH3Lam8zmWD17ls2JM6A9MtxndqIruqMoF6Eet1iepWMrP1ZtbDgusVV4aBYhFwGrDIzFaa2TbgbeCkcLfu4TrAGwRdYs455/aSRAaOaUCqpAaSKhJc3B4TmUFSzTAN4BrgMzNbDywFTpRURZKAjsC8MN9y4NRwuQPwUwLr4JxzLkpSog5sZtsl9SO4HlEeGGZmcyT1CdOHAscCL0vaAcwFrg7Tvpb0JvANsB2YCTwfHroXMCi8aL6Z8DqGc865vSNhgQPAzMYCY6O2DY1YngKk5rLvA8ADMbZPBloUbUmdc84VlD857pxzLi4eOJxzzsXFA4dzzrm4eOBwzjkXFw8czjnn4uKBwznnXFw8cDjnnIuLBw7nnHNx8cDhnHMuLh44nHPOxcUDh3POubh44HDOORcXDxzOOefi4oHDOedcXDxwOOeci4sHDuecc3FJaOCQ1EXSD5IWSOofI31/Se9Imi1pqqTGEWm3SJoj6XtJIyUlR6TdEB53jqTHElkH55xzu0pY4JBUHhgCnAE0ArpJahSV7R5glpk1Ba4EBoX7HgrcCKSZWWOCqWcvCdPaA12BpmZ2HPBEourgnHNud4lscaQDC8xsoZltBUYRfOFHagRMADCz+UB9SSlhWhJQOZxbvAqwPNx+HTDQzLaE+/2WwDo455yLksjAcSiwLGI9M9wW6VvgfABJ6cDhQF0z+4WgJbEUWAGsM7OPw32OBtpK+lrSJEktE1gH55xzUZISeGzF2GZR6wOBQZJmAd8BM4HtkvYnaJ00ANYCb0i63MxeISjz/sCJQEvgdUlHmNkux5bUG+gNkJKSQkZGRtwVeHXeFhat2cYjX/9f3PuWZDt27PA6lwFe57KhTuUdQEaRHjORgSMTqBexXpc/u5sAMLP1QA8ASQIWha/OwCIzWxmmvQ2cBLwSHvftMFBMlbQTqAWsjDr288DzAGlpadauXbu4KzBpwxyWrl9KzZo14963JFu7dq3XuQzwOpcNFXaupzDff3lJZOCYBqRKagD8QnBx+9LIDJJqAhvDayDXAJ+Z2XpJS4ETJVUBNgEdgenhbu8CHYAMSUcDFYHfE1GBB845joxqK2nXrnUiDr/PysjI8DqXAV7nsqEwvS35SVjgMLPtkvoB4wjuihpmZnMk9QnThwLHAi9L2gHMBa4O076W9CbwDbCdoAvr+fDQw4Bhkr4HtgLdo7upnHPOJU4iWxyY2VhgbNS2oRHLU4DUXPZ9AHggxvatwOVFW1LnnHMF5U+OO+eci4sHDuecc3HxwOGccy4uHjicc87FxQOHc865uHjgcM45FxeVhUcgJK0ElhRy91ok6AHDfZjXuWzwOpcNe1Lnw83soOiNZSJw7AlJ080srbjLsTd5ncsGr3PZkIg6e1eVc865uHjgcM45FxcPHPl7Pv8spY7XuWzwOpcNRV5nv8bhnHMuLt7icM45FxcPHM455+LigSOCpGGSfgvn+sjedoCk8ZJ+Ct/3L84yFiVJ9SRNlDRP0hxJN4XbS3OdkyVNlfRtWOe/h9tLbZ2zSSovaaakD8L1Ul1nSYslfSdplqTp4bbSXueakt6UND/8f906EXX2wLGr4UCXqG39gQlmlgpMCNdLi+3AbWZ2LMEc7n0lNaJ013kL0MHMjgeaAV0knUjprnO2m4B5Eetloc7tzaxZxHMMpb3Og4CPzOwY4HiCz7vo62xm/op4AfWB7yPWfwDqhMt1gB+Ku4wJrPt7QKeyUmegCsEsk61Ke52BuuGXRgfgg3Bbaa/zYqBW1LZSW2egOrCI8KanRNbZWxz5SzGzFQDhe+1iLk9CSKoPNAe+ppTXOeyymQX8Bow3s1JfZ+Bp4E5gZ8S20l5nAz6WNENS73Bbaa7zEcBK4L9hl+QLkvYjAXX2wOGQVBV4C7jZzNYXd3kSzcx2mFkzgl/h6ZIaF3OREkrS2cBvZjajuMuyl7UxsxOAMwi6YU8p7gIlWBJwAvAvM2sO/EGCuuI8cOTvf5LqAITvvxVzeYqUpAoEQeNVM3s73Fyq65zNzNYCGQTXtUpzndsA50paDIwCOkh6hdJdZ8xsefj+G/AOkE7prnMmkBm2oAHeJAgkRV5nDxz5GwN0D5e7E1wHKBUkCXgRmGdm/y8iqTTX+SBJNcPlysBpwHxKcZ3N7G4zq2tm9YFLgE/N7HJKcZ0l7SepWvYycDrwPaW4zmb2K7BMUsNwU0dgLgmosz85HkHSSKAdwTDE/wMeAN4FXgcOA5YCF5rZ6mIqYpGSdDLwOfAdf/Z930NwnaO01rkp8BJQnuCH0+tm9pCkAymldY4kqR1wu5mdXZrrLOkIglYGBF04r5nZgNJcZwBJzYAXgIrAQqAH4b9zirDOHjicc87FxbuqnHPOxcUDh3POubh44HDOORcXDxzOOefi4oHDOedcXDxwuGITPlMxWdL3ks6L2P6epEMKcayvw6EW2hZ5YfdRksZmP5dSkkhqJunM4i6HKxwPHK44dSN4pqI1cAeApHOAb7Kf+o1DR2C+mTU3s8+LonCSkoriOIk8t5mdGT4BX2KEdWsGeOAooTxwuOK0DagMVAJ2hl8oNwOP57aDpMMlTZA0O3w/LHzo6THgzHDuhcpR+7SU9GU4B8dUSdXCeTn+G87XMFNS+zDvVZLekPQ+wQB5+ymYp2VamK9rjDK1y57jIlx/VtJV4fJASXPD8j4RbjtI0lvhMadJahNuf1DS85I+Bl6WdFxY3lnh/qkxzr1YUi1J9cP5F/6jYJ6Rj6P/DmH+C8MW3reSPouo87MReT4IHxREUpakJyV9E/69Dwq3Z0h6Ovy7fi8pPdx+gKR3w/J+FT5wuVvdgIeAi8O6XZzb5+32UcU9FLC/yu4LqAF8CEwnaDHcCHTPZ5/3s/MAPYF3w+WrgGdj5M9+grZluF6d4Eni24D/htuOIXiiNjk8TiZwQJj2MHB5uFwT+BHYL+oc7QiHKg/Xnw2PcwDBkNbZD9rWDN9fA04Olw8jGPIF4EFgBlA5XH8GuCyiHpVj1G8xwUgH9QnmV2kWbn89u9xR+b8DDo0qzy5/O+ADoF24bBFl+Ft2PoIxvv4TLp9COBVBWOYHwuUOwKxc6hbz8/JXyXh5i8MVGzNbZ2ZnWTDJzjfA2cBb4a/mNyW1jrFba4IvXoARwMn5nKYhsMLMpoXnXG9m28P9RoTb5gNLgKPDfcbbn0MynA70VzAMewZBcDmsgFVcD2wGXpB0PrAx3H4a8Gx4zDFA9exxlYAxZrYpXJ4C3CPpLuDwiO25WWRms8LlGQTBJNoXwHBJvQiGXcnPTmB0uPwKu/69RwKY2WdhHWqy69/1U+BASTVi1M2VYB443L7ib8AAguseMwhaEw8XYL/8xsxRLnmUxz5/ROX7qwWzyDUzs8PMbF5U/u3s+n8pGSAMUOkEow+fB3wUppcDWkcc81Az2xB9bjN7DTgX2ASMk9QhjzJDMLthth0ELatdmFkf4D6gHjArHLspZvlzYbksZ6/H+rtm5/sjRporgTxwuGIX9t0fYmaTCGbl20nwZRPrC+xLghFeAS4DJudz+PnAIZJahueqFl5L+SzcH0lHE7Qifoix/zjgBkkK8zaPkWcJ0EhSpfDXdccwb1WghpmNJbh20yzM/zHQL6L+zYhBwUB9C81sMEHLpGk+dc2XpCPN7Gsz+xvwO0EAWQw0k1ROUj2CYJetHHBBuHwpu/69Lw6PeTKwzszWsevftR3wu8We42UDUC3GdlcCFNtdI85FGADcGy6PJBiR+CaCVki0G4Fhku4gmO2sR14HNrOt4cXXZ8KLxZsIuoqeA4ZK+o7gF/dVZrYljA+R/kEwe97sMHgsJuhSizzHMkmvA7OBn4CZYVI14D1JyQS/xG+JqMMQSbMJ/g9+BvSJUfyLgcslbQN+JbigvKceDwO1CKaS/Tbcvojg+sf3BN2G2f4AjpM0A1gXlinbGklfElw36hlue5BgBrrZBF1z3YltIn92AT5iZqNzyef2QT46rnMuV5KyzKxqjO0ZBMOzT9/7pXLFzbuqnHPOxcVbHM455+LiLQ7nnHNx8cDhnHMuLh44nHPOxcUDh3POubh44HDOOReX/w8nOiLARneL/gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_axis = [10, 20, 40, 60]\n",
    "y_axis = [1.0013,0.9986,0.9933, 0.9923]\n",
    "x1_axis = [10, 20, 40, 60]\n",
    "y1_axis = [0.9859, 0.9859, 0.9859, 0.9859]\n",
    "plt.plot(x1_axis, y1_axis, label=\"base line(100% support)\")\n",
    "plt.plot(x_axis, y_axis, label=\"% of core users\")\n",
    "plt.title('RMSE VS % of core user in support')\n",
    "plt.xlabel('% of core users in support')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "193329ff9dce9d81919d1650a0fae85851d0854e3de134d2444776b3f06aa6e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
